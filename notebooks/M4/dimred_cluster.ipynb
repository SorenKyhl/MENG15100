{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgements"
      ],
      "metadata": {
        "id": "zbOcojMuQrYw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4040901"
      },
      "source": [
        "Notebook developed by Andrew Ferguson for the UChicago course MENG15100: Machine Learning and Artificial Intelligence for Molecular Discovery and Engineering.\n",
        "\n",
        "Components of these notebooks and code were borrowed and adapted from the [Data@PSL](https://github.com/data-psl) tutorials originally written by Pierre Ablin, Mathieu Blondel and Arthur Mensch under the [MIT License](https://github.com/data-psl/lectures2025/blob/main/LICENSE). Some materials therein were adapted from [sklearn tutorials](https://github.com/jakevdp/sklearn_tutorial/tree/master) developed by Jake Vanderplas under the [BSD-3 License](https://github.com/jakevdp/sklearn_tutorial/blob/master/LICENSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xE_e18pS3kL"
      },
      "source": [
        "# Unsupervised Dimensionality Reduction w/ Principal Components Analysis (PCA)\n",
        "\n",
        "Here we'll explore **Principal Components Analysis** as a simple and powerful linear dimensionality reduction technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY48WKwCS3kM"
      },
      "source": [
        "## Data Exploration\n",
        "\n",
        "First let's generate a 2D data set that is amenable to easy visualization and which we will use to motivate, understand, and conduct PCA."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "num_points = 500\n",
        "mean = [3, -5]\n",
        "cov = [[10, 0], [0, 0.3]]  # Stretch along x-axis (ellipse)\n",
        "X = np.random.multivariate_normal(mean, cov, size=num_points)\n",
        "\n",
        "theta = np.radians(30)  # Rotate the dataset to make PCA more illustrative\n",
        "rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n",
        "                            [np.sin(theta),  np.cos(theta)]])\n",
        "X = X @ rotation_matrix.T\n",
        "\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "4lLnzA7gNUvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot the data."
      ],
      "metadata": {
        "id": "Gq1ygr_aOen_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.5, edgecolor='k')\n",
        "plt.axis('equal')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A1WhTwpPOcfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do you notice about the data?\n",
        "> * Is the data uniformly distributed, or does it have any obvious shape?\n",
        "* Is one direction in the data more important than another?\n",
        "* Is the X1 component of the data more important than the X2 component?\n",
        "* Is there another component that is a mix of X1 and X2 that is the most important?\n",
        "* What do we mean by \"important\"?"
      ],
      "metadata": {
        "id": "_MFYN95POw2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variance"
      ],
      "metadata": {
        "id": "I9JGpLXbTt24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute the **variance** of the data set along the X1 and X2 directions. The variance is a mathematical concept that reports how spread out the data is along a particular direction.\n",
        "\n",
        "Given some data point $\\{x_i\\}_{i=1}^N$, we can compute the **mean** as:\n",
        "\n",
        "$\\mu = \\frac{1}{N}\\sum_{i=1}^N x_i$\n",
        "\n",
        "and the **variance** as:\n",
        "\n",
        "$\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N (x_i - \\mu)^2$\n",
        "\n",
        "The **standard deviation** $\\sigma$ is just the square root of the variance."
      ],
      "metadata": {
        "id": "Gs7tdpbqPQcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, PCA should be applied to **mean-zeroed** data. This just means that we have to subtract out the mean from all of our features $X_k$ before we run PCA,\n",
        "\n",
        "$X_k \\leftarrow X_k - \\mu_k$,\n",
        "\n",
        "where,\n",
        "\n",
        "$\\mu_k = \\frac{1}{N}\\sum_{i=1}^N X_k[i]$.\n",
        "\n",
        "Let's first check that our data are (approximately) mean zero."
      ],
      "metadata": {
        "id": "fXQHyNxRoc7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mean(X1) = %.3f' % (np.mean(X[:, 0])))\n",
        "print('Mean(X2) = %.3f' % (np.mean(X[:, 1])))"
      ],
      "metadata": {
        "id": "HEUpI7xqT6Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So let's mean zero with a line of Python code. We will also store the means for posterity in case we ever need to add these back on in the future."
      ],
      "metadata": {
        "id": "NGqZieYFej0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu_X = np.mean(X, axis=0)\n",
        "X = X - mu_X\n",
        "print('Mean(X1) = %.3f' % (np.mean(X[:, 0])))\n",
        "print('Mean(X2) = %.3f' % (np.mean(X[:, 1])))"
      ],
      "metadata": {
        "id": "_AANFWN8eb-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the variance along each feature."
      ],
      "metadata": {
        "id": "entfabmYpCXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cov = np.cov(X.T)\n",
        "print('Var(X1) = %.3f' % (cov[0, 0]))\n",
        "print('Var(X2) = %.3f' % (cov[1, 1]))"
      ],
      "metadata": {
        "id": "ol2rkg0BPsMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we are in the **unsupervised learning** regime, so our data $X$ do not have any accompanying labels $y$. In the unsupervised regime, we do not have any concept of a predictive accuracy by which to define a loss function and fit a model.\n",
        "\n",
        "> *It is the primary goal of PCA to find a lower-dimensional representation of the data that preserves the maximum amount of variance in the data.*\n",
        "\n",
        "The underlying assumption of this objective is that more information is carried in elongated directions as opposed to squashed directions."
      ],
      "metadata": {
        "id": "0dS2dDKCSLJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"natural\" way to represent our data is by the $X1$ and $X2$ features.\n",
        "\n",
        "Let's take a quick look at the first few rows of the 2D $X = \\{X1,X2\\}$ data."
      ],
      "metadata": {
        "id": "yUUaC7AVTEyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"           X1\",\"         X2\")\n",
        "print(X[:10])"
      ],
      "metadata": {
        "id": "hNeU8Ur5TYKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data is 2D, meaning that each instance is represeted by two features $X1$ and $X2$.\n",
        "\n",
        "* If we had to choose to represent the data in 1D using only a single feature, would you pick -- $X1$ or $X2$?\n",
        "* Why?\n",
        "\n",
        "This is the essence of dimensionality reduction\\!"
      ],
      "metadata": {
        "id": "dM_Z8DHuUUEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rotation"
      ],
      "metadata": {
        "id": "Qxqdwq26U88k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can we do better than just picking $X1$ or $X2$ for our compressed 1D representation of the data?\n",
        "\n",
        "**Yes!** In our data exploration above, we noted that there was a mixed direction containing a lot of $X1$ mixed in with a bit of $X2$ that defines a sloped or diagonal direction that looks like it may contain even more variance than $X1$ or $X2$."
      ],
      "metadata": {
        "id": "63_KS1XHVG3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look into this further by trying to find some new directions that carry more variance. Graphically, we can do so by effectively **\"tilting our head\"** or **rotating the coordinate frame** to represent our data in a new rotated set of axes. We hypothesize that by passing one of our coordinate axes along the elongated aspect of the data might allow us to achieve a 1D representation preserving much more of the variance.\n",
        "\n",
        "Mathematically, we would like to **maximize the variance carried in each axis of the rotated frame**,\n",
        "\n",
        " $L = \\sum_i Var(PC_i)$,\n",
        "\n",
        " where $PC_i$ are the **principal components** defining the directions of our rotated axes.\n",
        "\n",
        "Let's play with the Python GUI below to rotate the coordinate frame and try to maximize the variance carried along the rotated axes."
      ],
      "metadata": {
        "id": "4qqHGz4AWZk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, FloatSlider\n",
        "import matplotlib\n",
        "\n",
        "# Ensure clean plots in Colab\n",
        "matplotlib.rcParams['figure.figsize'] = [6, 6]\n",
        "\n",
        "# Interactive plot function\n",
        "def plot_rotated_axes(theta_deg):\n",
        "    theta = np.radians(theta_deg)\n",
        "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n",
        "                                [np.sin(theta),  np.cos(theta)]])\n",
        "\n",
        "    # Unit vectors\n",
        "    unit_vec1 = rotation_matrix[:, 0]\n",
        "    unit_vec2 = rotation_matrix[:, 1]\n",
        "\n",
        "    # Project data onto rotated unit vectors\n",
        "    proj1 = X @ unit_vec1\n",
        "    proj2 = X @ unit_vec2\n",
        "\n",
        "    # Variances along rotated axes\n",
        "    var1 = np.var(proj1)\n",
        "    var2 = np.var(proj2)\n",
        "    total_var = var1 + var2\n",
        "\n",
        "    # Plot\n",
        "    plt.figure()\n",
        "    plt.scatter(X[:, 0], X[:, 1], alpha=0.3, edgecolor='k', label='Data')\n",
        "\n",
        "    origin = np.mean(X, axis=0)\n",
        "    scale = 5  # for visual length of arrows\n",
        "\n",
        "    # Draw unit vectors\n",
        "    arrow1 = unit_vec1 * scale\n",
        "    arrow2 = unit_vec2 * scale\n",
        "    plt.quiver(*origin, *arrow1, color='red', angles='xy', scale_units='xy', scale=1, label='PC1')\n",
        "    plt.quiver(*origin, *arrow2, color='blue', angles='xy', scale_units='xy', scale=1, label='PC2')\n",
        "\n",
        "    # Label variances next to each vector tip\n",
        "    tip1 = origin + arrow1\n",
        "    tip2 = origin + arrow2\n",
        "    plt.text(tip1[0], tip1[1], f\"Var(PC1) = {var1:.2f}\", color='red', fontsize=10, ha='left', va='bottom')\n",
        "    plt.text(tip2[0], tip2[1], f\"Var(PC2) = {var2:.2f}\", color='blue', fontsize=10, ha='left', va='bottom')\n",
        "\n",
        "    # Total variance in bottom-right corner\n",
        "    xmin, xmax = plt.xlim()\n",
        "    ymin, ymax = plt.ylim()\n",
        "    plt.text(xmax - 2, ymin + 1, f\"Total Variance: {total_var:.2f}\",\n",
        "             ha='right', va='bottom', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "    plt.title(f\"Rotated Axes at θ = {theta_deg:.1f}°\")\n",
        "    plt.axis('equal')\n",
        "    plt.xlabel('X1')\n",
        "    plt.ylabel('X2')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Slider widget\n",
        "interact(plot_rotated_axes, theta_deg=FloatSlider(value=0, min=0, max=360, step=5, description='θ (degrees)'))"
      ],
      "metadata": {
        "id": "SWRhQzDtZXcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graphically, we can see that $\\theta$ is controlling the mixing between $X1$ and $X2$. Mathematically, we can represent this as:\n",
        "\n",
        "$PC1 = \\cos(\\theta) X1 + \\sin(\\theta) X2$\n",
        "\n",
        "And since $PC2$ has to be **orthogonal** (i.e., at right angles to) $PC1$:\n",
        "\n",
        "$PC2 = -\\sin(\\theta) X1 + \\cos(\\theta) X2$"
      ],
      "metadata": {
        "id": "7W8LDam1bVlU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the optimal rotation angle that places the most variance in PC1?\n",
        "\n",
        "Store this value below."
      ],
      "metadata": {
        "id": "bdcCIpR5awK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta = NotImplemented"
      ],
      "metadata": {
        "id": "MmL1fypta84n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, now instead of keeping the *data fixed* and looking at our *rotated axes* move, let's flip the script and instead *keep the axes fixed* and *rotate the data*.\n",
        "\n",
        "We will also report out next to the plot the coordinates of the data in the original coordinate frame ($X1,X2$) and in the rotated frame ($PC1,PC2$).\n",
        "\n",
        "In the plot, we will show the 1D representation of the points in $PC1$ as dark blue crosses on the x-axis.\n",
        "\n",
        "Remember, our objective is to preserve as much variance as possible in $PC1$ so we want the projection of the points in $PC1$ to be as spread out as possible."
      ],
      "metadata": {
        "id": "zQbddMgEcZOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, FloatSlider\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Ensure clean plots in Colab\n",
        "matplotlib.rcParams['figure.figsize'] = [6, 6]\n",
        "\n",
        "# Interactive plot function\n",
        "def plot_rotated_axes(theta_deg):\n",
        "    theta = np.radians(theta_deg)\n",
        "\n",
        "    # Rotation matrix to rotate the data visually\n",
        "    rotation_matrix = np.array([[np.cos(theta), -np.sin(theta)],\n",
        "                                [np.sin(theta),  np.cos(theta)]])\n",
        "\n",
        "    # Rotate data\n",
        "    X_rotated = X @ rotation_matrix  # Rotate points\n",
        "\n",
        "    # Compute variances in rotated frame\n",
        "    var_pc1 = np.var(X_rotated[:, 0])\n",
        "    var_pc2 = np.var(X_rotated[:, 1])\n",
        "\n",
        "    # Prepare plot and table output widgets\n",
        "    output_plot = widgets.Output()\n",
        "    output_table = widgets.Output()\n",
        "\n",
        "    with output_plot:\n",
        "        plt.figure()\n",
        "\n",
        "        # Plot rotated data points\n",
        "        plt.scatter(X_rotated[:, 0], X_rotated[:, 1],\n",
        "                    alpha=0.3, edgecolor='k', label='Rotated Data', color='skyblue')\n",
        "\n",
        "        # Plot projection onto x-axis (PC1 axis)\n",
        "        plt.scatter(X_rotated[:, 0], np.zeros_like(X_rotated[:, 0]),\n",
        "                    alpha=0.6, label='Projections onto PC1', color='blue', marker='x',)\n",
        "\n",
        "        # Draw vertical lines from points to their projections\n",
        "        for x, y in X_rotated:\n",
        "            plt.plot([x, x], [0, y], color='blue', alpha=0.1)\n",
        "\n",
        "        # Variance info\n",
        "        xmin, xmax = plt.xlim()\n",
        "        ymin, ymax = plt.ylim()\n",
        "        info_text = f\"Var(PC1) = {var_pc1:.2f}\\nVar(PC2) = {var_pc2:.2f}\"\n",
        "        plt.text(xmax - 2, ymin + 1, info_text,\n",
        "                 ha='right', va='bottom', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
        "\n",
        "        # Final plot settings\n",
        "        plt.title(f\"Rotated Data and PC1 Projections at θ = {theta_deg:.1f}°\")\n",
        "        plt.xlim(-11, 11)\n",
        "        plt.ylim(-11, 11)\n",
        "        plt.xlabel('PC1')\n",
        "        plt.ylabel('PC2')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    with output_table:\n",
        "        # Show first 15 data points with original and rotated coordinates\n",
        "        df = pd.DataFrame({\n",
        "            'X1': X[:15, 0],\n",
        "            'X2': X[:15, 1],\n",
        "            'PC1': X_rotated[:15, 0],\n",
        "            'PC2': X_rotated[:15, 1]\n",
        "        })\n",
        "        display(df.round(3))\n",
        "\n",
        "    # Display side-by-side plot and table\n",
        "    display(widgets.HBox([output_plot, output_table]))\n",
        "\n",
        "# Slider widget for rotation angle\n",
        "interact(plot_rotated_axes, theta_deg=FloatSlider(value=0, min=0, max=360, step=5, description='θ (degrees)'))\n"
      ],
      "metadata": {
        "id": "_XF9-fPlcYlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dial in your optimal $\\theta$ to the plot above to maximize the variance contained in $PC1$.\n",
        "\n",
        "Now, within the rotated coordinate frame we still have 2D data represented by $PC1,PC2$ instead of $X1,X2$.\n",
        "\n",
        "* If we had to choose to represent the data in 1D using only a single feature, would you pick -- $PC1$ or $PC2$?\n",
        "* How much variance is preserved in your choice?\n",
        "* Scroll back up and see how much variance was preserved when you made a 1D dimensionality reduction in $X1,X2$. Use the cell below to compute what percentage more variance you have retained by doing 1D dimensionality reduction in $PC1,PC2$."
      ],
      "metadata": {
        "id": "8qrb5zAYeSop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Var_old = NotImplemented\n",
        "Var_new = NotImplemented\n",
        "Var_percentage_improvement = (Var_new - Var_old) / Var_old * 100\n",
        "print(f\"Percentage more variance retained: {Var_percentage_improvement:.2f}%\")"
      ],
      "metadata": {
        "id": "KQDtvUJhgGvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA"
      ],
      "metadata": {
        "id": "Ec9BWkK4g4R-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have developed an intuition for the concept of dimensionality reduction under PCA -- essentially rotating the coordinate frame to align our PCs along the directions of maximum variance.\n",
        "\n",
        "We also defined an objective function to be maximized to preserve the most variance under the rotated coordinate frame:\n",
        "\n",
        "$L = \\sum_i Var(PC_i)$"
      ],
      "metadata": {
        "id": "fhlUOyN1g6ZR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now use ML to find the optimal rotation angle in an automated fashion by tuning $\\theta$ to maximize $L$. Similar to the linear regression example, we can do this by gradient descent, but it turns out there is also an simpler analytical formula we can use based on a technique known as eigenvalue decomposition but we get to the same place either way.\n",
        "\n",
        "This is **principal components analysis (PCA)** -- finding the optimal rotation of the coordinates so $PC1$ contains the most variance, $PC2$ contains the next most variance, $PC3$ contains the next most variance, etc. and all $PC$s remain orthogonal (i.e., at right angles)."
      ],
      "metadata": {
        "id": "HyQtWWoZQ0de"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run PCA using the code below.\n",
        "\n",
        "Make sure you can identify where we are making the call to the PCA routine from the scikit-learn library (`sklearn`)."
      ],
      "metadata": {
        "id": "ioZmU3WlPw1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.rcParams['figure.figsize'] = [6, 5]\n",
        "\n",
        "# ---- Perform PCA ----\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# ---- Variance calculations ----\n",
        "var_X1 = np.var(X[:, 0])\n",
        "var_X2 = np.var(X[:, 1])\n",
        "var_PC1, var_PC2 = pca.explained_variance_\n",
        "\n",
        "# ---- Create output widgets ----\n",
        "output_plot = widgets.Output()\n",
        "output_table = widgets.Output()\n",
        "\n",
        "with output_plot:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # --- Original data plot ---\n",
        "    axes[0].scatter(X[:, 0], X[:, 1], alpha=0.3, edgecolor='k')\n",
        "    axes[0].set_title('Original Data (X1 vs X2)')\n",
        "    axes[0].set_xlabel('X1')\n",
        "    axes[0].set_ylabel('X2')\n",
        "    axes[0].axis('equal')\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # --- PCA-transformed plot ---\n",
        "    axes[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.3, edgecolor='k', color='purple')\n",
        "    axes[1].set_title('PCA Projection (PC1 vs PC2)')\n",
        "    axes[1].set_xlabel('PC1')\n",
        "    axes[1].set_ylabel('PC2')\n",
        "    axes[1].axis('equal')\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # Add variance text in bottom-right corner (inside axes)\n",
        "    for ax, var1, var2, label1, label2 in zip(\n",
        "        axes,\n",
        "        [var_X1, var_PC1],\n",
        "        [var_X2, var_PC2],\n",
        "        ['X1', 'PC1'],\n",
        "        ['X2', 'PC2']\n",
        "    ):\n",
        "        ax.text(\n",
        "            0.98, 0.02,\n",
        "            f\"Var({label1}) = {var1:.2f}\\nVar({label2}) = {var2:.2f}\",\n",
        "            ha='right',\n",
        "            va='bottom',\n",
        "            transform=ax.transAxes,\n",
        "            fontsize=9,\n",
        "            bbox=dict(facecolor='white', alpha=0.6, boxstyle='round,pad=0.3')\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "with output_table:\n",
        "    df = pd.DataFrame({\n",
        "        'X1': X[:12, 0],\n",
        "        'X2': X[:12, 1],\n",
        "        'PC1': X_pca[:12, 0],\n",
        "        'PC2': X_pca[:12, 1]\n",
        "    })\n",
        "    display(df.round(3))\n",
        "\n",
        "# ---- Display side-by-side ----\n",
        "display(widgets.HBox([output_plot, output_table]))\n"
      ],
      "metadata": {
        "id": "7QkOTjEoPt4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So what have we achieved?\n",
        "\n",
        "* How much variance is contained together within $X1$ and $X2$?"
      ],
      "metadata": {
        "id": "i528S7lHSXWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_X1X2 = NotImplemented"
      ],
      "metadata": {
        "id": "wBtPLVRISysr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* How much variance is contained together within $PC1$ alone?"
      ],
      "metadata": {
        "id": "rMbFal7GSy3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_PC1 = NotImplemented"
      ],
      "metadata": {
        "id": "X0djHdEaS0TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* What percentage of the original variance in 2D is preserved by a 1D projection into $PC1$?"
      ],
      "metadata": {
        "id": "_LS0a_BdS0dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pct_var_preserved = var_PC1 / var_X1X2 * 100\n",
        "print(\"Variance in 2D (X1,X2) data preserved by 1D projection into PC1: %.2f%%\" % pct_var_preserved)"
      ],
      "metadata": {
        "id": "FcCpLD25S1go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By what percentage have we compressed our representation of the data in going from a 2D ($X1,X2$) representation to a 1D $PC1$ representation?"
      ],
      "metadata": {
        "id": "ebT9q7EoTgnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compression_factor = NotImplemented"
      ],
      "metadata": {
        "id": "nNm6uNQETq7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here is the final result of PCA. It has given us a means to take our 2D data in ($X1,X2$) and represent it in 1D ($PC1$) while maintining a large fraction of the information content (i.e., variance).\n",
        "\n",
        "This is the sense in which dimensionality reduction \"works\": if you can approximate a data set in a lower dimension, we can have an easier time visualizing it, storing it, transmitting it, understanding it, or fitting complicated models to the data."
      ],
      "metadata": {
        "id": "AnA-uvC3UKp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# ---- Extract data for table ----\n",
        "df_left = pd.DataFrame({\n",
        "    'X1': X[:12, 0],\n",
        "    'X2': X[:12, 1]\n",
        "})\n",
        "\n",
        "df_right = pd.DataFrame({\n",
        "    'PC1': X_pca[:12, 0]\n",
        "})\n",
        "\n",
        "# ---- Create output widgets ----\n",
        "left_output = widgets.Output()\n",
        "arrow_output = widgets.Output()\n",
        "right_output = widgets.Output()\n",
        "\n",
        "with left_output:\n",
        "    display(df_left.round(3))\n",
        "\n",
        "with arrow_output:\n",
        "    display(HTML('<div style=\"font-size: 40px; padding: 20px;\">→</div>'))\n",
        "\n",
        "with right_output:\n",
        "    display(df_right.round(3))\n",
        "\n",
        "# ---- Combine and display side-by-side ----\n",
        "display(widgets.HBox([left_output, arrow_output, right_output]))\n"
      ],
      "metadata": {
        "id": "F32RyjhXUDIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFAx7t3XS3kP"
      },
      "source": [
        "## PCA -- Celebrity Faces\n",
        "\n",
        "In the example above we looked at a very simple 2D $\\rightarrow$ 1D dimensionality reduction using PCA.\n",
        "\n",
        "Remember how we talked about how we can represent images as high dimensional vectors just recording the intensity of each pixel? Let's do some dimensionality reduction on images of faces!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Let's load some celebrity faces from `sklearn`**"
      ],
      "metadata": {
        "id": "1D1U9ZnpfxSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "# Load dataset (grayscale faces of celebrities)\n",
        "lfw = fetch_lfw_people(min_faces_per_person=15, resize=0.5)\n",
        "X = lfw.data\n",
        "images = lfw.images\n",
        "targets = lfw.target\n",
        "target_names = lfw.target_names\n",
        "n_samples, h, w = images.shape\n",
        "\n",
        "# Count number of images per person\n",
        "counts = Counter(targets)\n",
        "name_counts = [(target_names[i], counts[i]) for i in range(len(target_names))]\n",
        "\n",
        "# Print name and count\n",
        "print(\"Number of images per person loaded:\\n\")\n",
        "for name, count in name_counts:\n",
        "    print(f\"- {name}: {count} images\")\n",
        "\n",
        "# Print dataset stats\n",
        "print(f\"\\nTotal number of images: {n_samples}\")\n",
        "print(f\"Image resolution: {h}x{w} pixels = {h * w} pixels per image\")"
      ],
      "metadata": {
        "id": "V87_bgxmfv-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. First let's visualize a few faces from the data set.**"
      ],
      "metadata": {
        "id": "SXHlN-e1fx4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "\n",
        "# Names to select\n",
        "selected_names = [\n",
        "    'Angelina Jolie',\n",
        "    'Arnold Schwarzenegger',\n",
        "    'David Beckham',\n",
        "    'Halle Berry',\n",
        "    'Lleyton Hewitt',\n",
        "    'Michael Schumacher',\n",
        "    'Nicole Kidman',\n",
        "    'Serena Williams'\n",
        "]\n",
        "\n",
        "# Find indices of the selected people in target_names\n",
        "selected_indices = [np.where(target_names == name)[0][0] for name in selected_names]\n",
        "\n",
        "# For each selected person, randomly pick one image index\n",
        "rng = np.random.default_rng(seed=42)\n",
        "chosen_images = []\n",
        "for idx in selected_indices:\n",
        "    person_image_indices = np.where(targets == idx)[0]\n",
        "    chosen_idx = rng.choice(person_image_indices)\n",
        "    chosen_images.append((selected_names[selected_indices.index(idx)], images[chosen_idx]))\n",
        "\n",
        "# Plot images in grid\n",
        "n = 2  # rows\n",
        "m = 4  # cols\n",
        "plt.figure(figsize=(3 * m, 4 * n))\n",
        "\n",
        "for i, (name, img) in enumerate(chosen_images):\n",
        "    ax = plt.subplot(n, m, i + 1)\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(name, fontsize=12)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle(\"Selected Celebrity Faces from LFW Dataset\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UxKJNBynfwQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Now let's run PCA**"
      ],
      "metadata": {
        "id": "U0jJqfthfyh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Sacling data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Compute PCA with all possible components\n",
        "max_components = min(n_samples, X_scaled.shape[1])\n",
        "pca_full = PCA(n_components=max_components, svd_solver='randomized', whiten=True, random_state=42)\n",
        "X_pca_full = pca_full.fit_transform(X_scaled)"
      ],
      "metadata": {
        "id": "GOxmYhi7fwZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Now let's visualize a few of the leading PCs**\n",
        "\n",
        "Remember how we found that our PCs in the example above were linear combinations of the input features? In this case we expect to see the same thing -- our PCs are going to be combinations of the various pixel intensities that make up our image.\n",
        "\n",
        "The neat thing about analyzing image data is that the PCs themselves (i.e., the mixing coefficients by which the pixel intensities are combined) are themselves viewable as images. Since PCA works using a mathematical technique known as eigenvalue decomposition, these have become known as \"eigenfaces\"!"
      ],
      "metadata": {
        "id": "tJDC3y1dkxAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_eigenfaces = 10\n",
        "eigenfaces = pca_full.components_[:n_eigenfaces].reshape((n_eigenfaces, h, w))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "cols = 5\n",
        "rows = 2\n",
        "\n",
        "for i in range(n_eigenfaces):\n",
        "    ax = plt.subplot(rows, cols, i + 1)\n",
        "    ax.imshow(eigenfaces[i], cmap='gray')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f'PC {i+1}', fontsize=10)\n",
        "\n",
        "plt.suptitle(\"Top 10 Eigenfaces\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2g1sDfb-l8Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Let's plot the culumative variance explained as a function of the number of retained PCs.**\n",
        "\n",
        "By doing so we can determine how we wish to trade off compression (i.e., few PCs or \"eigenfaces\" retained in the dimensionality reduction) against information preservation (i.e., fraction of variance captured within that reduced dimensional projection)."
      ],
      "metadata": {
        "id": "U0y6SA75fyzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cumulative variance\n",
        "cum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(cum_var, lw=2)\n",
        "plt.axhline(0.95, color='r', linestyle='--', label='95% variance')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance vs Number of Components')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "n_needed_80 = np.searchsorted(cum_var, 0.80) + 1\n",
        "print(f\"Number of PCs needed for 80% variance: {n_needed_80}\")\n",
        "n_needed_90 = np.searchsorted(cum_var, 0.90) + 1\n",
        "print(f\"Number of PCs needed for 90% variance: {n_needed_90}\")\n",
        "n_needed_95 = np.searchsorted(cum_var, 0.95) + 1\n",
        "print(f\"Number of PCs needed for 95% variance: {n_needed_95}\")\n",
        "n_needed_99 = np.searchsorted(cum_var, 0.99) + 1\n",
        "print(f\"Number of PCs needed for 99% variance: {n_needed_99}\")\n"
      ],
      "metadata": {
        "id": "Mc-hBFGsfwgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Finally, let's do some compression/decompression.**\n",
        "\n",
        "Remember how we discussed that dimensionality can be useful for storing and sending information at lower memory cost of file size? We can use PCA to make a compressed representation of each image, perhaps send it over email at much lower data transmission cost, and then our recipient can reconstruct the image within the basis of PCs.\n",
        "\n",
        "Let's see how accurately we can reconstruct each image for different numbers of PCs. We are testing how \"lossy\" our compression/decompression algorithm is in preserving the important features and information in the image."
      ],
      "metadata": {
        "id": "QRM32YmBfzDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "\n",
        "# List of selected people\n",
        "selected_people = [\n",
        "    'Angelina Jolie',\n",
        "    'Arnold Schwarzenegger',\n",
        "    'David Beckham',\n",
        "    'Halle Berry',\n",
        "    'Lleyton Hewitt',\n",
        "    'Michael Schumacher',\n",
        "    'Nicole Kidman',\n",
        "    'Serena Williams'\n",
        "]\n",
        "\n",
        "# Find one image index per selected person\n",
        "selected_indices = []\n",
        "for person in selected_people:\n",
        "    idx = np.where(lfw.target_names[lfw.target] == person)[0]\n",
        "    if len(idx) > 0:\n",
        "        selected_indices.append(idx[0])\n",
        "    else:\n",
        "        print(f\"Warning: {person} not found in dataset\")\n",
        "\n",
        "def interactive_pca_reconstruction(n_components=50):\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    # Fit PCA with n_components\n",
        "    pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True, random_state=42)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "    X_reconstructed = pca.inverse_transform(X_pca)\n",
        "    images_reconstructed = scaler.inverse_transform(X_reconstructed).reshape((n_samples, h, w))\n",
        "\n",
        "    n_selected = len(selected_indices)\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        # Original image\n",
        "        plt.subplot(2, n_selected, i + 1)\n",
        "        plt.imshow(images[idx], cmap='gray')\n",
        "        plt.title(f\"{lfw.target_names[lfw.target[idx]]}\\nOriginal\", fontsize=10)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Reconstructed image\n",
        "        plt.subplot(2, n_selected, n_selected + i + 1)\n",
        "        plt.imshow(images_reconstructed[idx], cmap='gray')\n",
        "        plt.title(f\"Reconstructed\", fontsize=10)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Reconstruction of Selected Faces with {n_components} PCs\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compression ratio = reduced dim / original dim\n",
        "    compression_ratio = n_components / X.shape[1]\n",
        "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
        "\n",
        "    print(f\"Compression ratio (reduced dim / original dim): {compression_ratio:.2%}\")\n",
        "    print(f\"Variance explained by {n_components} PCs: {explained_variance:.2%}\")\n",
        "\n",
        "# Interactive slider widget\n",
        "slider = widgets.IntSlider(\n",
        "    value=50,\n",
        "    min=1,\n",
        "    max=450,#min(n_samples, X_scaled.shape[1]),\n",
        "    step=1,\n",
        "    description='Number PCs:',\n",
        "    continuous_update=False,\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "widgets.interact(interactive_pca_reconstruction, n_components=slider)\n"
      ],
      "metadata": {
        "id": "Hi0SlW-efwma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8GZqdXRS3kR"
      },
      "source": [
        "## Other Dimensionality Reduction Algorithms\n",
        "\n",
        "The scikit-learn library contains many other unsupervised dimensionality reduction routines. Here are a few popular ones:\n",
        "\n",
        "- [sklearn.decomposition.PCA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.PCA.html):     \n",
        "Principal Component Analysis (PCA).\n",
        "- [sklearn.decomposition.RandomizedPCA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.RandomizedPCA.html):  \n",
        "Extremely fast approximate PCA implementation based on a randomized algorithm.\n",
        "- [sklearn.decomposition.SparsePCA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.SparsePCA.html):  \n",
        "PCA variant including L1 penalty for sparsity.\n",
        "- [sklearn.decomposition.FastICA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.FastICA.html):  \n",
        "Independent Component Analysis (ICA).\n",
        "- [sklearn.decomposition.NMF](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.NMF.html):  \n",
        "Non-negative Matrix Factorization (NMF).\n",
        "- [sklearn.manifold.LocallyLinearEmbedding](http://scikit-learn.org/0.13/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html):  \n",
        "Locally linear embedding (LLE) nonlinear manifold learning based on local neighborhood geometry.\n",
        "- [sklearn.manifold.IsoMap](http://scikit-learn.org/0.13/modules/generated/sklearn.manifold.Isomap.html):  \n",
        "Nonlinear manifold learning technique based on a sparse graph algorithm.\n",
        "   \n",
        "Each of these has its own strengths & weaknesses, and areas of application. You can read about them on the [scikit-learn website](http://sklearn.org)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "c1_mgS-EVyv8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kScTbDFfS8Ic"
      },
      "source": [
        "# Unsupervised Clustering w/ K-Means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-P7sLWYS8Id"
      },
      "source": [
        "We recall that k-means is an algorithm for **unsupervised clustering**. Our data do not have labels, so we are seeking to understand if there is some underlying structure in the data in terms of the emergence of clusters. We wish to allow the data to \"speak for itself\".\n",
        "\n",
        "K-means is a relatively easy-to-understand algorithm.  It searches for cluster centers which are the mean of the points within them, such that every point is closest to the cluster center it is assigned to."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "\n",
        "Let's start by taking a look at a simple 2D dataset that contains 4 clusters that are easily detectable by eye."
      ],
      "metadata": {
        "id": "VyVRKHDDqrqC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7vBPqj6S8Id"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=4,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRpMA500S8Id"
      },
      "source": [
        "## K-Means\n",
        "\n",
        "By eye, it is relatively easy to pick out the four clusters.\n",
        "\n",
        "For a computer, an exhaustive search of all possible different partitionings of the 300 data points would be extremely slow and ineficient.\n",
        "\n",
        "Fortunately, k-means can be solved (approximately) using an  *expectation-maximization (EM)* procedure which you can think of as operating conceptually similar to gradient descent.\n",
        "\n",
        ">1. Guess some initial points to serve as our $k$ cluster centers (\"centroid\").\n",
        "2. Assign all points to their nearest centroid to define our clusters.\n",
        "3. Update the cluster centroids by taking the mean over all points assigned to each cluster.\\*\n",
        "4. Repeat steps 2 and 3 until convergence (i.e., the cluster assignments no longer change).\n",
        "\n",
        "\\* In general, the mean of the data in a cluster will not correspond to any particular point in the data set. This may not be desirable in some applications (e.g., our data are discrete dollar values and we prefer not to have a cluster centroid that is a non-integer number of dollars). In such cases, we can use *k-medoids* instead of *k-means*, where the only change is that we compute centroids as the *medoid* data point (the point in the cluster that minimizes distances to all other points in the cluster) as opposed to the *mean* data point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZbo3cSyS8Ie"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "est = KMeans(4)  # 4 clusters\n",
        "est.fit(X)\n",
        "y_kmeans = est.predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='rainbow');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKOuuX_0S8Ie"
      },
      "source": [
        "The algorithm identifies the four clusters of points in a manner very similar to what we would do by eye!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-TDaC4FS8Ie"
      },
      "source": [
        "## Visualizing K-Means in Action\n",
        "\n",
        "Let's visualize the k-means algorithm in action."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Setup\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML, display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# Step 2: Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Step 3: K-means with history tracking (no trails)\n",
        "def kmeans_with_history(X, k, seed=42, max_iter=20):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    centroids = X[rng.choice(X.shape[0], size=k, replace=False)]\n",
        "    history = []\n",
        "\n",
        "    for _ in range(max_iter):\n",
        "        # E-step: Assign clusters\n",
        "        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n",
        "        labels = np.argmin(distances, axis=1)\n",
        "\n",
        "        # Record state\n",
        "        history.append((centroids.copy(), labels.copy()))\n",
        "\n",
        "        # M-step: Update centroids\n",
        "        new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(k)])\n",
        "\n",
        "        # Stop if converged\n",
        "        if np.allclose(centroids, new_centroids):\n",
        "            break\n",
        "\n",
        "        centroids = new_centroids\n",
        "\n",
        "    return history\n",
        "\n",
        "# Step 4: Animation function with WCSS annotation\n",
        "def animate_kmeans(k, seed):\n",
        "    history = kmeans_with_history(X, k, seed)\n",
        "    fig, ax = plt.subplots(figsize=(6, 6))\n",
        "    colors = plt.cm.tab10.colors\n",
        "\n",
        "    def update(i):\n",
        "        ax.clear()\n",
        "        centroids, labels = history[i]\n",
        "\n",
        "        # Plot each cluster and its centroid\n",
        "        for cluster_id in range(k):\n",
        "            points = X[labels == cluster_id]\n",
        "            ax.scatter(points[:, 0], points[:, 1], s=30,\n",
        "                       color=colors[cluster_id % 10],\n",
        "                       label=f\"Cluster {cluster_id}\")\n",
        "            ax.scatter(*centroids[cluster_id], marker='X', color='black', s=200, edgecolor='white')\n",
        "\n",
        "        # Compute WCSS for current clustering\n",
        "        wcss = np.sum([\n",
        "            np.sum((X[labels == cluster_id] - centroids[cluster_id]) ** 2)\n",
        "            for cluster_id in range(k)\n",
        "        ])\n",
        "\n",
        "        ax.set_title(f\"K-Means Iteration {i + 1}\")\n",
        "        ax.set_xlim(X[:, 0].min() - 1, X[:, 0].max() + 1)\n",
        "        ax.set_ylim(X[:, 1].min() - 1, X[:, 1].max() + 1)\n",
        "        ax.legend(loc='best')\n",
        "\n",
        "        # Add WCSS text in bottom-right\n",
        "        ax.text(0.95, 0.02, f\"WCSS = {wcss:.2f}\",\n",
        "                transform=ax.transAxes,\n",
        "                ha='right', va='bottom',\n",
        "                fontsize=10, color='darkred',\n",
        "                bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=len(history), interval=1000, repeat=True)\n",
        "    plt.close()\n",
        "    display(HTML(ani.to_jshtml()))\n",
        "\n",
        "# Step 5: Interactive interface using value input boxes\n",
        "k_input = widgets.BoundedIntText(\n",
        "    value=4,\n",
        "    min=1,\n",
        "    max=10,\n",
        "    step=1,\n",
        "    description='Number of clusters, k =',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "seed_input = widgets.IntText(\n",
        "    value=42,\n",
        "    description='Random number seed =',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "ui = widgets.VBox([k_input, seed_input])\n",
        "out = widgets.interactive_output(animate_kmeans, {'k': k_input, 'seed': seed_input})\n",
        "display(ui, out)\n"
      ],
      "metadata": {
        "id": "dX7Vauigsquh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-run the code to explore what happens for different choices of $k$ and the random seed used to initialize the initial guess for our centroids.\n",
        "\n",
        "* Do you always get the same results for different random seeds?"
      ],
      "metadata": {
        "id": "AQSReCSYtpsZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fGqJlFRS8If"
      },
      "source": [
        "The convergence of k-means is not guaranteed, and for that reason `sklearn` by default uses a large number of random initializations and chooses the best clustering that minimizes the within cluster sum of squares (WCSS) corresponding to the K-means clustering **objective function**.\n",
        "\n",
        "* The WCSS is reported in the bottom-right of the plot. Does a lower WCSS correspond to how you would visually judge the quality of the clustering?\n",
        "* What value of WCSS do you get for k = 4 with a random seed of 42?\n",
        "* What value of WCSS do you get for k = 4 with a random seed of 43?\n",
        "* Which random seed (i.e., random initialization of cluster centers) yields a better clustering of the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choosing $k$ via Silhouette Score\n",
        "\n",
        "The value of $k$ is a **hyperparameter**, the number of clusters must be set by the user beforehand and is not learned from the data. There are other clustering algorithms for which $k$ can be learned requirement may be lifted.\n",
        "\n",
        "One way to help us choose $k$ is to determine how well the points have been clustered after learning is complete. One way to do this is by computing the **silhouette score**.\n",
        "\n",
        "We compute the silhouette score as follows:\n",
        "\n",
        ">1. Compute for each point the intra-cluster distance $a$ -- this is the average distance to all other points within the same cluster and represents how well the point fits into its own cluster.\n",
        "2. Compute for each point the nearest-cluster distance $b$ -- this is the average distance from the point to all points in its nearest neighboring cluster and represents how well the point fits into the other cluster, not its assigned one.\n",
        "3. Compute for each point the silhouette score $(b - a) / max(a, b)$:\n",
        "* $+1$: Ideal clustering, where each point is far from other clusters and close to its own.\n",
        "* $0$: The point is close to the decision boundary between two clusters.\n",
        "* $-1$: The point is likely misclassified and should be in another cluster.\n",
        "4. Average the silhouette scores over all points.\n",
        "\n",
        "By graphing the average silhouette score as a function of $k$, we can try to identify an optimal number of clusters by identifying a **global maximum**, a **local maximum** or a **shoulder/knee**.\n",
        "\n",
        "Let's try it."
      ],
      "metadata": {
        "id": "_-SHV-dtu3P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Range of k values to test\n",
        "k_values = range(2, 16)\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot silhouette scores\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_values, silhouette_scores, marker='o')\n",
        "plt.title(\"Silhouette Score vs Number of Clusters\")\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Silhouette score\")\n",
        "plt.xticks(k_values)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print best k\n",
        "best_k = k_values[np.argmax(silhouette_scores)]\n",
        "print(f\"Best k by silhouette score: {best_k} (score = {max(silhouette_scores):.3f})\")\n"
      ],
      "metadata": {
        "id": "aYMOJxyjukLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA + K-Means Clustering -- AT&T Faces Data"
      ],
      "metadata": {
        "id": "VZD8k828wncc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course k-means can be applied to much higher dimensional datasets with much larger numbers of clusters. Although, of course, visualization becomes much more difficult.\n",
        "\n",
        "One profitable and common way to perform unsupervised data exploration and analysis is to combine PCA dimensionality reduction -- to help simplify the data, extract its inherent structure, and suppress noise -- with k-means clustering -- to extract clusters within the data.\n",
        "\n",
        "Let's do so for the AT&T faces data set.\n",
        "\n",
        "> Credit: AT&T Laboratories Cambridge [https://cam-orl.co.uk/facedatabase.html](https://cam-orl.co.uk/facedatabase.html)\n"
      ],
      "metadata": {
        "id": "ZL-6bHa1xUvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Let's import the data.**"
      ],
      "metadata": {
        "id": "o3COqd-qxmzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load list of image paths\n",
        "url_list = \"https://raw.githubusercontent.com/andrewlferguson/MENG15100/main/data/att_faces/att_faces_image_files.txt\"\n",
        "response = requests.get(url_list)\n",
        "raw_paths = response.text.strip().split(\"\\n\")\n",
        "\n",
        "# Step 2: Clean paths to remove duplicated base prefix\n",
        "image_paths = [p.replace(\"data/att_faces/\", \"\") for p in raw_paths]\n",
        "\n",
        "# Step 3: Set target size (downscale to 46x56)\n",
        "target_size = (46, 56)  # (width, height)\n",
        "\n",
        "# Step 4: Load, resize, and flatten images into vector format\n",
        "base_url = \"https://raw.githubusercontent.com/andrewlferguson/MENG15100/main/data/att_faces/\"\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "for path in image_paths:\n",
        "    full_url = base_url + path\n",
        "    try:\n",
        "        img_response = requests.get(full_url)\n",
        "        img = Image.open(BytesIO(img_response.content)).convert(\"L\")  # grayscale\n",
        "        img = img.resize(target_size, Image.LANCZOS)  # high-quality downsampling\n",
        "        img_array = np.array(img)\n",
        "        images.append(img_array.flatten())  # flatten 2D image to 1D vector\n",
        "\n",
        "        label = path.split(\"/\")[0]  # e.g., 's1', 's2', etc.\n",
        "        labels.append(label)\n",
        "\n",
        "    except UnidentifiedImageError:\n",
        "        print(f\"❌ Could not identify image file at: {full_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error loading {full_url}: {e}\")\n",
        "\n",
        "# Step 5: Convert lists to arrays\n",
        "X = np.array(images)        # shape (n_samples, width*height)\n",
        "labels = np.array(labels)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(f\"✅ Loaded {n_samples} images with {n_features} features each (flattened {target_size[1]}x{target_size[0]})\")\n",
        "print(f\"👤 Number of unique people: {len(np.unique(labels))}\")\n",
        "\n",
        "# Step 6: Show random samples (reshaped back to image shape)\n",
        "num_samples_to_show = 10\n",
        "np.random.seed(42)\n",
        "random_indices = np.random.choice(n_samples, size=num_samples_to_show, replace=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i, idx in enumerate(random_indices):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(X[idx].reshape(target_size[1], target_size[0]), cmap=\"gray\")\n",
        "    plt.title(f\"Image #{idx}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"Random Subsample of Loaded ATT Face Images\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wCPm_j_du6GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Now let's run PCA and keep a number of PCs to preserve 95% of the variance.**"
      ],
      "metadata": {
        "id": "gf3aknCmxzb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA to capture n% of variance\n",
        "pca = PCA(n_components=0.99, svd_solver='full', whiten=True, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"PCA reduced data from {X_scaled.shape[1]} to {X_pca.shape[1]} dimensions.\")\n"
      ],
      "metadata": {
        "id": "sXQAWcNXxwEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Now we will run k-means clustering and silhouette score analysis.**"
      ],
      "metadata": {
        "id": "DKSZX-VFzz8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_scores = []\n",
        "k_range = range(3, 51)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X_pca)\n",
        "    score = silhouette_score(X_pca, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot silhouette scores\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(k_range, silhouette_scores, marker='o')\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Silhouette score\")\n",
        "plt.title(\"Silhouette Score vs Number of Clusters (PCA space)\")\n",
        "plt.grid(True)\n",
        "plt.xticks(range(k_range.start, k_range.stop, 2))\n",
        "plt.show()\n",
        "\n",
        "# Select best k\n",
        "best_k = k_range[np.argmax(silhouette_scores)]\n",
        "print(f\"Best k by silhouette score: {best_k}\")\n"
      ],
      "metadata": {
        "id": "t6GbX1LKyMTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_k = 36"
      ],
      "metadata": {
        "id": "Vm_0sVwk1TPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Perform k-means clustering at user-selected $k$.**"
      ],
      "metadata": {
        "id": "XC_fhz-D0ENA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
        "print(\"\\nCluster membership counts:\")\n",
        "for cluster_id, count in zip(unique, counts):\n",
        "    print(f\"Cluster {cluster_id}: {count}\")\n"
      ],
      "metadata": {
        "id": "nUMfq2GZz4Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Show some illustrative images from each cluster.**"
      ],
      "metadata": {
        "id": "OunARLWK0Lem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "num_clusters = len(np.unique(cluster_labels))\n",
        "max_per_cluster = 5\n",
        "\n",
        "plt.figure(figsize=(max_per_cluster * 3, num_clusters * 3))\n",
        "\n",
        "for cluster_id in range(num_clusters):\n",
        "    # Get indices of images in this cluster\n",
        "    cluster_indices = np.where(cluster_labels == cluster_id)[0]\n",
        "\n",
        "    # Randomly sample up to 5 images from this cluster\n",
        "    sample_size = min(max_per_cluster, len(cluster_indices))\n",
        "    sampled_indices = np.random.choice(cluster_indices, size=sample_size, replace=False)\n",
        "\n",
        "    for i, img_idx in enumerate(sampled_indices):\n",
        "        plt_idx = cluster_id * max_per_cluster + i + 1\n",
        "        plt.subplot(num_clusters, max_per_cluster, plt_idx)\n",
        "        plt.imshow(X[img_idx].reshape(target_size[1], target_size[0]), cmap='gray')\n",
        "        plt.title(f\"Cluster {cluster_id}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zqFSQtOk0ImU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Clustering Algorithms\n",
        "\n",
        "The scikit-learn library contains a variety of unsupervised clustering algorithms. Here are a few popular ones:\n",
        "\n",
        "- [`sklearn.cluster.KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html):  \n",
        "  Classic centroid-based clustering using the k-means algorithm.\n",
        "\n",
        "- [`sklearn.cluster.MiniBatchKMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html):  \n",
        "  Faster, approximate version of KMeans suitable for large datasets.\n",
        "\n",
        "- [`sklearn_extra.cluster.KMedoids`](https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html):  \n",
        "  Cluster centers defined using medoids as opposed to means.\n",
        "\n",
        "- [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html):  \n",
        "  Density-based clustering that can find arbitrarily shaped clusters and outliers.\n",
        "\n",
        "- [`sklearn.cluster.AgglomerativeClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html):  \n",
        "  Hierarchical clustering using bottom-up agglomeration.\n",
        "\n",
        "- [`sklearn.cluster.MeanShift`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html):  \n",
        "  Clustering based on kernel density estimation and mode seeking.\n",
        "\n",
        "- [`sklearn.cluster.SpectralClustering`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html):  \n",
        "  Graph-based clustering method using the spectrum (eigenvalues) of a similarity matrix.\n",
        "\n",
        "- [`sklearn.cluster.Birch`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html):  \n",
        "  Balanced Iterative Reducing and Clustering using Hierarchies; designed for large datasets.\n",
        "\n",
        "- [`sklearn.mixture.GaussianMixture`](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html):  \n",
        "  Probabilistic model-based clustering using Gaussian Mixture Models (GMMs).\n",
        "\n",
        "Each of these algorithms has different assumptions, advantages, and use cases. You can learn more about them and see usage examples on the [official scikit-learn website](https://scikit-learn.org/stable/modules/clustering.html).\n"
      ],
      "metadata": {
        "id": "tMZXVuKQiXv5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e494Tc5slscV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "d3d699a5916e1aff8848f10bc57a89f93bc1cf3a71c02fefcecb8a901731c5d8"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}