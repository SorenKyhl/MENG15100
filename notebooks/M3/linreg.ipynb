{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Acknowledgements"
      ],
      "metadata": {
        "id": "zbOcojMuQrYw"
      },
      "id": "zbOcojMuQrYw"
    },
    {
      "cell_type": "markdown",
      "id": "b4040901",
      "metadata": {
        "id": "b4040901"
      },
      "source": [
        "Notebook developed by Andrew Ferguson for the UChicago course MENG15100: Machine Learning and Artificial Intelligence for Molecular Discovery and Engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b1b7562"
      },
      "source": [
        "## Data"
      ],
      "id": "8b1b7562"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58a13340"
      },
      "source": [
        "Let's start by importing some training data from a csv file using pandas.\n",
        "\n",
        "We will start working with a simple data set with one input variable $x$ and one output variable $y$."
      ],
      "id": "58a13340"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace with the actual raw GitHub URL of your file\n",
        "github_raw_url = 'https://raw.githubusercontent.com/andrewlferguson/MENG15100/main/notebooks/M3/linreg.csv'\n",
        "\n",
        "# Load the data into a Pandas DataFrame\n",
        "df = pd.read_csv(github_raw_url)\n",
        "\n",
        "# Show the data\n",
        "print(df)"
      ],
      "metadata": {
        "id": "SWxtHUlIU4hh"
      },
      "id": "SWxtHUlIU4hh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is our goal to fit a linear regression model to these data of the form $y = \\phi_0 + \\phi_1 x$.\n",
        "\n",
        "Recall that $\\{\\phi_0, \\phi_1\\}$ are the trainable parameters of our model, $x$ is the input data, and $y$ is the output data.\n",
        "\n",
        "In this simple example, mainly to make plotting easy, the $x$ data are just one dimensional (i.e., there is a single $x$ feature associated with each of the 18 instances or in the training data)."
      ],
      "metadata": {
        "id": "cnE3cTU5B8WB"
      },
      "id": "cnE3cTU5B8WB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human Learning"
      ],
      "metadata": {
        "id": "xRTrNKnXB21S"
      },
      "id": "xRTrNKnXB21S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's do some graphical exploration and \"human learning\" where we will guess the optimal values of the linear regression fitting parameters $\\phi_0$ (intercept) and $\\phi_1$ (slope).\n",
        "\n",
        "We will use matplotlib to make an interactive plot."
      ],
      "metadata": {
        "id": "cHhbYs4Z_kD3"
      },
      "id": "cHhbYs4Z_kD3"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from ipywidgets import interact, FloatSlider\n",
        "\n",
        "# Replace with the actual raw GitHub URL of your file\n",
        "github_raw_url = 'https://raw.githubusercontent.com/andrewlferguson/MENG15100/main/notebooks/M3/linreg.csv'\n",
        "\n",
        "# Load the data into a Pandas DataFrame\n",
        "df = pd.read_csv(github_raw_url)\n",
        "\n",
        "# Load CSV data\n",
        "x = df['x'].values\n",
        "y = df['y'].values\n",
        "\n",
        "# Define the interactive plot function\n",
        "def plot_line(slope=2.0, intercept=1.0):\n",
        "    y_line = slope * x + intercept\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.scatter(x, y, color='blue', label='Data')\n",
        "    plt.plot(x, y_line, color='red', label=f'y = {slope:.2f}x + {intercept:.2f}')\n",
        "    plt.xlabel('x')\n",
        "    plt.ylabel('y')\n",
        "    plt.title('Interactive Linear Fit')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Sliders for slope and intercept\n",
        "interact(\n",
        "    plot_line,\n",
        "    slope=FloatSlider(value=2.0, min=-10, max=10, step=0.1, description='Slope'),\n",
        "    intercept=FloatSlider(value=1.0, min=-10, max=10, step=0.1, description='Intercept')\n",
        ")\n"
      ],
      "metadata": {
        "id": "nMN1a0fb_hJe"
      },
      "id": "nMN1a0fb_hJe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What appears to be a good choice for $\\phi_0$ (intercept) and $\\phi_1$ (slope)?\n",
        "\n",
        "Enter them here:"
      ],
      "metadata": {
        "id": "296ip4m1CuH5"
      },
      "id": "296ip4m1CuH5"
    },
    {
      "cell_type": "code",
      "source": [
        "phi_0_HUMAN = 0.0\n",
        "phi_1_HUMAN = 0.0"
      ],
      "metadata": {
        "id": "DrHKcvWhC4fl"
      },
      "id": "DrHKcvWhC4fl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning"
      ],
      "metadata": {
        "id": "BwlO_6DRCrLk"
      },
      "id": "BwlO_6DRCrLk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "OK, now let's let the machine do the work by asking it to find the $\\boldsymbol\\phi = \\{\\phi_0,\\phi_1\\}$ that minimize the least squares loss function $L[\\boldsymbol\\phi] = \\sum_i (\\hat y_i - y_i)^2$.\n",
        "\n",
        "We recall that the loss function is the sum of square residuals: $y_i$ is the true value of $y$ in data instance $i$ and $\\hat y_i = f(x_i) = \\phi_0 + \\phi_1 x_i$ is the prediction of our model.\n",
        "\n",
        "The learning algorithm will optimize $\\boldsymbol\\phi = \\{\\phi_0,\\phi_1\\}$ by finding the lowest point on the loss lanscape $L[\\boldsymbol\\phi]$.\n",
        "\n",
        "One way to find the minimum is by gradient descent and this is the method of choice for large machine learning problems. For linear regression, it turns out that there is a closed form analytical solution that we can use to get there more efficiently, but we wind up in the same place either way.\n",
        "\n",
        "We will use scikit-learn (sklearn) to solve the problem for us."
      ],
      "metadata": {
        "id": "ZOEBwu_8NtPi"
      },
      "id": "ZOEBwu_8NtPi"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Load CSV data\n",
        "x = df['x'].values.reshape(-1, 1)\n",
        "y = df['y'].values.reshape(-1, 1)\n",
        "\n",
        "# Fit linear regression\n",
        "model = LinearRegression()\n",
        "model.fit(x, y)\n",
        "y_pred = model.predict(x)\n",
        "\n",
        "# Get slope and intercept\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Calculate residuals and loss\n",
        "residuals = y - y_pred\n",
        "squared_loss = np.sum(residuals**2)\n",
        "\n",
        "# Plot data, best-fit line, and residuals\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(x, y, color='blue', label='Data Points')\n",
        "plt.plot(x, y_pred, color='red', label='Best Fit Line')\n",
        "\n",
        "# Plot residuals\n",
        "for xi, yi, ypi in zip(x, y, y_pred):\n",
        "    plt.vlines(xi, ypi, yi, color='gray', linestyle='dashed')\n",
        "\n",
        "# Annotate least squares loss\n",
        "plt.text(0.05, 0.95, f'Least Squares Loss = {squared_loss:.2f}',\n",
        "         transform=plt.gca().transAxes,\n",
        "         fontsize=12,\n",
        "         verticalalignment='top',\n",
        "         bbox=dict(boxstyle='round,pad=0.4', facecolor='lightyellow', edgecolor='black'))\n",
        "\n",
        "# Final touches\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Regression with Residuals and Least Squares Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Reporting out best fit slope and intercept\n",
        "print(f\"Best Fit Slope (phi_1): {slope[0]:.2f}\")\n",
        "print(f\"Best Fit Intercept (phi_0): {intercept[0]:.2f}\")"
      ],
      "metadata": {
        "id": "R0XoScJQ_5AH"
      },
      "id": "R0XoScJQ_5AH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing nonlinearities"
      ],
      "metadata": {
        "id": "mOt6N6xgSCic"
      },
      "id": "mOt6N6xgSCic"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now consider a more interesting data set that exhibits a nonlinear relationship in $y = f(x)$ and that is therefore poorly fitted by a linear model $y = \\phi_0 + \\phi_1 x$.\n",
        "\n",
        "Remarkably, we can use all of the tools of linear regression to make a nonlinear fit. How? We can trick the algorithm to operate on nonlinear features. We add nonlinearities not into the model, but ask a linear model to operate on nonlinear features."
      ],
      "metadata": {
        "id": "R0KiBILFTmIc"
      },
      "id": "R0KiBILFTmIc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1\n",
        "\n",
        "In this first example case, we will stick to a 1D input variable $x$ and introduce nonlinearities by constucting additional features that are nonlinear versions of $x$.\n",
        "\n",
        "We will allow different features to be turned on and off, and the most complex model will look like:\n",
        "\n",
        "$y = \\phi_0 + \\phi_1 x + \\phi_2 x^2 + \\phi_3 x^3 + \\phi_4 x^4 + \\phi_5 \\ln(x) + \\phi_6 \\sin(x) + \\phi_7 \\cos(x) $"
      ],
      "metadata": {
        "id": "SHnKQRKYVxPc"
      },
      "id": "SHnKQRKYVxPc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ipywidgets in Colab\n",
        "!pip install ipywidgets --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# --- Generate a nonlinear dataset ---\n",
        "np.random.seed(0)\n",
        "x = np.linspace(1, 10, 100)\n",
        "y = 2 * x**2 - 3 * x + 4*np.sin(x) + np.random.normal(scale=5, size=len(x))  # Nonlinear function\n",
        "\n",
        "df = pd.DataFrame({'x': x, 'y': y})\n",
        "\n",
        "# --- Define available transformations ---\n",
        "transformations = {\n",
        "    'x': lambda x: x,\n",
        "    'x^2': lambda x: x**2,\n",
        "    'x^3': lambda x: x**3,\n",
        "    'x^4': lambda x: x**4,\n",
        "    'ln(x)': lambda x: np.log(x),\n",
        "    'sin(x)': lambda x: np.sin(x),\n",
        "    'cos(x)': lambda x: np.cos(x)\n",
        "}\n",
        "\n",
        "# --- Create checkboxes for each term ---\n",
        "checkboxes = {\n",
        "    name: widgets.Checkbox(value=(name in ['x']), description=name)\n",
        "    for name in transformations\n",
        "}\n",
        "\n",
        "# --- Output area for plot ---\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Generate selected features matrix ---\n",
        "def get_selected_features(x):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for name, func in transformations.items():\n",
        "        if checkboxes[name].value:\n",
        "            features.append(func(x))\n",
        "            labels.append(name)\n",
        "    if not features:\n",
        "        return None, None\n",
        "    X = np.vstack(features).T\n",
        "    return X, labels\n",
        "\n",
        "# --- Plotting and regression update function ---\n",
        "def update_plot(change=None):\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        X, labels = get_selected_features(df['x'].values)\n",
        "        if X is None:\n",
        "            print(\"Please select at least one feature.\")\n",
        "            return\n",
        "\n",
        "        y = df['y'].values\n",
        "\n",
        "        model = LinearRegression()\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict(X)\n",
        "        residuals = y - y_pred\n",
        "        loss = np.sum(residuals ** 2)\n",
        "\n",
        "        # Sort for plotting smooth lines\n",
        "        x_vals = df['x'].values\n",
        "        sort_idx = np.argsort(x_vals)\n",
        "        x_sorted = x_vals[sort_idx]\n",
        "        y_pred_sorted = y_pred[sort_idx]\n",
        "\n",
        "        # --- Plot ---\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(x_vals, y, color='blue', label='Data Points')\n",
        "        plt.plot(x_sorted, y_pred_sorted, color='red', label='Model Fit')\n",
        "\n",
        "        # Residuals\n",
        "        for xi, yi, ypi in zip(x_vals, y, y_pred):\n",
        "            plt.vlines(xi, yi, ypi, color='gray', linestyle='dashed')\n",
        "\n",
        "        # Coefficient annotation\n",
        "        coef_str = '\\n'.join([f'{lbl} = {coef:.2f}' for lbl, coef in zip(labels, model.coef_)])\n",
        "        plt.text(0.05, 0.95,\n",
        "                 f'Intercept = {model.intercept_:.2f}\\n' + coef_str + f'\\nLoss = {loss:.2f}',\n",
        "                 transform=plt.gca().transAxes,\n",
        "                 fontsize=11,\n",
        "                 verticalalignment='top',\n",
        "                 bbox=dict(boxstyle='round,pad=0.4', facecolor='lightyellow', edgecolor='black'))\n",
        "\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.title(\"Custom Linear Regression with Nonlinear Data\")\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# --- Attach update listeners ---\n",
        "for cb in checkboxes.values():\n",
        "    cb.observe(update_plot, names='value')\n",
        "\n",
        "# --- Display the GUI ---\n",
        "selectors = widgets.VBox(list(checkboxes.values()))\n",
        "display(widgets.HBox([selectors, output]))\n",
        "update_plot()\n"
      ],
      "metadata": {
        "id": "PeSjpkJFPgIC"
      },
      "id": "PeSjpkJFPgIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2\n",
        "\n",
        "In this second example case, we will consider a 2D input variable $\\boldsymbol x = \\{x_1,x_2\\}$ and introduce nonlinearities by constucting additional features that are nonlinear versions of $x_1$ and $x_2$.\n",
        "\n",
        "We will allow different features to be turned on and off, and the most complex model will look like:\n",
        "\n",
        "$y = \\phi_0 + \\phi_1 x_1 + \\phi_2 x_2 + \\phi_3 x_1^2 + \\phi_4 x_2^2 + \\phi_5 x_1 x_2 + \\phi_6 \\sin(x_1) + \\phi_7 \\sin(x_2) + \\phi_8 sin(x_1 x_2)$\n",
        "\n",
        "Of course now we will need to make a 3D plot since we have one output variable $y$ and two input variables $x_1$ and $x_2$."
      ],
      "metadata": {
        "id": "j-R5rXQ_V2TK"
      },
      "id": "j-R5rXQ_V2TK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ipywidgets in Colab\n",
        "!pip install ipywidgets --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# --- Generate nonlinear 2D data ---\n",
        "np.random.seed(0)\n",
        "x1 = np.linspace(-3, 3, 30)\n",
        "x2 = np.linspace(-3, 3, 30)\n",
        "X1, X2 = np.meshgrid(x1, x2)\n",
        "x1_flat = X1.flatten()\n",
        "x2_flat = X2.flatten()\n",
        "\n",
        "# True function (nonlinear)\n",
        "y = 1.5 * x1_flat - 2.0 * x2_flat + 0.8 * x1_flat * x2_flat + np.sin(x1_flat) + np.random.normal(0, 1, x1_flat.shape[0])\n",
        "\n",
        "df = pd.DataFrame({'x1': x1_flat, 'x2': x2_flat, 'y': y})\n",
        "\n",
        "# --- Define transformations ---\n",
        "transformations = {\n",
        "    'x1': lambda x1, x2: x1,\n",
        "    'x2': lambda x1, x2: x2,\n",
        "    'x1^2': lambda x1, x2: x1**2,\n",
        "    'x2^2': lambda x1, x2: x2**2,\n",
        "    'x1*x2': lambda x1, x2: x1 * x2,\n",
        "    'sin(x1)': lambda x1, x2: np.sin(x1),\n",
        "    'sin(x2)': lambda x1, x2: np.sin(x2),\n",
        "    'sin(x1*x2)': lambda x1, x2: np.sin(x1 * x2)\n",
        "}\n",
        "\n",
        "# --- Create checkboxes for terms ---\n",
        "checkboxes = {\n",
        "    name: widgets.Checkbox(value=(name in ['x1', 'x2']), description=name)\n",
        "    for name in transformations\n",
        "}\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Feature generation ---\n",
        "def get_selected_features(x1, x2):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for name, func in transformations.items():\n",
        "        if checkboxes[name].value:\n",
        "            features.append(func(x1, x2))\n",
        "            labels.append(name)\n",
        "    if not features:\n",
        "        return None, None\n",
        "    X = np.vstack(features).T\n",
        "    return X, labels\n",
        "\n",
        "# --- Plotting and regression update ---\n",
        "def update_plot(change=None):\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        x1_vals = df['x1'].values\n",
        "        x2_vals = df['x2'].values\n",
        "        y_vals = df['y'].values\n",
        "\n",
        "        X, labels = get_selected_features(x1_vals, x2_vals)\n",
        "        if X is None:\n",
        "            print(\"Please select at least one feature.\")\n",
        "            return\n",
        "\n",
        "        # Fit regression\n",
        "        model = LinearRegression()\n",
        "        model.fit(X, y_vals)\n",
        "        y_pred = model.predict(X)\n",
        "        residuals = y_vals - y_pred\n",
        "        loss = np.sum(residuals ** 2)\n",
        "\n",
        "        # For plotting surface\n",
        "        x1_grid = np.linspace(-3, 3, 40)\n",
        "        x2_grid = np.linspace(-3, 3, 40)\n",
        "        X1g, X2g = np.meshgrid(x1_grid, x2_grid)\n",
        "        X1g_flat = X1g.flatten()\n",
        "        X2g_flat = X2g.flatten()\n",
        "\n",
        "        X_grid, _ = get_selected_features(X1g_flat, X2g_flat)\n",
        "        y_grid_pred = model.predict(X_grid).reshape(X1g.shape)\n",
        "\n",
        "        # --- 3D Plot ---\n",
        "        fig = plt.figure(figsize=(10, 7))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "        # Surface\n",
        "        ax.plot_surface(X1g, X2g, y_grid_pred, alpha=0.6, cmap='viridis', edgecolor='none')\n",
        "\n",
        "        # Data points\n",
        "        ax.scatter(x1_vals, x2_vals, y_vals, color='red', s=10, label='Data Points')\n",
        "\n",
        "        ax.set_xlabel('x1')\n",
        "        ax.set_ylabel('x2')\n",
        "        ax.set_zlabel('y')\n",
        "        ax.set_title(\"3D Linear Regression Surface\")\n",
        "\n",
        "        # Coefficient display\n",
        "        coef_str = '\\n'.join([f'{lbl} = {coef:.2f}' for lbl, coef in zip(labels, model.coef_)])\n",
        "        print(f'Intercept: {model.intercept_:.2f}')\n",
        "        print(coef_str)\n",
        "        print(f'Least Squares Loss: {loss:.2f}')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# --- Attach checkbox callbacks ---\n",
        "for cb in checkboxes.values():\n",
        "    cb.observe(update_plot, names='value')\n",
        "\n",
        "# --- Display GUI ---\n",
        "selectors = widgets.VBox(list(checkboxes.values()))\n",
        "display(widgets.HBox([selectors, output]))\n",
        "update_plot()\n"
      ],
      "metadata": {
        "id": "hUrf2sjHSKl1"
      },
      "id": "hUrf2sjHSKl1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regularization"
      ],
      "metadata": {
        "id": "nOx3AfPZh6l7"
      },
      "id": "nOx3AfPZh6l7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization of a linear regression model adds some additional constraints that guard against overfitting by pushing the model towards simpler, more robust solutions that perform well on new data rather than overfitting to the training data.\n",
        "\n",
        "A standard way to introduce regularization is through an additional penalty term in the linear regression least squares loss function $L[\\boldsymbol \\phi] = \\sum_i \\left(f[x_i,\\boldsymbol\\phi] - y_i \\right)^2$ that punishes large values of the coefficients $\\phi_i$ and seeks to **shrink** them towards zero.\n",
        "\n",
        "Two popular flavors of this regularization pernalty are:\n",
        "\n",
        "1. Ridge regression (L2 regression) in which we add a penality on the sum of the squared values of the coefficients: $L[\\boldsymbol \\phi] = \\sum_i \\left(f[x_i,\\boldsymbol\\phi] - y_i \\right)^2 + \\lambda \\sum_k \\phi_k^2$\n",
        "\n",
        "2. Lasso regression (L1 regression) in which we add a penality on the sum of the absolute values of the coefficients: $L[\\boldsymbol \\phi] = \\sum_i \\left(f[x_i,\\boldsymbol\\phi] - y_i \\right)^2 + \\lambda \\sum_k |\\phi_k|$\n",
        "\n",
        "The strength of the penalty, and therefore how much we shrink the coefficients towards zero, is controlled by the hyperparameter $\\lambda$. We can think of $\\lambda$ as controlling the trade-off between fitting the data well (first term) and minimizing the size of the coefficients (second term).\n",
        "\n",
        "For $\\lambda=0$ we recover standard linear regression. For $\\lambda>0$ we increasingly punish large values of the coefficients.\n",
        "\n",
        "An amazing mathematical result is that lasso (L1) regression has the remarkable property of shrinking unimportant coefficients to exactly zero! This can help us to perform **feature selection** by retaining only the important features in our model and help us generate **sparse and interpretable** models."
      ],
      "metadata": {
        "id": "YmFIspF3iEoV"
      },
      "id": "YmFIspF3iEoV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ipywidgets and sklearn in Colab\n",
        "!pip install ipywidgets scikit-learn --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge, LassoLars\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# --- Create a nonlinear dataset with sparsity potential ---\n",
        "np.random.seed(1)\n",
        "x = np.linspace(1, 10, 200)\n",
        "y = 12 * np.sin(x) + 0.5 * x**2 + 2 * np.exp(x / 10) + np.random.normal(0, 2, size=len(x))\n",
        "df = pd.DataFrame({'x': x, 'y': y})\n",
        "\n",
        "# --- Define transformations ---\n",
        "transformations = {\n",
        "    'x': lambda x: x,\n",
        "    'x^2': lambda x: x**2,\n",
        "    'x^3': lambda x: x**3,\n",
        "    'x^4': lambda x: x**4,\n",
        "    'sin(x)': lambda x: np.sin(x),\n",
        "    'cos(x)': lambda x: np.cos(x),\n",
        "    'sin(2x)': lambda x: np.sin(2 * x),\n",
        "    'sin(3x)': lambda x: np.sin(3 * x),\n",
        "    'ln(x)': lambda x: np.log(x),\n",
        "    'exp(x)': lambda x: np.exp(x / 10)\n",
        "}\n",
        "\n",
        "# --- Checkboxes: All selected by default ---\n",
        "checkboxes = {\n",
        "    name: widgets.Checkbox(value=True, description=name)\n",
        "    for name in transformations\n",
        "}\n",
        "\n",
        "# --- Regularization method selection ---\n",
        "reg_type = widgets.RadioButtons(\n",
        "    options=['L1 (Lasso)', 'L2 (Ridge)'],\n",
        "    value='L2 (Ridge)',\n",
        "    description='Regularization:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# --- Lambda (alpha) slider ---\n",
        "lambda_slider = widgets.FloatSlider(\n",
        "    value=1.0,\n",
        "    min=0.1,\n",
        "    max=10,\n",
        "    step=0.1,\n",
        "    description='λ:',\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "# --- Output widget for interactive display ---\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Build selected feature matrix ---\n",
        "def get_selected_features(x):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for name, func in transformations.items():\n",
        "        if checkboxes[name].value:\n",
        "            features.append(func(x))\n",
        "            labels.append(name)\n",
        "    if not features:\n",
        "        return None, None\n",
        "    X = np.vstack(features).T\n",
        "    return X, labels\n",
        "\n",
        "# --- Main update and plot function ---\n",
        "def update_plot(change=None):\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        X, labels = get_selected_features(df['x'].values)\n",
        "        if X is None:\n",
        "            print(\"Please select at least one feature.\")\n",
        "            return\n",
        "\n",
        "        y = df['y'].values\n",
        "        alpha = lambda_slider.value\n",
        "\n",
        "        # Choose regularization model\n",
        "        if reg_type.value == 'L1 (Lasso)':\n",
        "            model = LassoLars(alpha=alpha)\n",
        "        else:\n",
        "            model = Ridge(alpha=alpha)\n",
        "\n",
        "        model.fit(X, y)\n",
        "        y_pred = model.predict(X)\n",
        "        residuals = y - y_pred\n",
        "        loss = np.sum(residuals ** 2)\n",
        "\n",
        "        # Sort for smooth plotting\n",
        "        x_vals = df['x'].values\n",
        "        sort_idx = np.argsort(x_vals)\n",
        "        x_sorted = x_vals[sort_idx]\n",
        "        y_pred_sorted = y_pred[sort_idx]\n",
        "\n",
        "        # --- Plot ---\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(x_vals, y, color='blue', label='Data Points', alpha=0.6)\n",
        "        plt.plot(x_sorted, y_pred_sorted, color='red', label='Model Fit', linewidth=2)\n",
        "\n",
        "        # Residuals\n",
        "        for xi, yi, ypi in zip(x_vals, y, y_pred):\n",
        "            plt.vlines(xi, yi, ypi, color='gray', linestyle='dashed', alpha=0.3)\n",
        "\n",
        "        # Text summary\n",
        "        coef_str = '\\n'.join([f'{lbl} = {coef:.2f}' for lbl, coef in zip(labels, model.coef_)])\n",
        "        plt.text(0.05, 0.95,\n",
        "                 f'{reg_type.value}\\nλ = {alpha:.2f}\\nIntercept = {model.intercept_:.2f}\\n' +\n",
        "                 coef_str + f'\\nLoss = {loss:.2f}',\n",
        "                 transform=plt.gca().transAxes,\n",
        "                 fontsize=11,\n",
        "                 verticalalignment='top',\n",
        "                 bbox=dict(boxstyle='round,pad=0.4', facecolor='lightyellow', edgecolor='black'))\n",
        "\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.title(\"Regularized Linear Regression with Interactive Feature Selection\")\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # --- Coefficient Table ---\n",
        "        coef_df = pd.DataFrame({\n",
        "            'Feature': labels,\n",
        "            'Coefficient': model.coef_,\n",
        "            'Zeroed (≈0)': np.isclose(model.coef_, 0, atol=1e-4)\n",
        "        })\n",
        "        display(coef_df.style.set_caption(\"Model Coefficients\"))\n",
        "\n",
        "# --- Connect interactivity ---\n",
        "for cb in checkboxes.values():\n",
        "    cb.observe(update_plot, names='value')\n",
        "reg_type.observe(update_plot, names='value')\n",
        "lambda_slider.observe(update_plot, names='value')\n",
        "\n",
        "# --- Display interface ---\n",
        "selectors = widgets.VBox(list(checkboxes.values()))\n",
        "controls = widgets.VBox([reg_type, lambda_slider])\n",
        "display(widgets.HBox([selectors, controls, output]))\n",
        "update_plot()\n"
      ],
      "metadata": {
        "id": "DS_7KZyqvOqM"
      },
      "id": "DS_7KZyqvOqM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Validation"
      ],
      "metadata": {
        "id": "CjJb5QeVYyXS"
      },
      "id": "CjJb5QeVYyXS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\lambda$ is a **hyperparameter** of our ML model -- we have to set it before we learn the $\\phi_i$ **parameters** of our ML model.\n",
        "\n",
        "> *N.B. Remember, a ML model **parameter** is learned from the data, whereas a ML model **hyperparameter** is set by the user before running learning and must be specfied by another means.*\n",
        "\n",
        "In the regularization block above, the fit changed as we modulated the $\\lambda$ hyperparameter, but how do we determine the optimal value? What is the \"Goldilocks\" $\\lambda$?\n",
        "\n",
        "**Cross validation** is the process by which we typically optimize model hyperparameters wherein we optimize their values by evaluating model performance on a held-out **validation set**. The optimal value of the hyperparameter is the one which gives the best performance on this unseen data that was not involved in model training. Since the validation set is used in tuning the hyperparameter, we can then ask the model if it really does perform well by challenging it on the held-out **test set**.\n",
        "\n",
        "> *We can think of cross validation as challenging the model to do well on data outside of its training ensemble to make sure we don't overfit and generalize well to new data.*"
      ],
      "metadata": {
        "id": "_pD1eChMiEHh"
      },
      "id": "_pD1eChMiEHh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 - Manual"
      ],
      "metadata": {
        "id": "aqzD8YPftJnG"
      },
      "id": "aqzD8YPftJnG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the plot below. Play around with including more and fewer features and the type and strength of regularization.\n",
        "\n",
        "In the bottom right of the plot, we report the **mean squared error (MSE)**, this is just the average of the least squares loss function over all of the points. It is better to work with the averaged value of the loss since the training and validation sets may contain different numbers of points. We can consider the MSE as telling us on average how well our model will predict the correct value of any single point.\n",
        "\n",
        "* Least squares loss function: $L[\\boldsymbol \\phi] = \\sum_{i=1}^N \\left(f[x_i,\\boldsymbol\\phi] - y_i \\right)^2$\n",
        "\n",
        "* Mean squared error (MSE): $\\text{MSE} = \\frac{1}{N}L[\\boldsymbol \\phi] = \\frac{1}{N}\\sum_{i=1}^N \\left(f[x_i,\\boldsymbol\\phi] - y_i \\right)^2$"
      ],
      "metadata": {
        "id": "-Kr4y7dg05gp"
      },
      "id": "-Kr4y7dg05gp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ipywidgets and sklearn in Colab\n",
        "!pip install ipywidgets scikit-learn --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import Ridge, LassoLars\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# --- Create nonlinear dataset ---\n",
        "np.random.seed(1)\n",
        "x = np.linspace(1, 10, 50)\n",
        "y = 12 * np.sin(x) + 0.5 * x**2 + 2 * np.exp(x / 10) + np.random.normal(0, 4, size=len(x))\n",
        "df = pd.DataFrame({'x': x, 'y': y})\n",
        "\n",
        "# --- Train/validation split ---\n",
        "train_df, val_df = train_test_split(df, test_size=0.5, random_state=42)\n",
        "\n",
        "# --- Define feature transformations ---\n",
        "transformations = {\n",
        "    'x': lambda x: x,\n",
        "    'x^2': lambda x: x**2,\n",
        "    'x^3': lambda x: x**3,\n",
        "    'x^4': lambda x: x**4,\n",
        "    'sin(x)': lambda x: np.sin(x),\n",
        "    'cos(x)': lambda x: np.cos(x),\n",
        "    'sin(2x)': lambda x: np.sin(2 * x),\n",
        "    'sin(3x)': lambda x: np.sin(3 * x),\n",
        "    'ln(x)': lambda x: np.log(x),\n",
        "    'exp(x)': lambda x: np.exp(x / 10)\n",
        "}\n",
        "\n",
        "# --- Feature checkboxes (all active by default) ---\n",
        "checkboxes = {\n",
        "    name: widgets.Checkbox(value=True, description=name)\n",
        "    for name in transformations\n",
        "}\n",
        "\n",
        "# --- Regularization type (L1 selected by default) ---\n",
        "reg_type = widgets.RadioButtons(\n",
        "    options=['L1 (Lasso)', 'L2 (Ridge)'],\n",
        "    value='L1 (Lasso)',  # Default to L1\n",
        "    description='Regularization:',\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "# --- Lambda (α) slider with high resolution ---\n",
        "lambda_slider = widgets.FloatSlider(\n",
        "    value=0.01,\n",
        "    min=0.001,\n",
        "    max=1.0,\n",
        "    step=0.001,\n",
        "    description='λ:',\n",
        "    continuous_update=False,\n",
        "    readout_format='.4f'\n",
        ")\n",
        "\n",
        "# --- Output display widget ---\n",
        "output = widgets.Output()\n",
        "\n",
        "# --- Build selected feature matrix ---\n",
        "def get_selected_features(x):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for name, func in transformations.items():\n",
        "        if checkboxes[name].value:\n",
        "            features.append(func(x))\n",
        "            labels.append(name)\n",
        "    if not features:\n",
        "        return None, None\n",
        "    X = np.vstack(features).T\n",
        "    return X, labels\n",
        "\n",
        "# --- Main update function ---\n",
        "def update_plot(change=None):\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        X_all, labels = get_selected_features(df['x'].values)\n",
        "        if X_all is None:\n",
        "            print(\"Please select at least one feature.\")\n",
        "            return\n",
        "\n",
        "        alpha = lambda_slider.value\n",
        "\n",
        "        # Select model type\n",
        "        if reg_type.value == 'L1 (Lasso)':\n",
        "            model = LassoLars(alpha=alpha)\n",
        "        else:\n",
        "            model = Ridge(alpha=alpha)\n",
        "\n",
        "        # Get train and validation sets\n",
        "        X_train, y_train = get_selected_features(train_df['x'].values)[0], train_df['y'].values\n",
        "        X_val, y_val = get_selected_features(val_df['x'].values)[0], val_df['y'].values\n",
        "\n",
        "        # Fit model and predict\n",
        "        model.fit(X_train, y_train)\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        y_all_pred = model.predict(X_all)\n",
        "\n",
        "        mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "        mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "\n",
        "        # Sorting for plot\n",
        "        x_vals_all = df['x'].values\n",
        "        sort_idx = np.argsort(x_vals_all)\n",
        "        x_sorted = x_vals_all[sort_idx]\n",
        "        y_sorted = y_all_pred[sort_idx]\n",
        "\n",
        "        # --- Plot ---\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(train_df['x'], train_df['y'], color='blue', label='Training Data', alpha=0.6)\n",
        "        plt.scatter(val_df['x'], val_df['y'], color='green', label='Validation Data', alpha=0.6)\n",
        "        plt.plot(x_sorted, y_sorted, color='red', label='Model Fit', linewidth=2)\n",
        "\n",
        "        # Residuals\n",
        "        for xi, yi, ypi in zip(train_df['x'], y_train, y_train_pred):\n",
        "            plt.vlines(xi, yi, ypi, color='gray', linestyle='dashed', alpha=0.3)\n",
        "\n",
        "        # Coefficients box (top-left)\n",
        "        coef_str = '\\n'.join([f'{lbl} = {coef:.4f}' for lbl, coef in zip(labels, model.coef_)])\n",
        "        plt.text(0.05, 0.95,\n",
        "                 f'{reg_type.value}\\nλ = {alpha:.4f}\\nIntercept = {model.intercept_:.4f}\\n' + coef_str,\n",
        "                 transform=plt.gca().transAxes,\n",
        "                 fontsize=11,\n",
        "                 verticalalignment='top',\n",
        "                 bbox=dict(boxstyle='round,pad=0.4', facecolor='lightyellow', edgecolor='black'))\n",
        "\n",
        "        # Losses box (bottom-right)\n",
        "        plt.text(0.95, 0.05,\n",
        "                 f'Train MSE = {mse_train:.4f}\\nValidation MSE = {mse_val:.4f}',\n",
        "                 transform=plt.gca().transAxes,\n",
        "                 fontsize=11,\n",
        "                 verticalalignment='bottom',\n",
        "                 horizontalalignment='right',\n",
        "                 bbox=dict(boxstyle='round,pad=0.4', facecolor='lightcyan', edgecolor='black'))\n",
        "\n",
        "        plt.xlabel(\"x\")\n",
        "        plt.ylabel(\"y\")\n",
        "        plt.title(\"Regularized Linear Regression with Train/Validation Split\")\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # --- Coefficient Table ---\n",
        "        coef_df = pd.DataFrame({\n",
        "            'Feature': labels,\n",
        "            'Coefficient': model.coef_,\n",
        "            'Zeroed (≈0)': np.isclose(model.coef_, 0, atol=1e-4)\n",
        "        })\n",
        "        display(coef_df.style.set_caption(\"Model Coefficients\"))\n",
        "\n",
        "# --- Attach widget listeners ---\n",
        "for cb in checkboxes.values():\n",
        "    cb.observe(update_plot, names='value')\n",
        "reg_type.observe(update_plot, names='value')\n",
        "lambda_slider.observe(update_plot, names='value')\n",
        "\n",
        "# --- GUI layout ---\n",
        "selectors = widgets.VBox(list(checkboxes.values()))\n",
        "controls = widgets.VBox([reg_type, lambda_slider])\n",
        "display(widgets.HBox([selectors, controls, output]))\n",
        "update_plot()\n"
      ],
      "metadata": {
        "id": "OHl2ouwfzkLI"
      },
      "id": "OHl2ouwfzkLI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activate all of the features and employ L1 (Lasso) regularization.\n",
        "\n",
        "Change the value of $\\lambda$ and determine the optimum by minimizing the **validation MSE**. Enter your optimal value below."
      ],
      "metadata": {
        "id": "MVxlYhrTgctb"
      },
      "id": "MVxlYhrTgctb"
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_opt = 0.0"
      ],
      "metadata": {
        "id": "ndmGp0H-1B1F"
      },
      "id": "ndmGp0H-1B1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens to the **train MSE** as we increase $\\lambda$? Why?"
      ],
      "metadata": {
        "id": "lcCaLn84fXy2"
      },
      "id": "lcCaLn84fXy2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 - Automatic"
      ],
      "metadata": {
        "id": "Nch7ZdnltQYE"
      },
      "id": "Nch7ZdnltQYE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was informative, but it is slow and laborious to optimize $\\lambda$ manually by trial-and-improvement. We can automate this process by scanning over a range of $\\lambda$ values using a `for` loop, recording the training and validation MSE at each value of $\\lambda$, then making a plot.\n",
        "\n",
        "The code below will do this for us, but before executing it see if you can locate the new for loop that will scan over $\\lambda$ values in the range 0.001 to 1.000 in increments of 0.001."
      ],
      "metadata": {
        "id": "SqICEMAAtQYG"
      },
      "id": "SqICEMAAtQYG"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
        "from sklearn.linear_model import LassoLars\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# --- Create nonlinear dataset ---\n",
        "np.random.seed(1)\n",
        "x = np.linspace(1, 10, 50)\n",
        "y = 12 * np.sin(x) + 0.5 * x**2 + 2 * np.exp(x / 10) + np.random.normal(0, 4, size=len(x))\n",
        "df = pd.DataFrame({'x': x, 'y': y})\n",
        "\n",
        "# --- Train/validation split ---\n",
        "train_df, val_df = train_test_split(df, test_size=0.5, random_state=42)\n",
        "\n",
        "# --- Define feature transformations ---\n",
        "transformations = {\n",
        "    'x': lambda x: x,\n",
        "    'x^2': lambda x: x**2,\n",
        "    'x^3': lambda x: x**3,\n",
        "    'x^4': lambda x: x**4,\n",
        "    'sin(x)': lambda x: np.sin(x),\n",
        "    'cos(x)': lambda x: np.cos(x),\n",
        "    'sin(2x)': lambda x: np.sin(2 * x),\n",
        "    'sin(3x)': lambda x: np.sin(3 * x),\n",
        "    'ln(x)': lambda x: np.log(x),\n",
        "    'exp(x)': lambda x: np.exp(x / 10)\n",
        "}\n",
        "\n",
        "# --- Helper function to compute feature matrix ---\n",
        "def get_features(x):\n",
        "    return np.vstack([func(x) for func in transformations.values()]).T\n",
        "\n",
        "# --- Get train and validation matrices ---\n",
        "X_train = get_features(train_df['x'].values)\n",
        "y_train = train_df['y'].values\n",
        "X_val = get_features(val_df['x'].values)\n",
        "y_val = val_df['y'].values\n",
        "\n",
        "# --- Sweep over lambda values ---\n",
        "results = []\n",
        "lambdas = np.arange(0.001, 1.001, 0.001)\n",
        "\n",
        "for alpha in lambdas:\n",
        "    model = LassoLars(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "    mse_val = mean_squared_error(y_val, y_val_pred)\n",
        "\n",
        "    results.append({\n",
        "        'lambda': alpha,\n",
        "        'train_mse': mse_train,\n",
        "        'val_mse': mse_val\n",
        "    })\n",
        "\n",
        "# --- Convert to DataFrame ---\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# --- Plot MSE curves ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_df['lambda'], results_df['train_mse'], label='Train MSE', color='blue')\n",
        "plt.plot(results_df['lambda'], results_df['val_mse'], label='Validation MSE', color='green')\n",
        "\n",
        "plt.xlabel('λ (Lambda)')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Train vs Validation MSE across λ (Lasso)')\n",
        "plt.legend()\n",
        "\n",
        "# --- Gridlines ---\n",
        "ax = plt.gca()\n",
        "ax.grid(True, which='major', linestyle='-', linewidth=0.5)\n",
        "ax.grid(True, which='minor', linestyle=':', linewidth=0.3, alpha=0.7)\n",
        "\n",
        "# Set minor ticks\n",
        "ax.xaxis.set_minor_locator(MultipleLocator(0.01))\n",
        "ax.yaxis.set_minor_locator(MultipleLocator(0.25))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Show first few rows of the table ---\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "TgnnMjFPK479"
      },
      "id": "TgnnMjFPK479",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's interrogate the plots:\n",
        "\n",
        "* Do the shapes of the curves for the training and validation MSE values make sense?\n",
        "\n",
        "* What is the optimal value of $\\lambda$ revealed by this plot?\n",
        "\n",
        "* How does the optimal value of $\\lambda$ you determined in the previous exercise agree with this value?"
      ],
      "metadata": {
        "id": "p9vsSFTnuIlv"
      },
      "id": "p9vsSFTnuIlv"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lEwwVIhbp98m"
      },
      "id": "lEwwVIhbp98m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}