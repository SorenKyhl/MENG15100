{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **MENG 15100** Lab 2\n",
        "\n",
        "Welcome to the second lab of MENG 15100: Machine Learning and Artificial Intelligence for Molecular Discovery and Engineering\n",
        "\n",
        "In this lab, you’ll learn the basics of **Linear Regression**, a foundational machine-learning model.\n",
        "\n",
        "We’ll walk through the six core steps of training any Machine Learning model:\n",
        "1. **Data:** Select inputs (features) and outputs (targets).\n",
        "2. **Model:** Define a function relating inputs to outputs, with *tunable parameters.*\n",
        "3. **Loss function:** Decide how to measure the model's error\n",
        "4. **Training:** Optimize the model parameters via *gradient descent.*\n",
        "5. **Evaluation:** Determine how the trained model performs on separate *test data*\n",
        "6. **Improvement:** Investigate ways to make the model better!\n",
        "\n",
        "This lab starts from first principles—**no prior coding experience is required.** For every exercise, you will be provided with baseline Python code, and you will only be asked to make minor edits or adaptations (e.g., change a variable’s value, adjust the number of iterations, or modify a plot’s formatting)."
      ],
      "metadata": {
        "id": "qsxOcqxThFTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note on Lab 2  \n",
        "\n",
        "Lab 2 will begin with **Section 0**, which asks you to do some partial reading of two scientific papers. This section is designed to help you start practicing how to read and interpret research articles — a skill we’ll keep coming back to throughout the course.  \n",
        "\n",
        "Please also plan ahead: although the reading is only partial, it is not something you can rush through. To get proper understanding, you should set aside around **30 minutes** to carefully work through Section 0.  \n",
        "\n",
        "In summary:  \n",
        "- Section 0 = reading two papers (30 minutes).  \n",
        "- Sections 1–6 = coding in Python, best done during the lab session.  \n",
        "- Do the reading early so your lab time can be dedicated to coding practice.  \n"
      ],
      "metadata": {
        "id": "uvy72EF_pkz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Structure and Grading\n",
        "The lab is organized as follows:\n",
        "- **Topics** – Broad units (e.g., *1. Exploring the Data, 2.Linear Regression*).\n",
        "- **Sections** – Subdivisions within each Topic (e.g., *1.4 Train / Test split*).\n",
        "- **Problems** – Each Section ends with a Problem to be completed in the Jupyter Notebook. Problems may involve:\n",
        "  - Short-answer questions\n",
        "  - Modifications to existing Python code\n",
        "  - Some Problems contain multiple tasks.\n",
        "  - Each Problem is worth 1 point total, with points divided equally among its tasks.\n",
        "\n",
        "- **Interactive code** - many sections include interactive graphical user interfaces. The code implementing these functions is located in a custom library titled `menglab`\n",
        "\n"
      ],
      "metadata": {
        "id": "alPSZ8oFhKp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "**TIP**: An interactive table of contents is avaialable on the left sidebar.\n",
        "\n",
        "### 0. Introduction to Molecular Boiling Points\n",
        "\n",
        "&emsp; 0.1 Molecular Boiling Points <br>\n",
        "&emsp; 0.2 Linear Regression Models in Scientific Papers\n",
        "\n",
        "### 1. Dataset\n",
        "\n",
        "&emsp; 1.1 Loading the Dataset <br>\n",
        "&emsp; 1.2 Interactive Data Explorer <br>\n",
        "&emsp; 1.3 Beginning with Linear Alkanes <br>\n",
        "&emsp; 1.4 Train/Test split <br>\n",
        "\n",
        "\n",
        "### 2. Model: Linear Regression\n",
        "\n",
        "&emsp; 2.1 Writing and Plotting our first model <br>\n",
        "\n",
        "### 3. Loss Function\n",
        "\n",
        "&emsp; 3.1 Root Mean Squared Error (RMSE) <br>\n",
        "&emsp; 3.2 Visualizing the Loss Landscape <br>\n",
        "\n",
        "### 4. Training: Gradient Descent\n",
        "\n",
        "&emsp; 4.1 Manual Training <br>\n",
        "&emsp; 4.2 Gradient Descent <br>\n",
        "&emsp; 4.3 Interactive Gradient Descent <br>\n",
        "\n",
        "### 5. Evaluation: Testing Sets and Generalization\n",
        "\n",
        "&emsp; 5.1 Performance Plots <br>\n",
        "&emsp; 5.2 Model Generalization <br>\n",
        "\n",
        "\n",
        "### 6. Improving and Expanding the Model\n",
        "\n",
        "&emsp; 6.1 Underfitting and Polynomial Regression <br>\n",
        "&emsp; 6.2 Expanding the Dataset to Branched Alkanes <br>\n",
        "&emsp; 6.3 Overfitting & Regularization <br>\n",
        "&emsp; 6.4 Multiple Linear Regression <br>\n",
        "&emsp; 6.5 Modelling the Entire Dataset <br>"
      ],
      "metadata": {
        "id": "oh5TIC7JhNax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports (Execute Once)\n",
        "Run the code cell below to install and import the modules necessary for this lab:"
      ],
      "metadata": {
        "id": "FCuomnF_gktq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute this cell once to import and install modules (may take several seconds)\n",
        "%pip -q install requests pyreadr rdkit py3Dmol\n",
        "\n",
        "# menglab library\n",
        "%pip install -q --no-cache-dir --upgrade --force-reinstall \\\n",
        "  \"git+https://github.com/andrewlferguson/MENG15100.git@main#subdirectory=labs/L2/menglab2\"\n",
        "\n",
        "import menglab2 as menglab\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# interactive gui settings\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "# Plotting settings\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams.update({\n",
        "    \"figure.figsize\": (6, 4),\n",
        "    \"figure.dpi\": 120,\n",
        "    \"axes.titlesize\": 13,\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"axes.grid\": True,\n",
        "    \"grid.linestyle\": \"--\",\n",
        "    \"grid.alpha\": 0.35,\n",
        "    \"lines.linewidth\": 2.0,\n",
        "    \"lines.markersize\": 6,\n",
        "    \"font.size\": 12,\n",
        "    \"xtick.direction\": \"out\",\n",
        "    \"ytick.direction\": \"out\",\n",
        "    \"xtick.minor.visible\": True,\n",
        "    \"ytick.minor.visible\": True,\n",
        "    \"legend.frameon\": False,\n",
        "    \"savefig.bbox\": \"tight\",\n",
        "})\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "metadata": {
        "id": "J1xdx0iYgudW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 Introduction"
      ],
      "metadata": {
        "id": "cSDe1y5uZZjD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Molecular boiling points\n",
        "In this lab, we’ll build a simple machine learning model to predict a liquid's **boiling point**—the temperature at which the substance changes state from a liquid to a gas.\n",
        "- For example, the boiling point of water is 100 °C / 212 °F (at atmospheric pressure).\n",
        "\n",
        "**Why boiling point?** It reflects how strongly molecules stick together in the liquid—that is, the strength of their **intermolecular forces**. Stronger attractions mean you must supply more heat to pull molecules apart, so:\n",
        "- Stronger intermolecular forces → higher boiling point\n",
        "\n",
        "**Goal:** Our model will use basic molecular properties—starting with size (**molecular weight**)—to predict boiling point with a linear regression model. Later, we will add more input features to improve the model.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "0PCpSn8Ms7_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boiling Point Video\n",
        "**Watch the YouTube video below to see a simulation of boiling from a molecular point of view**\n",
        "\n",
        "(execute the cell to load the YouTube player in Google Colab).\n"
      ],
      "metadata": {
        "id": "B67yFprrRMmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display youtube video in google colab\n",
        "# Ipython.display is a python module that contains functions like\n",
        "#   YouTubeVideo which can embed youtube videos for viewing.\n",
        "from IPython.display import YouTubeVideo\n",
        "\n",
        "# Replace 'dQw4w9WgXcQ' with your Short's video ID\n",
        "# For example, if the URL is https://www.youtube.com/watch?v=dQw4w9WgXcQ, the ID is dQw4w9WgXcQ\n",
        "video_id = 'rHUYYGdByAA'\n",
        "YouTubeVideo(video_id, width=315, height=560)"
      ],
      "metadata": {
        "id": "IQPeLq69NPsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Video Recap\n",
        "At very low temperatures, the argon atoms are a **solid**:\n",
        "- Atoms are held tightly together by intermolecular forces in an ordered crystal lattice.\n",
        "\n",
        "As the system is heated to about 60 K (–213 °C), it melts to become a **liquid**.\n",
        "- Thermal energy allows the atoms to move past one another while still remaining in contact.\n",
        "\n",
        "With further heating the atoms boil into the **gas phase**.\n",
        "- Atoms gain enough energy to completely escape their neighbors\n",
        "- The temperature at which this transition from liquid to gas occurs is known as the **boiling point**.\n",
        "- For argon, the boiling point is about 87 K (–186 °C), which is relatively low — a sign that the forces holding argon atoms together are quite weak.\n",
        "\n",
        "**FYI: The video above was produced using [molecular dynamics](https://www.wikiwand.com/en/articles/Molecular_dynamics), a computational method that simulates the real physical forces between atoms and molecules. This technique is not only useful for teaching but is also a focus of active research in molecular engineering labs, including the Ferguson Lab in the PME.**\n",
        "\n",
        "Although this lab will focus on **boiling**, we can use similar ideas to consider **freezing**. Watch the 30-second video below, showing a molecular dynamics simulation of liquid water turning to ice (i.e., freezing). Note how the disordered liquid forms a heaxagonally ordered crystal structure as it freezes into ice.\n"
      ],
      "metadata": {
        "id": "T5UOCRUPPprv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_id = 'zRUFzJrDtq0'\n",
        "YouTubeVideo(video_id, width=560, height=315)"
      ],
      "metadata": {
        "id": "QS8SbVe7Pou9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### ✅ Exercise 1: Boiling Point comparison [2 points]\n",
        "\n",
        "Consider **water** (H₂O) and **methane** (CH₄).\n",
        "\n",
        "- Water molecules stick together primarily due to strong **hydrogen bonds**: each molecule has highly polar O–H bonds that attract each another.\n",
        "\n",
        "- Methane molecules stick together only by weak **London dispersion forces** / **van der Waals forces**, because the molecule is nonpolar.\n",
        "\n",
        "\n",
        "**Task:** Which of these two molecules has the higher boiling point, and why? **Enter your answer in the text box below.**\n",
        "\n",
        "**Hint:** Don’t worry if you haven’t studied hydrogen bonds or London dispersion forces yet. For this question, all you need to remember is that hydrogen bonds are relatively strong, while London dispersion forces are relatively weak."
      ],
      "metadata": {
        "id": "QIYn1sHzGuyx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "IfT1MVBPH_pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 0.2 Linear Regression Models in Scientific Papers\n",
        "\n",
        "Our machine learning model will be based on **real scientific research** published in journal articles (\"papers\").\n",
        "\n",
        "We will **skim 2 papers** (linked below) that use **linear regression** to predict the boiling points of different molecules. We will also **recreate some of the results** from these studies.\n",
        "\n",
        "### Why Read Papers?\n",
        "\n",
        "1. Reading Scientific Literature is an **important skill** in any field of study.\n",
        "\n",
        "2. Reproducing published results is an important part of science — it helps us verify that findings are reliable and gives us a foundation to build new ideas and improvements.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7j1F9MUYIFBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Read a Scientific Paper\n",
        "\n",
        "Reading a Paper is **not like reading other types of literature.**.\n",
        "- **Don’t be discouraged**: It is intimidating, especially for newcomers to a field.\n",
        "- **It’s normal not to understand everything** - Even experienced researchers often find papers hard to read outside their specialty.\n",
        "- **Practice matters**: It is a skill you build with practice, not something you're supposed be good at right away.\n",
        "- **Read nonlinearly**: Do not read cover-to-cover like a textbook or novel! Instead, **follow the step-by-step strategy below:**\n",
        "\n",
        "---\n",
        "\n",
        "## Step-by-Step Strategy\n",
        "\n",
        "### 1. Title and Abstract\n",
        "- Read the **Title** and **abstract** in full. This is a one-paragraph summary of the entire study.\n",
        "  - This is the **only** section we will read front-to-back.\n",
        "  - Don’t get stuck on jargon.  \n",
        "  - Look for the **one sentence** that summarizes the **key finding** of the paper.  \n",
        "- After this step, you should be able to answer: *What is this paper about?*  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Skim the Introduction\n",
        "- The introduction provides **background context** and the **motivation** for the study.  \n",
        "- Skim quickly:\n",
        "  - What **problem** are the authors trying to solve?  \n",
        "  - Why is it important?  \n",
        "- Skip details and entire paragraphs you don’t understand, but **flag unfamiliar concepts** as things you could look up later.  \n",
        "- After this step, you should be able to answer: *Why did the authors do this study?*  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Go Straight to the Figures\n",
        "- The figures contain **all** the main results of the paper.\n",
        "  - Going to the figures is the most valuable and efficient use of your time.  \n",
        "- For each figure:  \n",
        "  - Read the **caption** carefully.  \n",
        "  - Ask: *What is the key result being shown?*  \n",
        "  - Try to connect the figure to the main finding from the abstract.  \n",
        "- At this stage, don’t worry if you don’t fully understand the methods that produced the figure — just focus on *what the result is*.  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Read the Conclusion\n",
        "- Once you’ve seen the results, skim the **conclusion**\n",
        "- This section tells you how the authors **interpret their findings**.  \n",
        "- Ask yourself: *Do the conclusions match what I saw in the figures?*  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. Dive Into the Methods & Discussion (Optional, for Later)\n",
        "- Methods and Discussion sections are often the hardest to read.  \n",
        "- Dive in if you need to understand *how* the results were obtained (e.g., if you’re planning to replicate or critique the work).  \n",
        "- Beginners can usually skip this section on the first read.  \n",
        "\n",
        "---\n",
        "\n",
        "## Extra Tips\n",
        "- You can glean **80% understanding from 20% of the text**\n",
        "- **Don’t read once — read multiple times**:  \n",
        "  - First pass: big picture (title, abstract, figures).  \n",
        "  - Second pass: details that matter for your understanding (methods, discussion)   \n",
        "- **Look up later**: don’t interrupt your reading flow every time you see an unfamiliar term — flag it, and return after the first read.  \n",
        "- **Talk it through**: papers make more sense when you discuss them with classmates or your instructor.  \n",
        "- **Remember:** The goal of reading a scientific paper is not to understand every single detail, but to be able to explain **what the authors asked, what they did, what they found, and why it matters.**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "rG-XXj51XM-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 2: Skim Paper 1 [9 points]\n",
        "\n",
        "Our first paper is shown below. We will dive into the **Abstract** and **Introduction** of paper because it is an approachable foundation to the topic of this lab.\n",
        "\n",
        "## Paper 1: (available on canvas under Modules/Lab2)\n",
        "Goll, E. S.; Jurs, P. C. *Prediction of the Normal Boiling Points of Organic Compounds from Molecular Structures with a Computational Neural Network Model.* **Journal of Chemical Information and Computer Sciences**, 1999, 39(5), 974–983. https://doi.org/10.1021/ci990071l\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "Read the **Abstract** (step 1 of the strategy guide), and answer the following questions:\n",
        "\n",
        "1. How many datsets were used in this paper?\n",
        "2. What three types of machine learning models were used in this paper?\n",
        "3. What property of molecules were the models trained to predict? (The output of the model).\n",
        "4. In your own words, *What is this paper about?*\n",
        "\n",
        "Skim the **Introduction** (step 2 of the strategy guide), and answer the following questions:\n",
        "\n",
        "5. For *nonpolar* molecules, what molecular property largely dictates the strength of intermolecular attraction?\n",
        "6. According to the authors, why is their model useful? (What arduous challenges does it circumvent?)\n",
        "7. What do the authors call \"Type I\" Models, \"Type II\" Models, and \"Type III\" models?\n",
        "8. On first reading, which paragraph did you skip entirely (or, which one did you want to skip)? **Remember,** picking which parts to skip/skim is a crucial skill in reading papers!\n",
        "9. In your own words, *Why did the authors do this study?*\n",
        "\n",
        "## Enter Your answers below:\n",
        "\n"
      ],
      "metadata": {
        "id": "cJrI6TZcKYz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How many datasets were used in this paper? YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "7ZKHP211e0aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What three types of machine learning models were used in this paper? YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "OpY64d8be6ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What property of molecules were the models trained to predict? (The output of the model). YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "FluYUHfTe6fU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. In your own words, *What is this paper about?* YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "_EwxtXaxe6pH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. For *nonpolar* molecules, what molecular property largely dictates the strength of intermolecular attraction? YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "UV6P1V8qe6t3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. According to the authors, why is their model useful? (What arduous challenges does it circumvent?) YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "tFiBzp57e6zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do the authors call \"Type I\" Models, \"Type II\" Models, and \"Type III\" models? YOUR ANSWER HERE\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gAo-t9MMfEnO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. On first reading, which paragraph did you skip entirely (or, which one did you want to skip)? **Remember,** picking which parts to skip/skim is a crucial skill in reading papers! YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "-D9Gxt2pfGEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. In your own words, *Why did the authors do this study?* YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "7ZbLZuxLfHDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 3: Skim Paper 2 [4 points]\n",
        "\n",
        "We will now turn to Paper 2 (available on Canvas under Modules/Lab2)\n",
        "\n",
        "Egolf, L. M.; Wessel, M. D.; Jurs, P. C. *Prediction of Boiling Points and Critical Temperatures of Industrially Important Organic Compounds from Molecular Structure.* **Journal of Chemical Information and Computer Sciences**, 1994, 34(2), 412–421. https://doi.org/10.1021/ci00020a032\n",
        "\n",
        "\n",
        "- Papers 1 and 2 are written by the same senior author (Prof. Peter C. Jurs, Penn State), with essentially the same datasets.\n",
        "- Paper 2 can be thought of as a **precursor** to Paper 1. It shows how the research ideas grew frand matured.  \n",
        "- The Introduction, motivation, and datasets are almost identical in both papers. The major difference is the types of models trained.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "Briefly read the **Abstract**, then skip to **Figure 2**, (step 3 of the strategy guide) read the caption and analyze the figure.\n",
        "\n",
        "**N.B.: We will reproduce Figure 2 in this Lab**\n",
        "\n",
        "1. What type of model (Type I, Type II, or Type III) is shown in Figure 2?\n",
        "2. Which axis (x or y) displays *experimental* boiling points, and which axis (x or y) displays *predictions* from the model?\n",
        "3. The authors include a black diagonal line which marks exactly where the x-coordinate is the same as the y-cordinate. It acts as a visual aid to the reader: what purpose does this line serve?\n",
        "4. In your own words, what is the key result being shown in Figure 2?\n",
        "\n"
      ],
      "metadata": {
        "id": "o00jY6peTRDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What type of model (Type I, Type II, or Type III) is shown in Figure 2? YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "01mwvhJXpMkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Which axis (x or y) displays *experimental* boiling points, and which axis (x or y) displays *predictions* from the model? YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "TF4ZWPcUpMou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The authors include a black diagonal line which marks exactly where the x-coordinate is the same as the y-cordinate. It acts as a visual aid to the reader: what purpose does this line serve? YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "uZMjSHBQpMs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. In your own words, what is the key result being shown in Figure 2? YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "00a5SMHlpMww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Exploring the Data\n",
        "\n",
        "The first step in any machine learning project is to carefully **curate your dataset**.\n",
        "- We will be using [dataset II](https://cdk-r.github.io/cdkr/reference/bpdata.html) from Paper I, which is readily available online.\n",
        "- The dataset contains boiling points of 277 primarily nonpolar molecules.*\n",
        "\n",
        "***Note**: The dataset has also been augmented with the following 18 simple alkanes and branched alkanes for pedagogical purposes, to bring the total size of the dataset to 295 molecules\n",
        "- linear n-alkanes from n=1 (methane) to n=10 (decane)\n",
        "- branched alkanes including: isobutane, isopentane, neopentane, isohexane, neohexane, isoheptane, neoheptane, and isooctane\n",
        "\n",
        "## Inputs and outputs\n",
        "We also need to clearly define the **inputs** and **outputs** of the model.\n",
        "\n",
        "- **Inputs (features):** the pieces of information the model will use to make predictions.  \n",
        "- **Outputs (targets):** the values we want the model to predict, based on the inputs.\n",
        "\n",
        "In our case:\n",
        "\n",
        "- **Input:** molecular properties (e.g., molecular weight)  \n",
        "- **Output:** the boiling point temperature of the molecule  \n",
        "\n",
        "We can think of the process as a simple flow:\n",
        "\n",
        "molecular properties (input) → machine learning model → boiling point temperature (output)"
      ],
      "metadata": {
        "id": "gyY9j3K4dR9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Loading the Dataset\n",
        "\n",
        "**Execute the following code to load and inspect the dataset:**\n",
        "- the data is stored in a **dataframe** from the **pandas** library (similar to an excel spreadsheet)\n",
        "- the `data.head()` function displays the **first 5 rows** of the dataframe\n",
        "\n",
        "**The dataset contains the following columns:**  \n",
        "- **name**: the English name of the molecule  \n",
        "- **smiles**: a [SMILES string](https://www.wikiwand.com/en/articles/Simplified_Molecular_Input_Line_Entry_System), which is a language-like, machine-readable representation that encodes the molecular structure (atoms and their connectivity).\n",
        "- **MW**: molecular weight, in grams per mole (g/mol)  \n",
        "- **bp_k**: boiling point, measured in [Kelvin](https://www.wikiwand.com/en/articles/Kelvin)  \n"
      ],
      "metadata": {
        "id": "Hn5UTYfSAdlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = menglab.load_dataset() # load the datset into in a pandas DataFrame object called 'data'\n",
        "data.head()                   # display the first 5 rows (the 'head') of the DataFrame"
      ],
      "metadata": {
        "id": "16vQAPq4ydMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Interactive Data Explorer\n",
        "\n",
        "This tool lets you explore the molecules in the dataset with a **3D molecular viewer** and **interactive scatter plot**.\n",
        "\n",
        "You can look at individual molecules in detail and also compare them with others in the dataset.  \n",
        "\n",
        "Detailed instructions for how to use the interactive data explorer are available below.\n"
      ],
      "metadata": {
        "id": "3jmFw9aQPTVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.data_explorer(data)"
      ],
      "metadata": {
        "id": "Sp5PTbaE4KdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Use the Interactive Data Explorer\n",
        "- **Dropdown menu (top left)**  \n",
        "  Choose a molecule by name. The selected molecule will be:\n",
        "  - Highlighted on the scatter plot  \n",
        "  - Displayed in 3D\n",
        "\n",
        "- **Molecule panel (left)**  \n",
        "  Shows an interactive 3D model of the molecule, along with its key properties (name, molecular weight, and boiling point)\n",
        "  - **Click and drag** to rotate the molecule  \n",
        "  - **Scroll** to zoom in or out  \n",
        "  - Atoms appear as **colored balls**, one for each element (see color table below)  \n",
        "  - Bonds appear as **sticks** connecting the atoms  \n",
        "  - A semi-transparent surface surrounds the molecule, showing the approximate size and shape of its electron cloud.  \n",
        "\n",
        "- **Scatter plot (right)**  \n",
        "  Each point on the plot represents one molecule.  \n",
        "  - The **x-axis** shows molecular weight  \n",
        "  - The **y-axis** shows boiling point  \n",
        "  - **Click** on any point to select that molecule — this will update both the dropdown menu and the 3D viewer automatically  \n",
        "\n",
        "### Colors of Atoms in the 3D Viewer\n",
        "\n",
        "| Atom          | Color (Convention)  |\n",
        "|---------------|----------------------|\n",
        "| Carbon (C)    | **Gray**             |\n",
        "| Hydrogen (H)  | **White**            |\n",
        "| Oxygen (O)    | **Red**              |\n",
        "| Nitrogen (N)  | **Blue**             |\n",
        "| Sulfur (S)    | **Yellow**           |\n",
        "| Phosphorus (P)| **Orange**           |\n",
        "| Fluorine (F)  | **Light green**      |\n",
        "| Chlorine (Cl) | **Green**            |\n",
        "| Bromine (Br)  | **Dark red / Brown** |\n",
        "| Iodine (I)    | **Purple / Violet**  |\n",
        "\n"
      ],
      "metadata": {
        "id": "iJG55vPidNdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Beginning with Linear Alkanes\n",
        "\n",
        "Developing a model to predict boiling points for the wide variety of molecules in the full dataset will be complicated.\n",
        "To build intuition, we will start small: by focusing on just one family of molecules — **the alkanes**.\n",
        "- Later in the lab, we will expand our model to cover the entire dataset\n",
        "\n",
        "### What are alkanes?\n",
        "- **Definition:** Alkanes are hydrocarbons consisting only of **carbon (C)** and **hydrogen (H)** atoms, where the carbons are connected by **single bonds**.  \n",
        "- **Structure:**  We will begin with *linear (n-alkanes):* a straight chain of carbon atoms.  \n",
        "\n",
        "### Why start with alkanes?\n",
        "- **Simplicity:** They are nonpolar, which makes the boiling point primarily dependent on **molecular weight**.\n",
        "- **Controlled variability:** By examining straight-chain, nonpolar molecules we can see how small structural changes affect physical properties.  \n",
        "- **Progressive learning:** Understanding trends in alkanes provides a foundation before moving on to more complex molecules.\n",
        "\n",
        "This progression mirrors the broader goal in AI for molecular sciences: starting with **simple models** and systematically expanding to capture the **complexity of chemical structure–property relationships**.\n",
        "\n",
        "---\n",
        "\n",
        "### Linear Alkanes Dataset Explorer\n",
        "\n",
        "Explore the Linear Alkanes dataset below for the following exercise.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bHZGyZxcRogW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, dataframe_visualization = menglab.load_alkanes()\n",
        "menglab.data_explorer(dataframe_visualization)"
      ],
      "metadata": {
        "id": "Pe7iMT2lJ96P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 4: Exploring the the dataset [4 points]\n",
        "\n",
        "**Tasks:** Use the interactive data explorer to identify key properties of linear alkanes. Answer the following questions in the text boxes below:\n",
        "\n",
        "1. What is the molecular weight of **methane**, and how many carbons does it contain?\n",
        "\n",
        "2. What is the molecular weight of **pentane**, and how many carbons does it contain?\n",
        "\n",
        "3. What is the molecular weight of **decane**, and how many carbons does it contain?\n",
        "\n",
        "4. Does boiling point generally increase or decrease with the size of the molecule?\n",
        "\n",
        "### Enter your answers below:"
      ],
      "metadata": {
        "id": "a-dN--_BUJse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What is the molecular weight of **methane**, and how many carbons does it contain? YOUR SOLUTION HERE\n"
      ],
      "metadata": {
        "id": "V3-V1uI6Ut60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the molecular weight of **pentane**, and how many carbons does it contain? YOUR SOLUTION HERE\n"
      ],
      "metadata": {
        "id": "rUw8oBF_VSkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. What is the molecular weight of **decane**, and how many carbons does it contain? YOUR SOLUTION HERE\n"
      ],
      "metadata": {
        "id": "sNI9qQB2VUGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Does boiling point generally increase or decrease with the size of the molecule? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "bzAGHpcvVU5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Train/Test split\n",
        "\n",
        "When we loaded the minimal alkanes dataset, we also divided it into two parts:\n",
        "\n",
        "- **Training data** (`X_train`, `y_train`)\n",
        "- **Test data** (`X_test`, `y_test`)\n",
        "\n",
        "Inputs and outputs:\n",
        "- `X` indicates the variable is an input (molecular weight)\n",
        "- `y` indicates the variable is an output (boiling point)\n",
        "\n",
        "### Why do we split the data?\n",
        "When building a machine learning model, we want it to *learn patterns* from some examples, but we also need a way to check later if it can make good predictions on molecules it hasn’t seen before.  \n",
        "\n",
        "- The **training set** is what the model looks at to discover the relationship between inputs (like molecular weight) and outputs (boiling point).  \n",
        "- The **test set** is set aside and not touched during training. Later, in Section 5, we will use these molecules to evaluate how well the model is working on these *never-before-seen* datapoints.  \n",
        "\n",
        "### How is the split done?\n",
        "- We take the full dataset and split it into two groups, usually with most of the data in training (e.g. 80%) and the rest in testing (e.g. 20%).  \n",
        "- The split is random so that both groups still represent the overall variety of molecules.  \n",
        "\n",
        "---\n",
        "\n",
        "Think of it like studying for an exam:  \n",
        "- You **practice** with your training examples.  \n",
        "- You then check your understanding with a **new set of test questions** that you haven’t practiced on before.  \n",
        "\n",
        "This way, we’ll be ready to see how well our model performs when it meets new molecules later on.\n"
      ],
      "metadata": {
        "id": "GF3dEcq4VyTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot of training and testing data points\n",
        "plt.plot(X_train, y_train ,'ko', label='training data')\n",
        "plt.plot(X_test, y_test ,'ro', label='testing data')\n",
        "plt.legend()\n",
        "plt.xlabel(\"molecular weight [g/mol]\")\n",
        "plt.ylabel(\"Boiling Point [K]\")\n",
        "plt.title(\"Train/Test split\")"
      ],
      "metadata": {
        "id": "VQl3WbDhWlL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 5: Train/Test split [3 points]\n",
        "\n",
        "**Task:** Explore the train/test split variables created by `load_alkanes()`.\n",
        "\n",
        "1. What type of Python object is each of the following: `X_train`, `X_test`, `y_train`, and `y_test`?  \n",
        "   - **Tip:** Use `print(X_train)` or `print(type(X_train))` (and similarly for the others) to check.\n",
        "\n",
        "2. How many data points are in the training set `N_train` and the testing set `N_test`?  \n",
        "   - **Tip:** Use Python’s built-in `len()` function.\n",
        "\n",
        "3. What is the molecular weight and boiling point of the **first molecule** in the training set?  \n",
        "   - **Tip:** Use square brackets to access elements by index, e.g. `list[index]`.  \n",
        "   - **Hint:** Remember that Python uses **zero-based indexing**, so the first element is at index `0`.\n",
        "\n",
        "### Enter your answers below:\n"
      ],
      "metadata": {
        "id": "NZ1kSr_pWgSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# Run this cell to initialize the variables\n",
        "X_train, X_test, y_train, y_test, _ = menglab.load_alkanes()"
      ],
      "metadata": {
        "id": "lzUtK-xRk4EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What type of variable are `X_train`, `X_test`, `y_train`, `y_test`? YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "7OYgxRfxZma_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "N_train = NotImplemented # YOUR SOLUTION HERE\n",
        "N_test = NotImplemented # YOUR SOLUTION HERE\n",
        "print(N_train)\n",
        "print(N_test)"
      ],
      "metadata": {
        "id": "FBsD8qnnXr0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "weight_first_train_molecule = NotImplemented # YOUR SOLUTION HERE\n",
        "bp_first_train_molecule = NotImplemented # YOUR SOLUTION HERE\n",
        "print(f\"the weight of the first training molecule is : {weight_first_train_molecule:.2f} g/mol.\")\n",
        "print(f\"the boiling point of the first training molecule is : {bp_first_train_molecule:.2f} K.\")"
      ],
      "metadata": {
        "id": "GC-nsxY8bFq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Linear Regression\n"
      ],
      "metadata": {
        "id": "LgAf_DbXsGk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Writing and Plotting our First Model\n",
        "\n",
        "We’ll start with a very simple machine learning model called **Linear Regression**.  \n",
        "\n",
        "This model assumes that the relationship between the input and the output can be described by a straight line.  \n",
        "\n",
        "The equation looks like this:\n",
        "\n",
        "$$ \\hat{y} = wx + b $$\n",
        "\n",
        "where:  \n",
        "- $x$ = the input (in our case, the **molecular weight**)  \n",
        "- $\\hat{y}$ = the output (the **predicted boiling point**)  \n",
        "- $w$ = the **slope** of the line (how steep the line is)  \n",
        "- $b$ = the **intercept** (the point where the line crosses the y-axis)  \n",
        "\n",
        "---\n",
        "\n",
        "### What do $w$ and $b$ mean?\n",
        "Think of $w$ and $b$ as the **adjustable knobs** that let us fit a line to the data:  \n",
        "- **$w$ (weight or slope)** controls the slope, or how strongly the input $x$ affects the output $\\hat{y}$.  \n",
        "- **$b$ (bias or intercept)** shifts the line up or down on the graph.  \n",
        "\n",
        "Our goal in machine learning is to find the **best values** for $w$ and $b$ so that the line matches the training data as closely as possible.  \n",
        "\n",
        "- In the next section, we’ll learn how to define what “best” means using a **loss function**.  \n",
        "- After that, we’ll see how to actually *find* these best values using a process called **optimization**.  \n",
        "\n",
        "---\n",
        "\n",
        "**Tip:** In algebra class, the slope is often written as $m$.  \n",
        "In machine learning, we usually use $w$, which stands for **weight**—showing how much influence the input has on the output.\n"
      ],
      "metadata": {
        "id": "chPDqRTZyvXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 6 : Write a Linear Regression Model [4 points]\n",
        "\n",
        "**Task:** Implement and test a simple linear regression model step by step.\n",
        "\n",
        "1. **Write the model function**:  \n",
        "   Fill in the code for the function called `model(x, w, b)` that returns predictions $\\hat{y}$ using the linear regression equation:  \n",
        "\n",
        "   $$ \\hat{y} = w x + b $$\n",
        "   \n",
        "2. **Initialize parameters**:  \n",
        "  assign the following initial values for $w$ and $b$:  \n",
        "    - $w = 1$  \n",
        "    - $b = 1$  \n",
        "\n",
        "3. **Make predictions**:  \n",
        "Use your `model` function to calculate the predicted boiling points for all the training molecular weights in `X_train`.  \n",
        "\n",
        "   - **Hint:** Your `model` function should work with both single inputs and lists/arrays:  \n",
        "     - `model(x_val, w, b)` → returns a single prediction (scalar).  \n",
        "     - `model(x_list, w, b)` → returns multiple predictions (list or array).  \n",
        "   - For this task, pass the entire `X_train` list so you get predictions for all the training molecules at once.\n",
        "\n",
        "4. **Visualize**  \n",
        "   Modify the code cell to compare your model’s predictions against the actual data.  \n",
        "   - plot the training features (X) vs targets (y)\n",
        "   - plot the training features (X) vs model predictions calculated in task 3 above.\n"
      ],
      "metadata": {
        "id": "2QidT1OS8r9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: write the model function\n",
        "def model(x, w, b):\n",
        "    \"\"\"Linear Regression Model\n",
        "    Parameters\n",
        "    ---\n",
        "      x: model input, molecular weight [g/mol]\n",
        "      w: model 'weight' parameter\n",
        "      b: model 'bias' parameter\n",
        "\n",
        "    Returns\n",
        "    ---\n",
        "      y_hat: model output (predicted boiling point, [K])\n",
        "\n",
        "    \"\"\"\n",
        "    y_hat = NotImplemented ### YOUR SOLUTION HERE\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "VYA_wHwxuiRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: initialize parameters\n",
        "w = NotImplemented ### YOUR SOLUTION HERE\n",
        "b = NotImplemented ### YOUR SOLUTION HERE\n",
        "print('Initial parameters: w =', w, ' b =', b)\n"
      ],
      "metadata": {
        "id": "PzwCdl2Yupju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: make predictions\n",
        "y_prediction = NotImplemented ### YOUR SOLUTION HERE\n",
        "print(\"Predicted boiling points: \", y_prediction)\n",
        "\n",
        "# Check your answer - this should assert statement should pass!\n",
        "answer = np.array([ 17.043, 31.07,  45.097, 59.124,  87.178, 101.205, 115.232, 129.259])\n",
        "assert(np.allclose(y_prediction, answer, rtol=0.01))\n",
        "print(\"✔️ Test passed! Your predictions looks correct.\")"
      ],
      "metadata": {
        "id": "J-Ui3SdsxETh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: execute to visualize untrained regression model predictions\n",
        "plt.plot(NotImplemented, NotImplemented, 'ko', label='training data')     # YOUR SOLUTION HERE\n",
        "plt.plot(NotImplemented, NotImplemented, '-ro', label='model prediction') # YOUR SOLUTION HERE\n",
        "plt.xlabel(\"molecular weight\")\n",
        "plt.ylabel(\"boiling point\")\n",
        "plt.title(\"Untrained Linear Regression Model\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "OdjNSfJWwkPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Loss Function\n",
        "\n",
        "Now we have a model, but it doesn't look so great:\n",
        "- We need a way to measure **how well (or poorly)** it makes predictions.  \n",
        "- This is the role of the **loss function**.  \n",
        "\n",
        "A loss function is a mathematical formula that measures the difference between the model’s predictions and the true values in the training data:  \n",
        "- If the predictions are close to the actual boiling points, the loss will be **small**.  \n",
        "- If the predictions are far from the actual values, the loss will be **large**.  \n",
        "\n",
        "---\n",
        "\n",
        "### Analogy: Throwing Darts\n",
        "Imagine each data point is a dart throw:  \n",
        "- The bullseye is the true value.  \n",
        "- Where your dart lands is the model’s prediction.  \n",
        "- The distance from the bullseye is the **error** for that single prediction.  \n",
        "\n",
        "The loss function doesn’t just look at one dart — it looks at **all the darts together**.  \n",
        "It adds up (or averages) the distances from all throws to give an **overall score** of how well the model is doing.  \n",
        "\n",
        "- If most darts land close to the bullseye, the total loss will be **low**.  \n",
        "- If many darts land far away, the total loss will be **high**.  \n",
        "\n",
        "So the loss function measures the **aggregate error** across all predictions, not just one.\n",
        "\n"
      ],
      "metadata": {
        "id": "dpjuJJiVeLfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Root Mean Squared Error (RMSE)\n",
        "\n",
        "For this project, we will use the **Root Mean Squared Error (RMSE)** as our loss function.  \n",
        "\n",
        "It is defined as:\n",
        "\n",
        "$$\n",
        "L(w, b) = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^{N} \\big( y_i - \\hat{y}_i \\big)^2 }\n",
        "$$\n",
        "\n",
        "**Note:** Sometimes the square root does not render properly; (the square root should extend over the entire expression).\n",
        "\n",
        "where:  \n",
        "- $N$ = number of training examples  \n",
        "- $y_i$ = the true boiling point of example $i$  \n",
        "- $\\hat{y}_i = wx_i + b$, the predicted boiling point from our model for example $i$  \n",
        "- $L(w, b)$ = the overall loss, given the current $w$ and $b$  \n",
        "\n",
        "---\n",
        "\n",
        "### Intuition  \n",
        "\n",
        "1. **Error for one data point**  \n",
        "   The difference between the true value and the prediction:  \n",
        "   $$\n",
        "   \\text{error}_i = y_i - \\hat{y}_i\n",
        "   $$\n",
        "\n",
        "2. **Squared error**  \n",
        "   Squaring ensures all errors are positive and penalizes larger mistakes more strongly:  \n",
        "   $$\n",
        "   \\text{squared_error}_i = (y_i - \\hat{y}_i)^2\n",
        "   $$\n",
        "\n",
        "3. **Mean squared error (MSE)**  \n",
        "   Taking the average gives the overall error across all data points:  \n",
        "   $$\n",
        "   \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
        "   $$\n",
        "\n",
        "4. **Root mean squared error (RMSE)**  \n",
        "   Finally, we take the square root to bring the loss back into the **same units as $y$**  \n",
        "   (in this case, degrees Kelvin):  \n",
        "   $$\n",
        "   \\text{RMSE} = \\sqrt{\\text{MSE}}\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "### Interpreting RMSE with Examples  \n",
        "\n",
        "RMSE tells us, on average, how far our predictions are from the true values (in the same units as the target).  \n",
        "\n",
        "- If **RMSE = 5**, the model’s predictions are typically about **5 K off** from the true boiling points.  \n",
        "- If **RMSE = 100**, predictions are very inaccurate — on average about **100 K away** from the true boiling points.  \n",
        "\n",
        "Smaller RMSE values mean the model is making more accurate predictions.  \n",
        "\n",
        "\n",
        "### Why RMSE?  \n",
        "\n",
        "- **Same units as the target**: RMSE has the the same units as the model output (Kelvin here). This makes the error easier to interpret.  \n",
        "- **Penalizes large errors**: Because errors are squared before averaging, RMSE emphasizes big mistakes more strongly than small ones.  \n",
        "- **Widely used**: RMSE is a standard metric in regression problems, so results are easy to compare with other models and datasets.  \n",
        "\n",
        "---\n",
        "\n",
        "### Goal  \n",
        "\n",
        "Our objective is to **find the values of $w$ and $b$ that make $L(w, b)$ as small as possible**.  \n",
        "This means the model’s predictions are as close as possible to the true boiling points.  \n"
      ],
      "metadata": {
        "id": "MkuMdcWF0C_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 7: Write Loss Function [5 points total]\n",
        "\n",
        "**Task:** Implement and test the loss function (RMSE).\n",
        "\n",
        "1. **Write the loss function** [4 points]\n",
        "\n",
        " Fill in the code for the function called `loss_function(y, y_prediction)` that computes the **Root Mean Squared Error (RMSE)** between the true values and the predictions.  \n",
        "\n",
        "   - `y` and `y_prediction` are **lists or arrays** of length $N$, representing the true and predicted boiling points for $N$ molecules.  \n",
        "   - Break the calculation into four steps:  \n",
        "     1. Calculate the `error`  \n",
        "     2. Calculate the `squared_error`  \n",
        "     3. Calculate the `mean_squared_error`  \n",
        "     4. Calculate the `root_mean_squared_error`  \n",
        "\n",
        "   **Tip:** Use NumPy functions like `np.square()`, `np.mean()`, and `np.sqrt()` to apply operations to entire arrays at once.  \n",
        "\n",
        "2. **Evaluate the loss function** [1 point]\n",
        "   Compute the RMSE for the **training data** using $w = 1$ and $b = 1$.  \n",
        "\n",
        "   **Tip:** First use your `model` function to generate predictions (`y_prediction`) from the training inputs, then pass those predictions into your `loss_function`.  \n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "G_JRC4Ld54qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 Write the loss function\n",
        "def loss_function(y, y_prediction):\n",
        "    \"\"\"Calculates model loss (RMSE)\n",
        "    Parameters\n",
        "    ---\n",
        "      y: List (or numpy array) of true molecular boiling points [K]\n",
        "      y_prediction: List (or numpy array) of predicted molecular boiling points [K]\n",
        "\n",
        "    Returns\n",
        "    ---\n",
        "      root_mean_squared_error: RMSE loss (floating point value)\n",
        "    \"\"\"\n",
        "    error = NotImplemented ### YOUR SOLUTION HERE\n",
        "    squared_error = NotImplemented ### YOUR SOLUTION HERE\n",
        "    mean_squared_error = NotImplemented ### YOUR SOLUTION HERE\n",
        "    root_mean_squared_error = NotImplemented ### YOUR SOLUTION HERE\n",
        "    return root_mean_squared_error"
      ],
      "metadata": {
        "id": "trtXfdfp8IMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2 Evaluate loss function\n",
        "y_prediction = NotImplemented ### YOUR SOLUTION HERE\n",
        "rmse = NotImplemented ### YOUR SOLUTION HERE\n",
        "print(\"RMSE is: {:.2f} K\".format(rmse))\n",
        "\n",
        "# Check your answer - this should assert statement should pass!\n",
        "assert(np.isclose(rmse, 228.56, rtol=0.01))\n",
        "print(\"✔️ Test passed! Your RMSE looks correct.\")"
      ],
      "metadata": {
        "id": "QVt51yNM9t71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2  Visualizing the Loss Landscape\n",
        "\n",
        "For every choice of $w$ and $b$, we can calculate the loss of our model.  \n",
        "In other words, the loss itself is a **function**: it takes two inputs ($w$ and $b$) and produces one output (the loss value).  \n",
        "\n",
        "\n",
        "This makes $L(w, b)$ a function of **two variables**:  \n",
        "- In algebra, you may have first seen functions of one variable (e.g., $f(x) = x^2$).  \n",
        "  - These are easy to draw in 2D, with one axis for the input $x$ and one for the output $f(x)$.  \n",
        "- With **two inputs** (here, $w$ and $b$), the function lives in 3D:  \n",
        "  - the x-axis for $w$  \n",
        "  - the y-axis for $b$  \n",
        "  - the z-axis for $L(w, b)$, the loss value  \n",
        "\n",
        "\n",
        "### Heat Maps  \n",
        "\n",
        "Another way to visualize a 3D surface is with a **heat map**:  \n",
        "\n",
        "- Instead of plotting the surface in 3D, we use **color** to represent the loss at each point $(w, b)$.  \n",
        "- Dark or cool colors (like blue) indicate **low loss**, while bright or warm colors (like yellow or red) indicate **high loss**.  \n",
        "- This helps us spot the “valley” (low-loss region) surrounded by higher-loss regions.  \n",
        "\n",
        "On the heat map, the **optimal solution** appears as the **darkest point** — the bottom of the valley.  \n",
        "\n",
        "### Try It Out  \n",
        "\n",
        "Run the code cell below to visualize the **interactive loss landscape** using both a 3D surface plot and a heat map.  \n",
        "\n",
        "- **Tip:** Click, drag, and zoom to explore the interactive 3D plot.\n"
      ],
      "metadata": {
        "id": "EBfF8VYK0vAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.interactive_loss_landscape(X_train, y_train)"
      ],
      "metadata": {
        "id": "LwgSLRktVD7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreting the Landscape\n",
        "The loss landscape looks like a set of hills and valleys:  \n",
        "- **High points** = bad models (large loss)  \n",
        "- **Low points** = good models (small loss)  \n",
        "- The **optimal solution** is the very bottom of the valley — the point where $L(w, b)$ is smallest.  \n",
        "  \n",
        "We will find the optimal solution at the bottom of the valley by **training** the model.\n",
        "\n",
        "\n",
        "### Why can we plot this?\n",
        "We can make this visualization **only because our model is simple and inexpensive to evaluate**\n",
        "- We can quickly compute the loss for thousands of different pairs of $(w, b)$.  \n",
        "- Plotting these values creates a visual 3D surface of the **loss landscape**.\n",
        "\n",
        "For more complex models with **many parameters** (hundreds, thousands, or even billions), this is no longer possible:  \n",
        "- The loss landscape still exists, but it lives in extremely high dimensions (one dimension for each parameter).\n",
        "- We cannot directly visualize it or compute the loss for every combination of paramter values.\n",
        "\n",
        "**In this simple case, we are “cheating\" a little by peeking at something that normally remains hidden in machine learning.**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1xG5R51f4ZQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 8: Interactive Loss Landscape [2 points]\n",
        "\n",
        "**Task:** Use the interactive loss landscape to answer the following questions:\n",
        "\n",
        "1. Approximately what values of $w$ (slope) and $b$ (intercept) give the optimal fit for this linear regression model?\n",
        "\n",
        "2. Approximately what is the RMSE loss at this optimal point?  \n",
        "\n",
        "**Hint:** Mouse over either of the two plots of the loss landscape to get `(x,y,z)` triplets corresponding to particular points on the surface.\n",
        "\n",
        "### Enter your answers in the code cells below:\n"
      ],
      "metadata": {
        "id": "0aN4PShScMEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "w_approx = NotImplemented ### YOUR SOLUTION HERE\n",
        "b_approx = NotImplemented ### YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "aqqhZHnOclyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "loss_approx = NotImplemented ### YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "v2ZH4c1tfOn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Training\n",
        "\n",
        "So far, we’ve defined our model equation and learned how to measure its performance using a **loss function**.  \n",
        "Now comes the next step: **training the model**.  \n",
        "\n",
        "Training the model means **finding the lowest point in the loss landscape**.  \n",
        "- In simple cases (like Section 3), we could visualize the entire landscape to see where the minimum lies.  \n",
        "- For most real-world models, however, the landscape is far too complex and high-dimensional to plot.  \n",
        "- Instead, we need to use clever techniques to search for the low point without seeing the whole landscape.  \n"
      ],
      "metadata": {
        "id": "Rwo_5ITo-43W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Manual Training  \n",
        "\n",
        "In practice, training is usually done with algorithms that automatically adjust the parameters $w$ (slope) and $b$ (intercept) so the loss decreases step by step.  \n",
        "\n",
        "Before we get there, we’ll start by **manual training**:  \n",
        "- Hand-tuning $w$ and $b$ ourselves.  \n",
        "- Observing how these changes affect the model’s predictions and the loss.  "
      ],
      "metadata": {
        "id": "wHpT24reyFO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 9 : Manually Train the model [2 points]\n",
        "\n",
        "**Task:** Use the interactive sliders to manually adjust the parameters $w$ (slope) and $b$ (intercept) in order to reduce the loss.  \n",
        "Your goal is to bring the RMSE within **20% of the minimum value**.  \n",
        "\n",
        "As you move the sliders:  \n",
        "- The line on the plot will tilt and shift.  \n",
        "- The loss value (RMSE) will update in real time to show how well your chosen line fits the data.  \n",
        "- Keep tuning $w$ and $b$ until the loss is close enough to the minimum.  \n",
        "\n",
        "**Tips:**  \n",
        "- A **small $w$** makes the line flatter.  \n",
        "- A **large $w$** makes the line steeper.  \n",
        "- Changing **$b$** moves the line up or down.  \n",
        "- Adjust $w$ and $b$ in the direction that makes the **RMSE** go down.  \n",
        "\n",
        "**Run the code cell below to launch the interactive plot and try manual training.**  \n"
      ],
      "metadata": {
        "id": "gX4DPfHkThYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.manually_train_linear_regression(X_train, y_train)"
      ],
      "metadata": {
        "id": "_vWWZcbvAIR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Enter your solution (\"trained\" w and b values) below:**"
      ],
      "metadata": {
        "id": "ugi-gRrG-KP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: enter your trained w and b parameter values\n",
        "w = NotImplemented ### YOUR SOLUTION HERE\n",
        "b = NotImplemented ### YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "i3EjkVcB-Tan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Gradient Descent: Training the Model Automatically\n",
        "\n",
        "So far, you’ve tried **manually adjusting** $w$ and $b$ to get a good fit.  \n",
        "That was a useful intuition, but it’s not practical when we have millions of data points or thousands of parameters.  \n",
        "We need a way for the **computer** to do this automatically.  \n",
        "\n",
        "This is where **gradient descent** comes in.   \n",
        "\n",
        "---\n",
        "\n",
        "### The Big Idea\n",
        "Gradient Descent is like a game of **“hot and cold”**.\n",
        "- Our goal is to make the **loss function** as small as possible.  \n",
        "- The loss function is like a **landscape of hills and valleys**, where:  \n",
        "  - The height of the land = the loss value $L(w, b)$  \n",
        "  - The lowest valley = the best parameters (the minimum loss)  \n",
        "- We start somewhere on this landscape (with random $w$ and $b$) and **take steps downhill until we reach the bottom.**  \n",
        "\n",
        "The Big Idea is:  \n",
        "- The **slope** of the land tells us which way is uphill.  \n",
        "- If we step *with the slope*, we’ll go uphill (bad).  \n",
        "- If we step *against the slope*, we’ll go downhill (good).  \n",
        "\n",
        "So gradient descent is just a method of **taking repeated small steps downhill** until we reach the bottom of the valley.\n",
        "\n",
        "---\n",
        "\n",
        "### How It Works (Step by Step)\n",
        "1. **Start** with random values for $w$ and $b$.  \n",
        "2. **Compute the loss** (how far predictions are from the true values).  \n",
        "3. **Find the slope** of the loss function (the direction of steepest downhill).  \n",
        "4. **Update $w$ and $b$** by taking a small step in that downhill direction.  \n",
        "5. **Repeat** steps 2–4 many times until the loss stops improving.  \n",
        "\n",
        "---\n",
        "\n",
        "### The Update Rule\n",
        "In math form, each update looks like this:\n",
        "\n",
        "$$\n",
        "w_{new} = w_{old} - \\alpha \\cdot \\frac{\\partial L}{\\partial w}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_{new} = b_{old} - \\alpha \\cdot \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "Don’t be scared by the symbols! Let’s break it down:  \n",
        "- $\\frac{\\partial L}{\\partial w}$ means *how much the loss changes if we increase $w$*. (The slope with respect to $w$.)  \n",
        "- $\\frac{\\partial L}{\\partial b}$ means *how much the loss changes if we increase $b$*. (The slope with respect to $b$.)\n",
        "- $\\alpha$ (alpha) is called the **learning rate**, which controls how big our steps are.  \n",
        "- notice the *subtraction* signs, because we want to step downhill, not uphill!\n",
        "---\n",
        "\n",
        "\n",
        "### How do we get the slope?\n",
        "\n",
        "To figure out which way is downhill, we need the **slope of the loss function** at our current position.\n",
        "- In fact, these formulas can be **derived using tools from calculus**. For now, you don’t need to worry about the details—just know that they tell us:\n",
        "\n",
        "  - How does the loss change if we increase $w$ a little bit?\n",
        "\n",
        "  - How does the loss change if we increase $b$ a little bit?\n",
        "\n",
        "Without going through the derivations, here are the equations:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w} = \\frac{2}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)\\,x_i\n",
        "$$  \n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial b} = \\frac{2}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)\n",
        "$$  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Intuition with an Analogy\n",
        "Imagine you’re **hiking in the mountains on a foggy day**:  \n",
        "- You can’t see the bottom of the valley, but you want to get there.  \n",
        "- You feel the ground around you to figure out which way slopes downward.  \n",
        "- You take a small step downhill.  \n",
        "- Repeat until you reach the valley floor.  \n",
        "\n",
        "Next, we’ll implement Gradient Descent in code so you can watch $w$ and $b$ being updated automatically until the model finds its best fit.  \n"
      ],
      "metadata": {
        "id": "ccUA3IEJKE8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 10: Write Gradient Descent Step [4 points total]\n",
        "\n",
        "You are provided with a helper function `gradient(w, b)` that returns the slopes $(\\frac{\\partial L}{\\partial w}$, $\\frac{\\partial L}{\\partial b})$ in the w and b directions according to the equations above derived using calculus.\n",
        "\n",
        "Now we will implement the **gradient descent algorithm** step by step.  \n",
        "\n",
        "**Tasks:** [2 points each]\n",
        "\n",
        "1. **Implement one gradient descent step.**\n",
        "\n",
        "    - Complete the function `gradient_descent_step(w, b, learning_rate)`.\n",
        "\n",
        "    - This function takes in the current parameter values $w$, $b$, and a learning rate $\\alpha$.\n",
        "\n",
        "   - Inside the function:  \n",
        "     - Call `gradient(w, b)` to compute the slopes with respect to $w$ and $b$.  \n",
        "     - Apply the **gradient descent update rule** to calculate the new values of $w$ and $b$.  \n",
        "     - Return the updated $w$ and $b$.\n",
        "\n",
        "2. **Take one gradient descent step**\n",
        "   - Start from $w = 1$, $b = 1$.  \n",
        "   - Use `gradient_descent_step` with a learning rate of $\\alpha$ = 0.00002.  \n",
        "\n",
        "   - Print the new values of $w$ and $b$ after one update step.  \n"
      ],
      "metadata": {
        "id": "gDqhW_6Nerni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient function (provided)\n",
        "# Run this cell to define the function so you can use it later\n",
        "def gradient(w, b):\n",
        "  '''Calculates the gradient of the loss function\n",
        "  Parameters\n",
        "  ---\n",
        "    w: model weight parameter\n",
        "    b: model bias parameter\n",
        "\n",
        "  Returns\n",
        "  ---\n",
        "    slope_w: (dL/dw) Slope of the loss function in the w direction\n",
        "    slope_b: (dL/db) Slope of the loss function in the b direction\n",
        "  '''\n",
        "  y_pred = model(X_train, w, b)\n",
        "  error = y_pred - y_train\n",
        "  N = len(y_pred)\n",
        "\n",
        "  slope_w = 2/N * np.dot(error, X_train)\n",
        "  slope_b = 2/N * np.sum(error)\n",
        "  return slope_w, slope_b"
      ],
      "metadata": {
        "id": "R5j1dGl7evsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Implement gradient descent step\n",
        "def gradient_descent_step(w, b, learning_rate):\n",
        "  '''Takes one step of gradient descent, returns updated w and b parameters\n",
        "  Parameters\n",
        "  ---\n",
        "    w: model weight parameter\n",
        "    b: model bias parameter\n",
        "    learning rate: gradient descent learning rate\n",
        "\n",
        "  Returns\n",
        "  ---\n",
        "    w: new model weight parameter after one gradient descent step\n",
        "    b: new model bias parameter after one gradient descent step\n",
        "  '''\n",
        "  slope_w, slope_b = NotImplemented ### YOUR SOLUTION HERE\n",
        "  w = NotImplemented ### YOUR SOLUTION HERE\n",
        "  b = NotImplemented ### YOUR SOLUTION HERE\n",
        "  return w, b"
      ],
      "metadata": {
        "id": "A1k5nslJgKRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Take one gradient descent step\n",
        "w = 1\n",
        "b = 1\n",
        "learning_rate = 0.00002\n",
        "w_new, b_new = NotImplemented ### YOUR SOLUTION HERE\n",
        "print(\"New values for w: \", w_new, \", and b: \", b_new)\n",
        "\n",
        "# Check your answer - this should assert statement should pass!\n",
        "assert(np.allclose(np.array([w_new, b_new]), np.array([1.73, 1.01]), rtol=0.01))\n",
        "print(\"✔️ Test passed! Your predictions looks correct.\")"
      ],
      "metadata": {
        "id": "bOSj_0dzmtHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Interactive Gradient Descent, Step by Step\n",
        "\n",
        "This interactive tool lets you **train a linear regression model** using **gradient descent** in a step-by-step fashion.  \n",
        "The interface combines three synchronized plots with controls for stepping through training, resetting, and adjusting parameters.  \n",
        "\n",
        "Detailed instructions for how to use the interactive gradient descent interface are provided below the figure."
      ],
      "metadata": {
        "id": "jKb7SXYfoP8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.interactive_gradient_descent(X_train, y_train)"
      ],
      "metadata": {
        "id": "dz-oHqh9UHfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to Use Interactive Gradient Descent\n",
        "\n",
        "- **Controls**  \n",
        "  - **Step**: take one (or more) gradient descent steps.\n",
        "  - **Steps per click**: how many steps to take each time you press *Step*.  \n",
        "  - **Reset**: return to the optimal solution.  \n",
        "  - **Learning rate (α)**: controls the size of each step.  \n",
        "  - **Init w / Init b**: set custom starting values for slope ($w$) and intercept ($b$).  \n",
        "  - **Apply start**: apply your chosen initialization.  \n",
        "  - **Auto-run**: continue stepping automatically until convergence or a maximum number of iterations.  \n",
        "\n",
        "- **Plots**  \n",
        "  1. **Loss Heatmap**  \n",
        "     - Heatmap of RMSE as a function of $w$ and $b$.  \n",
        "     - The gradient descent trajectory is shown as a path.  \n",
        "  2. **Model vs Training Data**  \n",
        "     - Scatter plot of the training data.  \n",
        "     - Displays the current regression line and the optimally trained reference line.  \n",
        "  3. **Training Curve (RMSE vs Iteration)**  \n",
        "     - Tracks how the loss changes across iterations.  \n",
        "     - Should decrease as the model improves.  "
      ],
      "metadata": {
        "id": "yaWM7-7h12GO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 11: Interactive Gradient Descent [3 points]\n",
        "\n",
        "**Tasks** Experiment with different settings with interactive Gradient Descent. For the following questions, employ an initial starting point of $w = 1$, $b=1$.\n",
        "\n",
        "1. Approximately how many steps does it take to reach the optimum using a learning rate of 0.2?\n",
        "\n",
        "2. What happens if you decrease the learning rate to 0.005? Approximately many steps does it take to reach the optimum?\n",
        "\n",
        "    **Hint:** Click the \"Apply Start\" box to return to your initial settings for $w$ and $b$.\n",
        "\n",
        "    **Hint:** Check the \"auto-run\" box to automatically take 500 steps\n",
        "\n",
        "3. What happens if you increase the learning rate $\\alpha$ to 1.2? Provide a description and explanation for the behavior you are seeing. Does the algorithm ever reach the optimum value?\n",
        "\n",
        "    **Hint:** If odd behavior is occuring, make sure to uncheck the \"auto-run\" box.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "akhIkeFy5Kpt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "iAgldI816RYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "EUmEpLG26VA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "k2V6umsT6VQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 12: Write a Gradient Descent Loop [2 points]\n",
        "\n",
        "**Task:** Implement a full automatic gradient descent loop and plot the loss over time.\n",
        "\n",
        "1. **Complete the function `train(w, b, learning_rate, n_steps)`**  \n",
        "   - In each iteration of the `for` loop, call your previously written `gradient_descent_step(w, b, learning_rate)` to update `w` and `b`.  \n",
        "\n",
        "2. **Train and visualize**  \n",
        "   - Start from **$w = 1$**, **$b = 1$**, with **learning rate = 0.0001**.  \n",
        "   - Run for **100,000 steps**.  \n",
        "   - Execute the cell to plot **loss vs. gradient descent step**.\n",
        "\n",
        "> **Note:** This implementation differs slightly from the one used in the previous 'interactive gradient descent' tool.  \n",
        "> As a result, the loss values for the same learning rate will not match exactly.\n"
      ],
      "metadata": {
        "id": "WDbgEz5rq3uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1 Complete the 'train' function\n",
        "def train(w, b, learning_rate, n_steps):\n",
        "  '''Trains linear regression model using gradient descent for n_steps\n",
        "  Parameters:\n",
        "  ---\n",
        "    w: model slope\n",
        "    b: model intercept\n",
        "    learning_rate: gradient descent learning rate\n",
        "    n_steps: number of gradient descent steps to take\n",
        "\n",
        "  Returns:\n",
        "  ---\n",
        "    w: trained model slope\n",
        "    b: trained model intercept\n",
        "    loss_history: list of model loss values for each gradient step.\n",
        "  '''\n",
        "  loss_history = []\n",
        "\n",
        "  for i in range(n_steps):\n",
        "    # take one step of gradient descent\n",
        "    w, b = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "    # calculate loss and append it to a loss_history list\n",
        "    y_prediction = model(X_train, w, b)\n",
        "    loss = loss_function(y_train, y_prediction)\n",
        "    loss_history.append(loss)\n",
        "\n",
        "  return w, b, loss_history"
      ],
      "metadata": {
        "id": "7xgyjvpeAJY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2, train and visualize\n",
        "learning_rate = NotImplemented ### YOUR SOLUTION HERE\n",
        "w_init = NotImplemented ### YOUR SOLUTION HERE\n",
        "b_init = NotImplemented ### YOUR SOLUTION HERE\n",
        "n_steps = NotImplemented ### YOUR SOLUTION HERE\n",
        "w, b, loss_history = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "plt.plot(loss_history)\n",
        "plt.title(\"Model Loss vs Gradient Descent Iteration\")\n",
        "plt.xlabel(\"Gradient Descent step number\")\n",
        "plt.ylabel(\"Model Loss (RMSE) [K]\")"
      ],
      "metadata": {
        "id": "_0Wdkjb6BJv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Model Evaluation\n",
        "We trained our model using the **training data**. Now it’s time to answer a simple question:\n",
        "\n",
        "> **How well does the model work on molecules it hasn’t seen before?**\n",
        "\n",
        "That’s exactly what the **test set** (`X_test`, `y_test`) is for. We kept these molecules separate so we could make an honest check at the end.\n",
        "\n",
        "---\n",
        "\n",
        "### What “evaluation on the test set” means\n",
        "1. **Freeze the model** (no more training or tweaking).\n",
        "2. **Make predictions** for `X_test`.\n",
        "3. **Compare** those predictions to the true values `y_test` using the `loss_function`"
      ],
      "metadata": {
        "id": "lLPvxd-vrCEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Performance Plots  \n",
        "\n",
        "We can visualize model performance with two types of plots:  \n",
        "\n",
        "---\n",
        "\n",
        "### Regression Plot  \n",
        "- This plot shows the **training data points** (molecular weight vs. boiling point) along with the **regression line** predicted by our model.  \n",
        "- It helps us see how well the model’s line fits the actual data distribution.  \n",
        "- A good fit will have the line passing close to most of the data points.  \n",
        "\n",
        "---\n",
        "\n",
        "### Parity Plot  \n",
        "- **Figure 2** from the Paper we read in section 0 is a **parity plot**!\n",
        "- This plot compares the **true boiling points** (on the x-axis) against the **model’s predicted boiling points** (on the y-axis).  \n",
        "- If the model were perfect, every point would fall exactly on the **diagonal line $y = x$**.  \n",
        "- Points above the diagonal are **over-predictions** (model predicts too high),  \n",
        "  while points below the diagonal are **under-predictions** (model predicts too low).  \n",
        "- The closer the points cluster around the diagonal, the better the model’s accuracy.  \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Together, these plots give us both a **visual check of fit** (regression plot) and a **direct measure of prediction accuracy** (parity plot).  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G-FwgyPwFQFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# w and b are defined in the previous section, from model training.\n",
        "menglab.plot_performance(w, b, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "ZQ1VIJ5BF31T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Model Generalization  \n",
        "\n",
        "The performance plots give us a **visual check** of how well the model fits the data, but the **ultimate measure of performance** is a **numerical metric**.  \n",
        "In our case, that metric is the **RMSE loss on the testing set**.  \n",
        "\n",
        "- The **training RMSE** measures how well the model fits the data it was trained on.  \n",
        "- The **testing RMSE** measures how well the model can make predictions on new, unseen data.  \n",
        "\n",
        "---\n",
        "\n",
        "The testing loss is the **true benchmark** of performance.  \n",
        "\n",
        "- If the testing loss is **close to the training loss**, the model is able to **generalize** its predictions to new data.  \n",
        "- If the testing loss is **much higher than the training loss**, the model is not **generalizing** to new data.\n",
        "- A common pitfall is to focus only on minimizing training loss.  \n",
        "  - But training loss **doesn’t matter on its own**—what matters is whether the model performs well on unseen data.  \n",
        "\n",
        "\n",
        "The goal in machine learning is not just to fit the training set, but to build a model that can **generalize**.  \n",
        "\n"
      ],
      "metadata": {
        "id": "WERM2KiGHa6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 13: Test set performance [3 points]\n",
        "\n",
        "**Tasks:**  \n",
        "\n",
        "1. **Make predictions**  \n",
        "   - Use your `model` function to calculate predictions for both the training data (`y_pred_train`) and the testing data (`y_pred_test`).\n",
        "   - optimal values for w (`w_opt`) and b (`b_opt`) are provided for grading purposes, but would otherwise be determined through training.\n",
        "\n",
        "2. **Calculate RMSE loss**  \n",
        "   - Compute the RMSE loss for the model on both the training and testing sets.  \n",
        "   - Run the code cell to generate a bar plot comparing training vs. testing loss.  \n",
        "\n",
        "3. **Interpret the results**  \n",
        "   - Which is higher: the loss on the training set or the loss on the testing set?  \n",
        "   - **What does this tell you about the model’s ability to generalize? (Is it generalizing?)**\n"
      ],
      "metadata": {
        "id": "3IEe9eEam8v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# Execute this cell to define the variables\n",
        "w_opt = 2.66\n",
        "b_opt = 99.7"
      ],
      "metadata": {
        "id": "K4VP3GsDnZx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "y_pred_train = NotImplemented # YOUR SOLUTION HERE\n",
        "y_pred_test = NotImplemented # YOUR SOLUTION HERE\n",
        "print(f\"The predictions for the model on the training data are: {y_pred_train}\")\n",
        "print(f\"The predictions for the model on the testing data are: {y_pred_test}\")"
      ],
      "metadata": {
        "id": "wG8LeEs50J2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "loss_train = NotImplemented # YOUR SOLUTION HERE\n",
        "loss_test = NotImplemented # YOUR SOLUTION HERE\n",
        "print(f\"The loss for the model on the training data is: {loss_train:1f}\")\n",
        "print(f\"The loss for the model on the testing data is: {loss_test:.1f}\")"
      ],
      "metadata": {
        "id": "8BWueqzlnw2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Plot results and Analyze (enter your answer in the text cell below)\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "bars = ax.bar([\"Train\", \"Test\"], [loss_train, loss_test],\n",
        "              color=[\"steelblue\", \"seagreen\"], alpha=0.85)\n",
        "\n",
        "# Add value labels inside bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2., height/2,\n",
        "            f\"{height:.1f}\",\n",
        "            ha='center', va='center', fontsize=11, color=\"white\", fontweight=\"bold\")\n",
        "\n",
        "ax.set_ylabel(\"RMSE Loss\")\n",
        "ax.set_title(\"Model Performance: Training vs Testing Loss\")\n",
        "ax.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L6o_1UNX0j_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Which is higher: the loss on the training set or the loss on the testing set?  What does this tell you about the model’s ability to generalize? (Is it generalizing?)\n",
        "YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "H1IlfkkrpMGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Improving the model"
      ],
      "metadata": {
        "id": "0B1ZiIWYrid6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6.1 Underfitting (the problem)\n",
        "\n",
        "Our model is not performing as well as we would like.\n",
        "- Currently, **our training AND test losses are both relatively large**, as seen in the previous section.\n",
        "- This situation is called **underfitting**\n",
        "- Underfitting indicates the model is **too simple** for the pattern we’re trying to learn.\n",
        "\n",
        "Our straight-line model (boiling point vs. molecular weight) captures the **trend** but misses the **curvature** in the data!\n",
        "\n",
        "\n",
        "## Polynomial Regression (a solution)\n",
        "**Idea:** We can expand the inputs with new **features** so the model can *bend* to fit the data.  \n",
        "The simplest upgrade is to let the model use powers of molecular weight: $x, x^2, x^3, \\dots$ — this is **polynomial regression**.\n",
        "\n",
        "$$\n",
        "\\hat{y} = b \\;+\\; w_1 x \\;+\\; w_2 x^2 \\;+\\; \\cdots \\;+\\; w_d x^d\n",
        "$$\n",
        "\n",
        "- $x$ = molecular weight  \n",
        "- $d$ = polynomial degree (start with 2 or 3)  \n",
        "\n",
        "Even though it’s called “polynomial regression,” it’s still **linear regression** under the hood — just on **expanded features**:\n",
        "- Recall that the term *linear* refers to the linearly of the model, not to the *features* $[x, x^2, x^3...]$ being incorporated into the model.\n",
        "- We can see this mathematically: Each feature is multiplied by a weight $w_i$ and simply added to the model prediction, so the model is **linear** in the weights $w_i$\n",
        "\n",
        "### Why this helps\n",
        "Adding $x^2, x^3, \\ldots$ lets the fitted curve **bend**, which often matches physical property trends better than a strict straight line.\n",
        "- We can also use other functions of $x$, such as $log(x)$ which curve in different directions\n"
      ],
      "metadata": {
        "id": "OoFC2KUUuD4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's augment the interactive tool from above with the possibility to include additional features and analyze how this affects model performance."
      ],
      "metadata": {
        "id": "Es1gpG7RXow3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, _ = menglab.load_alkanes()\n",
        "model = menglab.interactive_polynomial_regression(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "8eX37mjkY9WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅  Exercise 14: Polynomial Regression Models [3 points total]\n",
        "\n",
        "**Tasks [1 point each]:** Train 5 polynomial regressions using the following features:\n",
        "\n",
        "- Model 1, features: $[x]$\n",
        "- Model 2, features: $[x, x^2]$\n",
        "- Model 3, features: $[x, x^2, x^3]$\n",
        "- Model 4, features: $[x, x^2, x^3, x^4]$\n",
        "- Model 5, features: $[x, x^2, x^3, x^4, \\log(x)]$\n",
        "\n",
        "\n",
        "\n",
        "1. What is the loss (RMSE) for each of the five models above? Enter the loss in the cell below. Execute the code cell to display a bar graph of model losses\n",
        "\n",
        "2. Which is the best model, (lowest testing loss)? Answer in the text box provided below.\n",
        "\n",
        "3. Does adding additional features always improve the model (i.e., decrease the test loss), in this example? Answer in the text box provided below."
      ],
      "metadata": {
        "id": "sH0ncckitICq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# Execute this cell to define the provided variables\n",
        "training_losses = [0,0,0,0,0]\n",
        "testing_losses = [0,0,0,0,0]\n",
        "model_names = [\"model 1\", \"model 2\", \"model 3\", \"model 4\", \"model 5\"]"
      ],
      "metadata": {
        "id": "GoayJtcRN7A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "training_losses[0] = NotImplemented # model 1, YOUR ANSWER HERE\n",
        "training_losses[1] = NotImplemented # model 2, YOUR ANSWER HERE\n",
        "training_losses[2] = NotImplemented # model 3, YOUR ANSWER HERE\n",
        "training_losses[3] = NotImplemented # model 4, YOUR ANSWER HERE\n",
        "training_losses[4] = NotImplemented # model 5, YOUR ANSWER HERE\n",
        "\n",
        "testing_losses[0] = NotImplemented # model 1, YOUR ANSWER HERE\n",
        "testing_losses[1] = NotImplemented # model 2, YOUR ANSWER HERE\n",
        "testing_losses[2] = NotImplemented # model 3, YOUR ANSWER HERE\n",
        "testing_losses[3] = NotImplemented # model 4, YOUR ANSWER HERE\n",
        "testing_losses[4] = NotImplemented # model 5, YOUR ANSWER HERE\n",
        "\n",
        "menglab.plot_model_losses(model_names, training_losses, testing_losses)"
      ],
      "metadata": {
        "id": "62T6DdkMtG1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Which is the best model, (lowest testing loss)?  YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "zC9KSyt57whU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Does adding additional features always improve the model (i.e., decrease the test loss), in this example? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "s-DBEJ9ZCzuu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 Expanding to Branched Alkanes\n",
        "\n",
        "Now that we’ve mastered **straight-chain (n-) alkanes**, let’s expand to **branched alkanes** and see how branching changes boiling points.\n",
        "\n",
        "### What “branching” means\n",
        "- Alkanes are hydrocarbons with only single C–C and C–H bonds (general formula $\\,\\mathrm{C_nH_{2n+2}}$).\n",
        "- **Isomers** share the same molecular formula (and thus the same molecular weight) but differ in **connectivity/shape**.\n",
        "- In branched isomers, some carbons have **more than two** carbon neighbors (secondary, tertiary, quaternary carbons), creating **side chains** off the main skeleton.\n",
        "\n",
        "### How branching affects boiling point\n",
        "At a **fixed carbon count $n$** (same molecular weight), **more branching $\\Rightarrow$ lower boiling point**.  \n",
        "Why? Branching makes molecules **more compact**, reducing **surface area** for intermolecular contact. With less contact area, **London dispersion forces** are weaker, so less energy (lower temperature) is required to boil.\n",
        "\n",
        "### Why add branched alkanes to our dataset?\n",
        "- They **decouple** “mass” from “shape.” Among isomers, molecular weight is **constant**, so any boiling-point differences are largely due to **shape/branching**.\n",
        "- This makes a great testbed for improving the model: beyond $x=\\text{MW}$, we now add **shape features**.\n",
        "\n",
        "\n",
        "## Use the data explorer to visualize the branched molecules in the new, expanded dataset"
      ],
      "metadata": {
        "id": "xt_kEZe2x9jX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, data_visualization = menglab.load_expanded_alkanes()\n",
        "menglab.data_explorer(data_visualization)"
      ],
      "metadata": {
        "id": "UQT06qO8zFgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's go back to our interactive tool with the branched molecules now included in the data."
      ],
      "metadata": {
        "id": "mQQL0tH2YOIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = menglab.interactive_polynomial_regression(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "9Htd7BuGzQyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 15: Training branched alkane model, Part 1. [3 points total]\n",
        "\n",
        "**Task: [1 point each]** Train 5 polynomial regressions on the branched alkanes dataset using the following features\n",
        "\n",
        "- Model 1, features: $[x]$\n",
        "- Model 2, features: $[x, x^2]$\n",
        "- Model 3, features: $[x, x^2, x^3]$\n",
        "- Model 4, features: $[x, x^2, x^3, x^4]$\n",
        "- Model 5, features: $[x, x^2, x^3, x^4, \\log(x)]$\n",
        "\n",
        "\n",
        "1. What is the loss (RMSE) for each of the five models above? Enter the loss in the cell below and execute the code cell to display a bar graph of model losses.\n",
        "\n",
        "2. Which is the best model (lowest testing loss)?\n",
        "\n",
        "3. Does adding additional features always improve the model (i.e., decrease the test loss), in this example? Answer in the text box provided below."
      ],
      "metadata": {
        "id": "cvn3-zlZzrGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# Execute this cell to define the provided variables\n",
        "training_losses_branched = [0,0,0,0,0]\n",
        "testing_losses_branched  = [0,0,0,0,0]\n",
        "model_names = [\"model 1\", \"model 2\", \"model 3\", \"model 4\", \"model 5\"]"
      ],
      "metadata": {
        "id": "fxLb_mdrOzfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Task 1\n",
        "training_losses_branched[0] = NotImplemented # model 1, YOUR ANSWER HERE\n",
        "training_losses_branched[1] = NotImplemented # model 2, YOUR ANSWER HERE\n",
        "training_losses_branched[2] = NotImplemented # model 3, YOUR ANSWER HERE\n",
        "training_losses_branched[3] = NotImplemented # model 4, YOUR ANSWER HERE\n",
        "training_losses_branched[4] = NotImplemented # model 5, YOUR ANSWER HERE\n",
        "\n",
        "testing_losses_branched[0] = NotImplemented # model 1, YOUR ANSWER HERE\n",
        "testing_losses_branched[1] = NotImplemented # model 2, YOUR ANSWER HERE\n",
        "testing_losses_branched[2] = NotImplemented # model 3, YOUR ANSWER HERE\n",
        "testing_losses_branched[3] = NotImplemented # model 4, YOUR ANSWER HERE\n",
        "testing_losses_branched[4] = NotImplemented # model 5, YOUR ANSWER HERE\n",
        "\n",
        "menglab.plot_model_losses(model_names, training_losses_branched, testing_losses_branched)"
      ],
      "metadata": {
        "id": "e6ieNB8kPTBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: What is the best model (lowest testing loss)? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "lvUrRMOy8vxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Does adding additional features always improve the model (i.e., decrease the test loss), in this example? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "V8Q0E0ldCTBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 Overfitting (the problem)\n",
        "\n",
        "As seen in the previous section, as we add more features, the training loss improves, but for large numbers of features the **testing loss can get worse!**\n",
        "- This is a classic example of **overfitting**.\n",
        "- Overfitting is considered when **training loss is small**, but **test loss is large**.  \n",
        "- In a sense, it is the opposite of **underfitting**\n",
        "- Overfitting indicates the model is **too complex** - it uses many features to fit the training data closely, but fails to generalize to new data\n",
        "\n",
        "**In our case**: The high-degree polynomials wiggle through every training point closely... but then make wilds, unrealistic predictions for new molecules in the test set.  \n",
        "\n",
        "---\n",
        "\n",
        "## Regularization (a solution)\n",
        "\n",
        "**Idea:** Prevent the model from fitting the noise by discouraging “overly large” weights.  \n",
        "- Large weights often mean the model is bending too aggressively to chase every training point.  \n",
        "- Regularization adds a **penalty term** to the loss function that keeps weights small.\n",
        "\n",
        "### Ridge (L2) regularization\n",
        "We add a penalty proportional to the sum of squared weights:\n",
        "\n",
        "$$\n",
        "\\text{RidgeLoss} = \\text{RMSE} \\;+\\; \\lambda \\sum_i w_i^2\n",
        "$$\n",
        "\n",
        "- $\\lambda$ = regularization strength (a hyperparameter you choose).  \n",
        "- Larger $\\lambda$ = stronger penalty = smoother, less wiggly curves.  \n",
        "\n",
        "### Lasso (L1) regularization\n",
        "Instead of squared weights, we penalize the absolute values:\n",
        "\n",
        "$$\n",
        "\\text{LassoLoss} = \\text{RMSE} \\;+\\; \\lambda \\sum_i |w_i|\n",
        "$$\n",
        "\n",
        "- Tends to drive some weights exactly to **zero**, effectively simplifying the model by dropping unnecessary features.  \n",
        "\n",
        "---\n",
        "\n",
        "### Why this helps\n",
        "- Regularization stops the model from “chasing noise.”  \n",
        "- The fitted curve is smoother, more stable, and more **generalizable** to unseen molecules.  \n",
        "- In practice, we often tune both the **degree of the polynomial** *and* the **regularization strength** together to find the right balance.  \n"
      ],
      "metadata": {
        "id": "d256174A44RT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go back to the interactive tool, and switch from vanilla linear regression to L2-regularized ridge regression."
      ],
      "metadata": {
        "id": "yXOK6JwKZEqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = menglab.interactive_polynomial_regression(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "2fMEW1i05zyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 16: Training a Regularized Model [3 points total]\n",
        "\n",
        "**Task: [1 point each]** Train 5 polynomial regressions with on the branched alkanes dataset using the following features. Use **ridge regularization** with $\\lambda = 0.1$\n",
        "\n",
        "- Model 1, features: $[x]$\n",
        "- Model 2, features: $[x, x^2]$\n",
        "- Model 3, features: $[x, x^2, x^3]$\n",
        "- Model 4, features: $[x, x^2, x^3, x^4]$\n",
        "- Model 5, features: $[x, x^2, x^3, x^4, \\log(x)]$\n",
        "\n",
        "**Tip:** In the \"model\" dropdown, select \"Ridge\" instead of \"Linear Regression.\"\n",
        "\n",
        "**Tip:** Don't forget to set $\\lambda$ to the appropriate value.\n",
        "\n",
        "\n",
        "\n",
        "1. What is the loss (RMSE) for each of the five models above? Enter the loss in the cell below. Execute the code cell to display a bar graph of model losses\n",
        "\n",
        "2. Which is the best model (lowest testing loss)?\n",
        "\n",
        "3. Did regularization improve the testing loss for the best model? Does regularization always improve the testing loss, in this case?"
      ],
      "metadata": {
        "id": "MN-2bjiO51Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# Execute this cell to define the provided variables\n",
        "training_losses_regularized = [0,0,0,0,0]\n",
        "testing_losses_regularized  = [0,0,0,0,0]\n",
        "model_names = [\"model 1\", \"model 2\", \"model 3\", \"model 4\", \"model 5\"]"
      ],
      "metadata": {
        "id": "S4P227ly_h8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "training_losses_regularized[0] = NotImplemented # model 1, YOUR ANSWER HERE\n",
        "training_losses_regularized[1] = NotImplemented # model 2, YOUR ANSWER HERE\n",
        "training_losses_regularized[2] = NotImplemented # model 3, YOUR ANSWER HERE\n",
        "training_losses_regularized[3] = NotImplemented # model 4, YOUR ANSWER HERE\n",
        "training_losses_regularized[4] = NotImplemented # model 5, YOUR ANSWER HERE\n",
        "\n",
        "testing_losses_regularized[0] = NotImplemented # model 1, YOUR ANSWER HERE\n",
        "testing_losses_regularized[1] = NotImplemented # model 2, YOUR ANSWER HERE\n",
        "testing_losses_regularized[2] = NotImplemented # model 3, YOUR ANSWER HERE\n",
        "testing_losses_regularized[3] = NotImplemented # model 4, YOUR ANSWER HERE\n",
        "testing_losses_regularized[4] = NotImplemented # model 5, YOUR ANSWER HERE\n",
        "\n",
        "menglab.plot_model_losses(model_names, training_losses_regularized, testing_losses_regularized)"
      ],
      "metadata": {
        "id": "QIoczONo_uGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: What is the best model (lowest testing loss)? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "v6SNrk1HDMLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Did regularization improve the testing loss for the best model? Does regularization always improve the testing loss, in this case? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "9M2jh5OeDMUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 Multiple Linear Regression\n",
        "\n",
        "Ok, now we have underfitting and overfitting covered.\n",
        "\n",
        "But we still can't capture the behavior of our data: they don't lie on a line (be it linear or nonlinear!)  \n",
        "- We need to go to **higher dimensions**.  \n",
        "\n",
        "### Adding more features\n",
        "So far, we’ve only used **molecular weight** (and its powers) to predict boiling point.  \n",
        "But molecules have many other structural characteristics. For example:  \n",
        "- **Branching index**: a measure of how branched the molecule is.  \n",
        "  - Highly branched molecules tend to have *lower* boiling points than straight-chain molecules of the same weight.  \n",
        "\n",
        "By adding branching index as another feature, we move from a **single-variable regression** to a **multiple linear regression**:\n",
        "\n",
        "$$\n",
        "\\hat{y} = b \\;+\\; w_1 \\cdot (\\text{molecular weight}) \\;+\\; w_2 \\cdot (\\text{branching index})\n",
        "$$\n",
        "\n",
        "Now, the model can capture differences between straight and branched molecules that a simple curve in one variable would miss.  \n",
        "\n",
        "---\n",
        "\n",
        "### Geometric view\n",
        "- With one feature, the model was a **line**.  \n",
        "- With polynomial features, it was a **curve** along one axis.  \n",
        "- With two features (weight and branching), the model becomes a **plane** in 3D space.  \n",
        "- With more features, it generalizes to a **hyperplane** in higher dimensions.  \n",
        "\n",
        "---\n",
        "\n",
        "### Why this helps\n",
        "- Many molecular properties depend on **multiple descriptors**: weight, branching, polarity, etc.  \n",
        "- By including more features, we allow the model to fit richer, more realistic patterns.  \n",
        "- Multiple linear regression is the foundation of most machine learning models: combine **many features**, keep the model **linear in the weights**.  \n",
        "\n",
        "---\n",
        "\n",
        "The code below shows a multiple linear regression model using two input features: log(MW) and branching index\n",
        "- the model can be visualized as a plane, which is trained to lie close to the data points.\n"
      ],
      "metadata": {
        "id": "ePL3JBMUxkyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datastet\n",
        "X_train, X_test, y_train, y_test = menglab.load_multilinear_alkanes()\n",
        "\n",
        "# Plot Multiple linear regression\n",
        "menglab.plot_multiple_linear_regression(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "uJ2CrrWJcPbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Interactive Multiple Linear Regression**\n",
        "\n",
        "The code cell below can be used to interactively train a mutliple linear regression model using any combination of three possible feature inputs:\n",
        "- Molecular Weight\n",
        "- Log(Molecular Weight)\n",
        "- branching index\n",
        "\n",
        "If **one** feature is used, the model is visualized as \"feature\" vs. boiling point\n",
        "- this is exactly how we have plotted the model previously\n",
        "\n",
        "If **multiple** features are used, the model is visualized as molecular weight v.s boiling point\n",
        "- this is as if we view the higher-dimensional model (a plane in 3D space) from the side.\n",
        "- since the model is higher dimensional, it no longer represents a single line through molecular weight vs. boiling point. Therefore, we visualize the model predictions as points, rather than a continuous line.\n"
      ],
      "metadata": {
        "id": "VJvYPi9VduTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.interactive_multiple_linear_regression(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "pKk6iz0ayTyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 16: Training a Regularized Model [2 points total]\n",
        "\n",
        "**Task: [1 point each]** Use the interactive tool to train 5 polynomial regressions with on the branched alkanes dataset using the following features.\n",
        "\n",
        "- Model 1, features: $[\\text{MW}]$\n",
        "- Model 2, features: $[\\text{MW}, \\text{log(MW)}]$\n",
        "- Model 3, features: $[\\text{MW}, \\text{branching_index}]$\n",
        "- Model 4, features: $[\\text{MW}, \\text{log(MW)}, \\text{branching_index}]$\n",
        "- Model 5, features: $[\\text{log(MW)}, \\text{branching_index}]$\n",
        "\n",
        "\n",
        "\n",
        "1. What is the loss (RMSE) for each of the five models above? Enter the loss in the cell below. Execute the code cell to display a bar graph of model losses\n",
        "\n",
        "2. Which is the best model (lowest testing loss)?"
      ],
      "metadata": {
        "id": "LbTOis5GES8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# Execute this cell to define the provided variables\n",
        "training_losses_multiple = [0,0,0,0,0]\n",
        "testing_losses_multiple  = [0,0,0,0,0]\n",
        "model_names = [\"model 1\", \"model 2\", \"model 3\", \"model 4\", \"model 5\"]"
      ],
      "metadata": {
        "id": "MPXIoMR4Fsn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "training_losses_multiple[0] = NotImplemented # model 1, YOUR ANSWER HERE\n",
        "training_losses_multiple[1] = NotImplemented # model 2, YOUR ANSWER HERE\n",
        "training_losses_multiple[2] = NotImplemented # model 3, YOUR ANSWER HERE\n",
        "training_losses_multiple[3] = NotImplemented # model 4, YOUR ANSWER HERE\n",
        "training_losses_multiple[4] = NotImplemented # model 5, YOUR ANSWER HERE\n",
        "\n",
        "testing_losses_multiple[0] = NotImplemented # model 1, YOUR ANSWER HERE\n",
        "testing_losses_multiple[1] = NotImplemented # model 2, YOUR ANSWER HERE\n",
        "testing_losses_multiple[2] = NotImplemented # model 3, YOUR ANSWER HERE\n",
        "testing_losses_multiple[3] = NotImplemented # model 4, YOUR ANSWER HERE\n",
        "testing_losses_multiple[4] = NotImplemented # model 5, YOUR ANSWER HERE\n",
        "\n",
        "menglab.plot_model_losses(model_names, training_losses_multiple, testing_losses_multiple)"
      ],
      "metadata": {
        "id": "s-9IrVScFy5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: What is the best model (lowest testing loss)? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "a0N-Yt7fGFjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5 Putting it all together: The full dataset\n",
        "\n",
        "Let's try putting it all together to model the full dataset, which includes a wide variety of molecules.\n",
        "\n",
        "A number of molecular features have been calculated for you, to use as inputs to a mutiple linear regression model.\n",
        "These features describe three main aspects of molecules:  \n",
        "\n",
        "- **Size** (e.g., Molecular Weight)  \n",
        "- **Shape** (e.g., branching index, flexibility, rotatable bonds)  \n",
        "- **Stickiness** (e.g., hydrogen bonding, polarity, oxygen content)  \n",
        "\n",
        "| **Feature**         | **Meaning** | **Why It Matters for Boiling Point Prediction** |\n",
        "|----------------------|-------------|------------------------------------------------|\n",
        "| **name**            | The common name of the molecule (e.g., ethanol, benzene). | Mostly just for identification; not used directly in math models. |\n",
        "| **smiles**          | A text code that describes the molecule’s structure. | From this, features like size and functional groups can be derived. |\n",
        "| **bp_k**            | Boiling point in **Kelvin** (K). | Target variable (what you’re trying to predict). Kelvin is the standard scientific temperature scale. |\n",
        "| **bp_c**            | Boiling point in **Celsius** (°C). | Same as above, but in °C. Having both units helps for comparison. |\n",
        "| **MW** (Molecular Weight) | How heavy the molecule is, measured as the sum of its atoms’ weights. | Heavier molecules often need more energy (higher temperature) to boil. |\n",
        "| **LogP**            | A measure of how soluble the molecule is in oil vs. water (hydrophobicity). | Molecules that dissolve in oil but not water often have weaker interactions |\n",
        "| **TPSA** (Topological Polar Surface Area) | A measure of how much of the molecule’s surface can form hydrogen bonds. | Molecules with large TPSA stick together more strongly, usually raising boiling points. |\n",
        "| **RotatableBonds**  | The number of single bonds that can freely rotate. | Flexibility of molecules affects how they pack together, influencing boiling. |\n",
        "| **HBD** (Hydrogen Bond Donors) | Number of “–OH” or “–NH” groups that can participate in a hydrogen bond. | More donors → stronger interactions → higher boiling point. |\n",
        "| **HBA** (Hydrogen Bond Acceptors) | Number of atoms (like oxygen or nitrogen) that can accept hydrogen bonds. | More acceptors → more interactions → higher boiling point. |\n",
        "| **OxygenCount**     | Total number of oxygen atoms in the molecule. | Oxygen atoms increase polarity and hydrogen bonding, often raising boiling point. |\n",
        "| **branching_index** | A measure of how branched the molecule is compared to a straight chain. | More branching usually lowers boiling point because molecules don’t pack together as tightly. |\n",
        "\n"
      ],
      "metadata": {
        "id": "qVrUTJVOIuZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "df = menglab.load_full_dataset()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "phc-i44cfc04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = menglab.load_full_train_test_split()\n",
        "menglab.interactive_full_linear_regression(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "vuFNY9Z-IEth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 17: Training a full Model [3 points total]\n",
        "\n",
        "**Task:** Use the interactive GUI above to train a regression model on the full dataset.  \n",
        "Your goal is to find a model with a **training RMSE lower than 30K** and a **testing RMSE lower than 34K**. Once you do, answer the following questions:\n",
        "\n",
        "1. **Feature selection**  \n",
        "   - Which features did you use?  \n",
        "   - If you included *all* features, simply write “all.”  \n",
        "\n",
        "2. **Feature sensitivity & underfitting**  \n",
        "   - Does the model’s performance change much depending on which features are included?  \n",
        "   - What does this tell you about the possibility of underfitting?  \n",
        "\n",
        "3. **Regularization choice**  \n",
        "   - Which model did you use: **Linear Regression, Ridge Regression, or Lasso Regression**?  \n",
        "   - Did this involve any form of regularization?  \n",
        "\n",
        "4. **Regularization sensitivity & overfitting**  \n",
        "   - Is the model sensitive to the type or strength of regularization (i.e. the choice of $\\lambda$)?  \n",
        "   - What does this tell you about the possibility of overfitting?  \n",
        "\n",
        "5. **Comparison to the literature**  \n",
        "   - How does your model perform compared to the results shown in *Figure 2, Figure 3, or Table 1 of Paper 2*?  \n",
        "   - What training loss and testing loss did they report (**Hint:** See the Discussion section)?  \n",
        "   - What do you think accounts for any differences in performance between your model and theirs?  \n"
      ],
      "metadata": {
        "id": "gRV0R06BJ4Av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Which features did you use? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "P49KVZ3qmGjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Does the model’s performance change much depending on which features are included? What does this tell you about the possibility of underfitting? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "40QbYThjmGpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3: Which model did you use: Linear Regression, Ridge Regression, or Lasso Regression? Did this involve any form of regularization? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "2YMrGRl_oySx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Is the model sensitive to the type or strength of regularization (i.e. the choice of  𝜆 )? What does this tell you about the possibility of overfitting? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "_Dh1_tnio5vM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5: How does your model perform compared to the results shown in Figure 2, Figure 3, or Table 1 of Paper 2?\n",
        "What training loss did they report (**Hint:** See the Discussion section)? What do you think accounts for any differences in performance between your model and theirs? YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "KZda-hXGmGyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations** You have completed Lab 2!"
      ],
      "metadata": {
        "id": "7om53AtGxJCe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uuE2hai-xMJD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}