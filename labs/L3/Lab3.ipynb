{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a71defe1",
      "metadata": {
        "id": "a71defe1"
      },
      "source": [
        "# **MENG 15100** Lab 3\n",
        "\n",
        "Welcome to the third lab of MENG 15100: Machine Learning and Artificial Intelligence for Molecular Discovery and Engineering\n",
        "\n",
        "In this lab, you’ll learn the basics of **Dimensinality Reduction** and **Clustering**, powerful tools in Machine Learning workflows.\n",
        "\n",
        "We’ll cover two major techniques:\n",
        "- **Principal Component Analysis** (for Dimensionality Reduction)\n",
        "- **k-means clustering**\n",
        "\n",
        "\n",
        "This lab starts from first principles—**no prior coding experience is required.** For every exercise, you will be provided with baseline Python code, and you will only be asked to make minor edits or adaptations (e.g., change a variable’s value, adjust the number of iterations, or modify a plot’s formatting)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lab Structure and Grading\n",
        "The lab is organized as follows:\n",
        "- **Topics** – Broad units (e.g., *1. Introduction to Dimensionality Reduction, 2. Principal Component Analysis*).\n",
        "- **Sections** – Subdivisions within each Topic (e.g., *1.1 Variance*).\n",
        "- **Problems** – Each Section ends with a Problem to be completed in the Jupyter Notebook. Problems are indicated with the ✅ character along with a listing of the number of points available that is indicative of the level of effort required for the solution. Problems may involve:\n",
        "  - Short-answer questions\n",
        "  - Modifications to existing Python code\n",
        "  - Note: Many Problems contain multiple tasks.\n",
        "\n",
        "- **Custom code** - many sections include custom code and interactive graphical user interfaces. The code implementing these functions is located in a custom library titled `menglab`"
      ],
      "metadata": {
        "id": "ZKbe6DCH9_Nb"
      },
      "id": "ZKbe6DCH9_Nb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **MENG 15100** Lab 3\n",
        "\n",
        "Lab 3 will begin with **Section 0**, which asks you to do some partial reading of a scientific paper. This section is designed to help you start practicing how to read and interpret research articles — a skill we’ll keep coming back to throughout the course.  \n",
        "\n",
        "Please also plan ahead: although the reading is only partial, it is not something you can rush through. To get proper understanding, you should set aside around **30 minutes** to carefully work through Section 0.  \n",
        "\n",
        "In summary:  \n",
        "- Section 0 = reading two papers (30 minutes).  \n",
        "- Sections 1–5 = coding in Python, best done during the lab session.  \n",
        "- Do the reading early so your lab time can be dedicated to coding practice.  "
      ],
      "metadata": {
        "id": "hEN3wG0fKtva"
      },
      "id": "hEN3wG0fKtva"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "**TIP**: An interactive table of contents is avaialable on the left sidebar.\n",
        "\n",
        "### 0. PCA in Scientific Literature\n",
        "&emsp; 0.1 Paper (available on canvas under Modules/Lab3)<br>\n",
        "\n",
        "### 1. Introduction to Dimensionality Reduction\n",
        "\n",
        "&emsp; 1.1 Projection: The simplest dimensionality reduction technique <br>\n",
        "\n",
        "&emsp; 1.2 Variance: Measuring the importance of different dimensions <br>\n",
        "\n",
        "&emsp; 1.3 Scree Plots and Explained Variance <br>\n",
        "\n",
        "&emsp; 1.4 Drawbacks of Simple Projection<br>\n",
        "\n",
        "### 2. Introduction to Principal Component Analysis\n",
        "\n",
        "&emsp; 2.1 Interactive PCA: Step 1, Rotation <br>\n",
        "\n",
        "&emsp; 2.2 Interactive PCA: Step 2, Projection <br>\n",
        "\n",
        "&emsp; 2.3 PCA vs. Clustering <br>\n",
        "\n",
        "\n",
        "### 3. PCA on Wines dataset\n",
        "\n",
        "&emsp; 3.1 Import Wine Chemistry Dataset (sklearn) <br>\n",
        "\n",
        "&emsp; 3.2 Standardizing the Data <br>\n",
        "\n",
        "&emsp; 3.3 PCA on Wines Dataset <br>\n",
        "\n",
        "&emsp; 3.4 Scree Plot and Cumulative Variance Explained <br>\n",
        "\n",
        "&emsp; 3.5 Visualize Data Projected into PCA Dimensions <br>\n",
        "\n",
        "### 4. Introduction to K-means clustering\n",
        "\n",
        "&emsp; 4.1 Step 1: Initialize Cluster Centers <br>\n",
        "\n",
        "&emsp; 4.2 Step 2: Assign Points to Nearest Cluster <br>\n",
        "\n",
        "&emsp; 4.3 Step 3: Update Cluster Centers <br>\n",
        "\n",
        "&emsp; 4.4 Step 4: Repeat! (Putting it all together) <br>\n",
        "\n",
        "&emsp; 4.5 Determining Best k with Silhouette Scores <br>\n",
        "\n",
        "### 5. K-means clustering on wines dataset.\n",
        "\n",
        "&emsp; 5.1 Interactive K-means Clustering Demo <br>\n",
        "\n",
        "&emsp; 5.2 WCSS and Elbow Plots <br>\n",
        "\n",
        "&emsp; 5.3 Silhouette Score Plot <br>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K1MMclGQ-BXq"
      },
      "id": "K1MMclGQ-BXq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports (Execute Once)\n",
        "Run the code cell below to install and import the modules necessary for this lab:"
      ],
      "metadata": {
        "id": "nI7smlLEEefz"
      },
      "id": "nI7smlLEEefz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ed7bd8",
      "metadata": {
        "id": "b2ed7bd8"
      },
      "outputs": [],
      "source": [
        "# Execute this cell once to import and install modules (may take several seconds)\n",
        "\n",
        "# menglab library\n",
        "%pip install -q --no-cache-dir --upgrade --force-reinstall \\\n",
        "  \"git+https://github.com/SorenKyhl/MENG15100.git@Lab3#subdirectory=labs/L3/menglab3\"\n",
        "\n",
        "import menglab3 as menglab\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "import io, requests\n",
        "\n",
        "# interactive gui settings\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"colab\"\n",
        "\n",
        "# Plotting settings\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams.update({\n",
        "    \"figure.figsize\": (6, 4),\n",
        "    \"figure.dpi\": 120,\n",
        "    \"axes.titlesize\": 13,\n",
        "    \"axes.labelsize\": 12,\n",
        "    \"axes.grid\": True,\n",
        "    \"grid.linestyle\": \"--\",\n",
        "    \"grid.alpha\": 0.35,\n",
        "    \"lines.linewidth\": 2.0,\n",
        "    \"lines.markersize\": 6,\n",
        "    \"font.size\": 12,\n",
        "    \"xtick.direction\": \"out\",\n",
        "    \"ytick.direction\": \"out\",\n",
        "    \"xtick.minor.visible\": True,\n",
        "    \"ytick.minor.visible\": True,\n",
        "    \"legend.frameon\": False,\n",
        "    \"savefig.bbox\": \"tight\",\n",
        "})\n",
        "%config InlineBackend.figure_format = 'retina'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 PCA in Scientific Literature\n",
        "\n",
        "Before we dive into applying Principal Component Analysis (PCA) ourselves, let’s explore how this method is used in real research within the molecular sciences. PCA is not just a mathematical tool—it’s a powerful way to uncover structure in complex chemical datasets such as spectra, fingerprints, or compositional measurements.\n",
        "\n",
        "In this exercise, we will look at a scientific study that used PCA to distinguish wines by their country of origin based on molecular characteristics. This type of analysis is an excellent example of how unsupervised learning can reveal meaningful chemical and geographical patterns hidden within high-dimensional data."
      ],
      "metadata": {
        "id": "rMMPEziV-Quw"
      },
      "id": "rMMPEziV-Quw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1 Paper: (available on canvas under Modules/Lab3)\n",
        "\n",
        "Hu, X.-Z., Liu, S.-Q., Li, X.-H., Wang, C.-X., Ni, X.-L., Liu, X., Wang, Y., Liu, Y., & Xu, C.-H. (2019). *Geographical origin traceability of Cabernet Sauvignon wines based on infrared fingerprint technology combined with chemometrics.* Scientific Reports, 9, 8256. https://doi.org/10.1038/s41598-019-44521-8\n"
      ],
      "metadata": {
        "id": "uZLhx6oxJ19Y"
      },
      "id": "uZLhx6oxJ19Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 1 [8 Points]: Skim Paper\n",
        "\n",
        "\n",
        "**Tasks:** Skim the specified sections of the paper and answer the following questions. Focus on identifying key ideas rather than technical details.\n",
        "\n",
        "Skim the **Abstract** and **Introduction** of the paper carefully, then answer the following questions.\n",
        "\n",
        "1. What **molecular property, experimental method, or dataset** was used as the input to PCA to classify wines by country of origin?\n",
        "\n",
        "2. What **three countries** were the Cabernet Sauvignon wines produced in?\n",
        "\n",
        "3. What **three algorithms** were used to classify the wines by country of origin?\n",
        "\n",
        "4. What is the main practical motivation for classifying wines by geographical region?\n",
        "\n",
        "Skim the **Figures** of the paper quickly, then answer the following questions:\n",
        "\n",
        "5. Which figure displays example input data used to cluster wines by origin?\n",
        "\n",
        "6. Which figure displays wines clustered by principal component analysis in 2D?\n",
        "\n",
        "7. Which figure displays wines clustered by principal component analysis in 3D?\n",
        "\n",
        "8. Although this paper did not use k-means clustering, imagine you were to cluster the wines in PCA space (as in the plots from questions 6 and 7). How many clusters do you think would best describe the data, and why?"
      ],
      "metadata": {
        "id": "BTWOwgue-_qG"
      },
      "id": "BTWOwgue-_qG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. What **molecular property, experimental method, or dataset** was used as the input to PCA to classify wines by country of origin?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "ucbIWVLpDHvt"
      },
      "id": "ucbIWVLpDHvt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. What **three countries** were the Cabernet Sauvignon wines produced in?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "_mhORZlLDJH8"
      },
      "id": "_mhORZlLDJH8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. What **three algorithms** were used to classify the wines by country of origin?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "ZWRj15UbDJ_r"
      },
      "id": "ZWRj15UbDJ_r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. What is the main practical motivation for classifying wines by geographical region?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "xSetKOfHDK8J"
      },
      "id": "xSetKOfHDK8J"
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Which figure displays example input data used to cluster wines by origin?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "d8F0Z_ONDL40"
      },
      "id": "d8F0Z_ONDL40"
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Which figure displays wines clustered by principal component analysis in 2D?\n",
        "\n",
        "YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "QwPCetuDDOdn"
      },
      "id": "QwPCetuDDOdn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Which figure displays wines clustered by principal component analysis in 3D?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "R5AMr-ODDQDb"
      },
      "id": "R5AMr-ODDQDb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Although this paper did not use k-means clustering, imagine you were to cluster the wines in PCA space (as in the plots from questions 6 and 7). How many clusters do you think would best describe the data, and why?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "Xop-lPVqDRZo"
      },
      "id": "Xop-lPVqDRZo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Introduction to Dimensionality Reduction\n",
        "\n",
        "In many areas of **machine learning** and **artificial intelligence**, we work with datasets that have **many dimensions** — sometimes dozens, hundreds, or even thousands.  \n",
        "Each “dimension” represents a different measured property or feature of our samples.\n",
        "\n",
        "For example, in the molecular datset from Lab 2, each compound was  described by a set of numerical features such as:  \n",
        "- Molecular weight  \n",
        "- Boiling point  \n",
        "- Surface area  \n",
        "- Branching index  \n",
        "- Number of hydrogen bond donors or acceptors  \n",
        "\n",
        "These features together form a **high-dimensional feature space** — one in which each molecule is represented as a single point defined by all its measured properties.\n",
        "\n",
        "---\n",
        "\n",
        "### **The Challenge of High Dimensionality**\n",
        "\n",
        "When we first look at a dataset like this, several natural questions arise:\n",
        "\n",
        "- Do the data points form **distinct groups or clusters**?  \n",
        "- What **features** or **properties** drive those clusters?  \n",
        "- How many **clusters or patterns** might exist in the data?  \n",
        "\n",
        "However, there’s a problem: we humans can **only visualize data directly in 1D, 2D or 3D**.  \n",
        "Once our dataset has more than three features, it becomes impossible to “see” its structure directly. Even though computational models can operate in hundreds of dimensions, our intuition cannot.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **What Is Dimensionality Reduction?**\n",
        "\n",
        "**Dimensionality reduction** refers to a family of techniques that **compress high-dimensional data** into a **lower-dimensional space** while preserving as much meaningful structure as possible.  \n",
        "\n",
        "These reduced dimensions are sometimes called **latent dimensions** or **principal components**, and they often capture the **essential variation** or **patterns** in the data.\n",
        "\n",
        "In other words, dimensionality reduction tries to answer:\n",
        "\n",
        "> *“Can we find a smaller set of new features that summarize what really matters in the data?”*\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use Dimensionality Reduction?**\n",
        "\n",
        "Dimensionality reduction helps us:\n",
        "- **Visualize** complex datasets in 2D or 3D plots  \n",
        "- **Reveal clusters or trends** that may not be obvious in the original feature space  \n",
        "- **Remove noise or redundancy** by combining correlated variables  \n",
        "- **Simplify models** and reduce computational cost  \n",
        "\n",
        "However, there is always a **trade-off**. By compressing data into fewer dimensions, we **lose some information**. The goal is to **minimize that loss** while still capturing the main structure of the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **In This Lab**\n",
        "\n",
        "In this lab, we’ll focus on two key techniques that often go hand-in-hand:\n",
        "\n",
        "1. **Principal Component Analysis (PCA)** — a method for finding new coordinate axes that capture the greatest variation in the data.  \n",
        "2. **k-Means Clustering** — an algorithm for grouping similar data points.\n",
        "\n",
        "We’ll begin by using PCA to reduce the dimensionality of a molecular dataset (the *Wine* dataset from `scikit-learn`), and then explore whether those reduced dimensions reveal meaningful groupings using k-means.\n"
      ],
      "metadata": {
        "id": "x9nfke75KA2q"
      },
      "id": "x9nfke75KA2q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1.1 Projection: the simplest dimensionality reduction technique\n",
        "\n",
        "The simplest way to perform **dimensionality reduction** is by using a **projection** — that is, selecting only a few of the available features (axes) to visualize.\n",
        "\n",
        "In this interactive example, we have a *fictitious dataset* with three numerical features: **x**, **y**, and **z**. These form a 3D coordinate system in which each data point has a specific position in space.\n",
        "\n",
        "However, let's imagine we can only visualize **two dimensions** at a time. This means that when we project a high-dimensional dataset into 2D, **important structure might be lost or hidden**.\n",
        "\n",
        "Use the 3D plot and the projection controls to explore how each view changes your perception of the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Use the Interactive Tool**\n",
        "\n",
        "The interactive figure below allows you to visualize this dataset and experiment with **different 2D projections** of the same 3D data.\n",
        "\n",
        "1. **Run the code cell below** to launch the interactive visualization.  \n",
        "   You’ll see two panels:\n",
        "   - **Left panel** — a 3D scatter plot of the dataset.  \n",
        "     - You can **rotate** the view by dragging your mouse.  \n",
        "     - You can **zoom** in/out using the scroll wheel or trackpad.  \n",
        "   - **Right panel** — a 2D projection (or “flattened” view) of the same data.\n",
        "\n",
        "2. **Use the “Project onto ...” dropdown menu** above the figure to select which pair of axes to visualize:\n",
        "   - **XY projection:** shows what the data looks like if we ignore the z-axis.  \n",
        "   - **XZ projection:** shows what the data looks like if we ignore the y-axis.  \n",
        "   - **YZ projection:** shows what the data looks like if we ignore the x-axis.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zt1YDPEOXapN"
      },
      "id": "zt1YDPEOXapN"
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.interactive_projection()"
      ],
      "metadata": {
        "id": "QZp7Qm3IKr4_"
      },
      "id": "QZp7Qm3IKr4_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 2 [5 Points]: Interactive Dimensionality Reduction (Projection)\n",
        "\n",
        "**Tasks** Use the interactive projection plots above to answer the following questions:\n",
        "\n",
        "1. **Visual interpretation**\n",
        "   - When viewing the **3D plot**, what do you notice about the structure of the dataset?  \n",
        "     - How many clusters are visible?  \n",
        "     - How are they arranged in space?\n",
        "\n",
        "2. **Projection onto the XY plane**\n",
        "   - Switch the projection to **XY**.\n",
        "   - How do the two clusters appear in this view?  \n",
        "     - Do they seem to overlap or remain distinct?  \n",
        "     - Why do you think that happens?\n",
        "\n",
        "3. **Projection onto the XZ plane**\n",
        "   - Switch the projection to **XZ**.\n",
        "   - Are the clusters easier or harder to distinguish here compared to the XY projection?  \n",
        "     - Which dimension (x, y, or z) seems to separate the data most clearly?\n",
        "\n",
        "4. **Projection onto the YZ plane**\n",
        "   - Switch to the **YZ** view.\n",
        "   - How is this view similar and different from the XZ projection?  \n",
        "     - Which projection (XZ vs. YZ) preserves the most information? (i.e. has the least loss of information).\n",
        "\n",
        "5. **Connecting to PCA**\n",
        "   - Principal Component Analysis (PCA) is a technique that finds the *most informative* directions in the dataset.\n",
        "     - Based off your prior analysis, list the three cardinal directions (x, y, z) in order of *most informative* to *least informative*\n"
      ],
      "metadata": {
        "id": "uya5VtcpV-aY"
      },
      "id": "uya5VtcpV-aY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. When viewing the **3D plot**, what do you notice about the structure of the dataset? How many clusters are visible? How are they arranged in space?\n",
        "\n",
        "     YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "zVcX-MHzOJTr"
      },
      "id": "zVcX-MHzOJTr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. **Projection onto the XY plane**. Switch the projection to **XY**. How do the two clusters appear in this view? Do they seem to overlap or remain distinct? Why do you think that happens?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "n9S9HRNQOe2L"
      },
      "id": "n9S9HRNQOe2L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Projection onto the XZ plane**. Switch the projection to **XZ**. Are the clusters easier or harder to distinguish here compared to the XY projection? Which dimension (x, y, or z) seems to separate the data most clearly?\n",
        "\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "v3VJD7QaO8Rw"
      },
      "id": "v3VJD7QaO8Rw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Projection onto the YZ plane**. Switch to the **YZ** view. How is this view similar and different from the XZ projection?  Which projection (XZ vs. YZ) preserves the most information (i.e. has the least loss of information)?\n",
        "\n",
        "YOUR SOLUTION HERE\n"
      ],
      "metadata": {
        "id": "A7vBUEhoPRAC"
      },
      "id": "A7vBUEhoPRAC"
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. **Connecting to PCA**. Principal Component Analysis (PCA) is a technique that finds the *most informative* directions in the dataset. Based off your prior analysis, list the three cardinal directions (x, y, z) in order of *most informative* to *least informative*\n",
        "\n",
        " YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "z45b3-csQ6x6"
      },
      "id": "z45b3-csQ6x6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Variance: Measuring the Importance of Different Dimensions\n",
        "\n",
        "From the previous section, we saw that not all dimensions (or features) in our dataset are equally informative.  \n",
        "Some dimensions — like **z** in our example — clearly help distinguish between clusters, while others — like **x** and **y** — do not.\n",
        "\n",
        "So how can we *quantitatively* identify which dimensions are most important?\n",
        "\n",
        "---\n",
        "\n",
        "### **Understanding Variance**\n",
        "\n",
        "**Variance** is a measure of how much the values in a dataset **spread out** from their average (mean).  \n",
        "It tells us *how much change* or *variation* exists along a particular feature or dimension.\n",
        "\n",
        "- A **high-variance** feature means that data points are widely dispersed along that axis.  \n",
        "  → This feature may capture important differences or structure in the data.\n",
        "- A **low-variance** feature means that data points are tightly grouped near the mean.  \n",
        "  → This feature carries less distinctive information.\n",
        "\n",
        "---\n",
        "\n",
        "### **How Variance is Calculated**\n",
        "\n",
        "To compute variance, we follow three simple steps:\n",
        "\n",
        "1. **Compute the mean** of the data along one dimension  \n",
        "   $$\n",
        "   \\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
        "   $$\n",
        "\n",
        "2. **Compute the squared difference** of each point from the mean  \n",
        "   $$\n",
        "   (x_i - \\bar{x})^2\n",
        "   $$\n",
        "\n",
        "3. **Average these squared differences**  \n",
        "   $$\n",
        "   \\text{Variance} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n",
        "   $$\n",
        "\n",
        "The larger the average squared difference, the more the data vary along that dimension.\n",
        "\n",
        "**TIP:** The units of variance are the **square of the original units**, which is why we often take its square root (the **standard deviation**) when we want to measure spread in the same units as the data.\n",
        "\n",
        "$$ \\text{Standard Deviation} = \\sqrt{\\text{Variance}} $$\n",
        "\n",
        "---\n",
        "\n",
        "### **Let’s Explore Variance Numerically**\n",
        "\n",
        "In the next step, we’ll calculate the variance of each feature in our dataset and compare them.\n",
        "\n",
        "We’ll use simple Python operations to:\n",
        "1. Compute the **mean** and **variance** for each dimension (`x`, `y`, and `z`).  \n",
        "2. Interpret which dimension carries the most information.  \n",
        "3. Visualize the differences in variance as a simple bar chart.\n",
        "\n",
        "This will give us a quantitative understanding of which features vary most — a key first step before we later connect this idea to **dimensionality reduction methods** like PCA.\n"
      ],
      "metadata": {
        "id": "lBItu8BMJo10"
      },
      "id": "lBItu8BMJo10"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅  Exercise 3 [8 Points]: Calculate Variance\n",
        "\n",
        "**Tasks:** In this exercise, you’ll quantify how much the dataset varies along each dimension (`x`, `y`, and `z`).  \n",
        "This will help you connect the visual patterns you observed earlier to a numerical measure of **spread** — the **variance**.\n",
        "\n",
        "The dataset from **Exercise 1** (the interactive projection) has been preloaded into the variable `dataset`.\n",
        "\n",
        "1. **Plot histograms of the dataset in the x, y, and z dimensions.**  \n",
        "\n",
        "   A **histogram** is a plot that shows how data are distributed along a single dimension.  \n",
        "   It divides the range of values into small intervals (called **bins**) and counts how many data points fall into each bin.  \n",
        "   The taller the bar, the more data points lie in that range.\n",
        "\n",
        "   In this way, a histogram shows the **density** or **frequency** of data along one axis.  \n",
        "   You can think of it as a way of **projecting** a higher-dimensional dataset into **1D** —  \n",
        "   for example, looking only at how the data spread along `x`, while ignoring `y` and `z`.\n",
        "\n",
        "   **Hint:**  You can access each dimension of the dataset like this:\n",
        "   `dataset['x']` or `dataset.x`, And so on for the x, y, and z directions\n",
        "\n",
        "2. Modify the code below to implement the `variance` function, which takes in data from one dimension, and calculates the variance of the data in that dimension. Calculate the following intermediate variables:\n",
        "\n",
        "  - `mean`: the mean of the input data\n",
        "  - `difference`: The difference of each datapoint from the mean\n",
        "  - `squared_difference`: the squared difference of each datapoint from the mean\n",
        "  - `variance`: the mean squared difference.\n",
        "\n",
        "    **Tip:** Use numpy functions such as `np.mean`, and  `np.square` to do calculations on entire arrays of data at once.\n",
        "\n",
        "3. Use the `variance` function you wrote in the previous task to calculate the variance of the data in the x, y, and z directions.\n",
        "\n",
        "4. Execute the code cell to visualize the magnitude of the variance in the x, y, and z directions. How does the magnitue of the variance in these directions correspond to your answer from Exercise 1, \"Based off your prior analysis, list the three cardinal directions (x, y, z) in order of *most informative* to *least informative*.\"\n"
      ],
      "metadata": {
        "id": "FG_u5S1DjACq"
      },
      "id": "FG_u5S1DjACq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# Run this cell to initialize the variables\n",
        "variances = [0, 0, 0]\n",
        "labels = ['x','y','z']\n",
        "\n",
        "dataset = menglab.generate_projection_data()\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "xVTHr9tka45R"
      },
      "id": "xVTHr9tka45R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "fig, ax = plt.subplots(1, 3, figsize=(12, 4), sharey=True)  # 1 row, 3 columns\n",
        "\n",
        "data_x_dimension = NotImplemented ### YOUR SOLUTION HERE\n",
        "data_y_dimension = NotImplemented ### YOUR SOLUTION HERE\n",
        "data_z_dimension = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "# Plot histograms for each column\n",
        "ax[0].hist(data_x_dimension, bins=25, color=\"steelblue\", alpha=0.7)\n",
        "ax[1].hist(data_y_dimension, bins=25, color=\"seagreen\", alpha=0.7)\n",
        "ax[2].hist(data_z_dimension, bins=25, color=\"indianred\", alpha=0.7)\n",
        "\n",
        "# Set axis limits for consistency\n",
        "for i in range(3):\n",
        "    ax[i].set_xlim(-4, 4)\n",
        "    ax[i].set_ylim(0, None)\n",
        "    ax[i].set_title(f\"Distribution of {['x','y','z'][i].upper()}\")\n",
        "    ax[i].set_xlabel(['x','y','z'][i])\n",
        "    ax[i].grid(alpha=0.3)\n",
        "    if i == 0:\n",
        "        ax[i].set_ylabel(\"Count\")\n",
        "\n",
        "fig.suptitle(\"Histograms along each dimension\", fontsize=14)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YPu8J2EYl_gN"
      },
      "id": "YPu8J2EYl_gN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "def variance(data):\n",
        "  \"\"\"\n",
        "  Compute the variance of data in one dimension.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : array-like\n",
        "      A one-dimensional list, NumPy array, or pandas Series containing numerical values.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  variance: float\n",
        "      The variance of the input data.\n",
        "  \"\"\"\n",
        "  data = np.array(data) # convert input to numpy array\n",
        "  mean = NotImplemented               ### YOUR SOLUTION HERE\n",
        "  difference = NotImplemented         ### YOUR SOLUTION HERE\n",
        "  squared_difference = NotImplemented ### YOUR SOLUTION HERE\n",
        "  variance = NotImplemented           ### YOUR SOLUTION HERE\n",
        "  return variance"
      ],
      "metadata": {
        "id": "ZNheZyRPmLNt"
      },
      "id": "ZNheZyRPmLNt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "variances[0] = NotImplemented ### YOUR SOLUTION HERE\n",
        "variances[1] = NotImplemented ### YOUR SOLUTION HERE\n",
        "variances[2] = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "# Check your answer - this should assert statement should pass!\n",
        "assert(np.allclose(variances, dataset[['x', 'y', 'z']].var(ddof=0)))\n",
        "print(\"✔️ Test passed! Your variance calculation looks correct.\")"
      ],
      "metadata": {
        "id": "sgZVfLain83T"
      },
      "id": "sgZVfLain83T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: plot for vizualization\n",
        "plt.bar(labels, variances)\n",
        "plt.xlabel(\"dimension\")\n",
        "plt.ylabel(\"variance\")"
      ],
      "metadata": {
        "id": "86WyhFkXnm1F"
      },
      "id": "86WyhFkXnm1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: Based off your prior analysis, list the three cardinal directions (x, y, z) in order of most informative to least informative. Provide a quantitative rationale for your answer.\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "XaPVjo7tnpk4"
      },
      "id": "XaPVjo7tnpk4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Scree Plots and Explained Variance\n",
        "\n",
        "So far, we’ve explored how different **dimensions** (x, y, and z) of our dataset can have different amounts of **variance** —  \n",
        "some directions capture a lot of variation (information), while others capture very little.\n",
        "\n",
        "We are almost ready to jump into **Principal Component Analysis (PCA)**, but it’s worth previewing a key idea from PCA that we can already apply to our simpler **projection-based** dimensionality reduction.\n",
        "\n",
        "---\n",
        "\n",
        "### **Explained Variance**\n",
        "\n",
        "If we think of projecting our dataset onto one dimension (say, the x-axis),  \n",
        "the **explained variance** tells us how much of the total spread in the dataset we still retain after that projection.\n",
        "\n",
        "Mathematically, the explained variance ratio for a dimension (or component) is:\n",
        "\n",
        "$$\n",
        "\\text{Explained Variance Ratio}_i = \\frac{\\text{Variance of Dimension } i}{\\text{Total Variance in all Dimensions (x + y + z)}}\n",
        "$$\n",
        "\n",
        "These ratios tell us **how informative** each dimension is relative to the others.\n",
        "\n",
        "---\n",
        "\n",
        "### **Scree Plots**\n",
        "\n",
        "A **Scree plot** is a simple line plot that shows how much variance is captured by each dimension (or later, each principal component in PCA).\n",
        "\n",
        "- The **x-axis** shows each dimension (in this case, x, y, or z), **sorted** in order from highest variance explained to lowest variance explained.\n",
        "- The **y-axis** shows the **explained variance ratio** — how much of the total variance that dimension captures.\n",
        "\n",
        "Typically, the first dimension explains the most variance, and the curve **drops off** as we move to less informative dimensions.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Cumulative Explained Variance**\n",
        "\n",
        "We can also plot the **cumulative explained variance**, which shows how much total variance is captured as we include more dimensions:\n",
        "\n",
        "$$\n",
        "\\text{Cumulative Explained Variance}_k = \\sum_{i=1}^{k} \\text{Explained Variance Ratio}_i\n",
        "$$\n",
        "\n",
        "For example:\n",
        "- In the case where the first two dimensions ($k = 2$) capture **90%** of the total variance, most of the structure in the dataset can be understood just by looking at those two directions.\n",
        "- In the case where the first 10 dimensions ($k$ = 10) capture **50%** of the data, we might not have a good ability to capture a low-dimensional projection of the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This Matters**\n",
        "\n",
        "Although we’re not performing PCA yet, this same idea —  \n",
        "ranking dimensions by how much variance they explain —  \n",
        "is at the core of **dimensionality reduction** techniques.\n",
        "\n",
        "When we *do* move on to PCA, we’ll always make:\n",
        "1. a **Scree plot** (to see how variance is distributed among components), and  \n",
        "2. a **Cumulative explained variance plot** (to decide how many components to keep).\n",
        "\n",
        "For now, we’ll create both of these plots using our simple **x–y–z projection** data.  \n",
        "This will help us practice the same logic that PCA uses to identify the *most informative directions* in a dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "ntw4O4P_n3XZ"
      },
      "id": "ntw4O4P_n3XZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 4 [8 Points]: Plot Explained Variance\n",
        "\n",
        "**Tasks** In the following tasks, we will calculate explained variance ratios and plot them in a Skree plot and Cumulative Explained Variance plot.\n",
        "\n",
        "We will use the `variances` list calculated in Exercise 3, so make sure that variable is defined and calculated correctly!\n",
        "\n",
        "1. First, sort the variances in each dimension so that the highest variance dimension is first. Modify the code to calculate `sorted_variances` from the `variances` list you calculated in Exercise 3.\n",
        "- you can use the python function `sorted(list)` to sort a list or numpy array\n",
        "- you can pass an optional argument `reverse` to reverse the ordering of the sorted list. `sorted(list, reverse=True)`. Should reverse be `True` or `False` in this case?\n",
        "\n",
        "2. Next, calculate the total variance across all dimensions, and then get the ratios for each dimension. Execute the code cell to display a Scree Plot.\n",
        "\n",
        "3. Finally, calculate the cumulative variance explained. Execute the code cell to display the cumulative variance explained plot.\n",
        "\n",
        "    **Tip:** Use the numpy function `np.cumsum(list)` to calculate the cumulative sum of a list of values\n",
        "\n",
        "4. Examine the Scree Plot and Cumulative Variance Explained Plot. If you project into only into the **one** best dimension, how much variance will be explained/retained? If you project into the **two** best dimensions, how much variance is explained/retained? Answer in the text box below.\n",
        "\n",
        "    **Hint:** Can you use a code cell to print the value of python variables to answer this question rather than just reading off the plot?"
      ],
      "metadata": {
        "id": "QvRIJY6Nqyok"
      },
      "id": "QvRIJY6Nqyok"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "sorted_variances = NotImplemented ### YOUR SOLUTION HERE\n",
        "print(\"The sorted variances are: \", sorted_variances)"
      ],
      "metadata": {
        "id": "RpclnDrbtgps"
      },
      "id": "RpclnDrbtgps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "total_variance = NotImplemented            ### YOUR SOLUTION HERE\n",
        "explained_variance_ratios = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "# Skree Plot\n",
        "labels = ['z', 'x', 'y']\n",
        "plt.bar(labels, explained_variance_ratios)\n",
        "plt.xlabel(\"Dimension\")\n",
        "plt.ylabel(\"Explained Variance Ratios\")\n",
        "plt.title(\"Scree Plot\")\n",
        "plt.figure()"
      ],
      "metadata": {
        "id": "iY2090X0uNVd"
      },
      "id": "iY2090X0uNVd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "cumulative_variance_ratios = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "cumulative_labels = ['z', 'z+x', 'z+x+y']\n",
        "plt.plot(cumulative_labels, cumulative_variance_ratios, '-o')\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Cumulative Explained Variance\")\n"
      ],
      "metadata": {
        "id": "jiI0jRTYvCE1"
      },
      "id": "jiI0jRTYvCE1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4: If you project into only into the **one** best dimension, how much variance will be explained/retained? If you project into the **two** best dimensions, how much variance is explained/retained?\n",
        "\n",
        "**Hint:** Can you use a code cell to print the value of python variables to answer this question rather than just reading off the plot?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "xEim22qRvrlE"
      },
      "id": "xEim22qRvrlE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Drawbacks of Simple Projection\n",
        "\n",
        "At first glance, it seems natural to look at complex data by simply projecting it — for example, plotting 3D points on a flat 2D screen using the x-y, x-z, or y-z planes. But what if that simple view hides something important?\n",
        "\n",
        "Execute the interactive projection code below again, but now with a different, more problematic dataset."
      ],
      "metadata": {
        "id": "8bGnVxBJoNpD"
      },
      "id": "8bGnVxBJoNpD"
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.interactive_projection(projection_problem=True)"
      ],
      "metadata": {
        "id": "3EO-bpC1ybTF"
      },
      "id": "3EO-bpC1ybTF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 5 [6 Points]: Problematic Projection\n",
        "\n",
        "**Tasks:**  Use the interactive plots above — including both the 3D scatter and its 2D projections — to answer the following quesions. Code is provided below to load the new dataset into the variable `problematic_dataset`\n",
        "\n",
        "1. **Overall structure:**  \n",
        "   - How many clusters do you observe in the full 3D view? Are they well separated in 3D?\n",
        "   - Along what direction, roughly speaking, are they separated?\n",
        "\n",
        "2. **Projection views:**  \n",
        "   - Examine each 2D projection (XY, XZ, YZ).  \n",
        "   - Do you still see two clusters in each projection, or do they appear merged?  \n",
        "   - What is it about the shape of the clusters in the dataset that gives this result?\n",
        "\n",
        "3. **Variances**.\n",
        "    - Calculate the variance in each cardinal direction, and execute the code to plot the variances.\n",
        "\n",
        "4. **Which dimension is best?** By this visualization, does any cardinal dimension have dramatically more variance explained than others? Is there a clear choice for the best cardinal dimension to project onto?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0jdjlxk12OkV"
      },
      "id": "0jdjlxk12OkV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# Run this cell to initialize the variables\n",
        "problematic_variances = [0, 0, 0]\n",
        "labels = ['x','y','z']\n",
        "\n",
        "problematic_dataset = menglab.generate_projection_data(projection_problem=True)\n",
        "problematic_dataset.head()"
      ],
      "metadata": {
        "id": "JzTcub9i3PUH"
      },
      "id": "JzTcub9i3PUH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Overall structure:**  - How many clusters do you observe in the full 3D view? Are they well separated in 3D? Along what direction, roughly speaking, are they separated?\n",
        "\n",
        "YOUR SOLUTON HERE"
      ],
      "metadata": {
        "id": "SxwUKmwZ5877"
      },
      "id": "SxwUKmwZ5877"
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Projection views:** Examine each 2D projection (XY, XZ, YZ). Do you still see two clusters in each projection, or do they appear merged? What is it about the shape of the clusters in the dataset that gives this result?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "G2tMEtK-6ZcZ"
      },
      "id": "G2tMEtK-6ZcZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "problematic_variances[0] = NotImplemented ### YOUR SOLUTION HERE\n",
        "problematic_variances[1] = NotImplemented ### YOUR SOLUTION HERE\n",
        "problematic_variances[2] = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "# Check your answer - this should assert statement should pass!\n",
        "assert(np.allclose(problematic_variances, problematic_dataset[['x', 'y', 'z']].var(ddof=0)))\n",
        "print(\"✔️ Test passed! Your variance calculation looks correct.\")\n",
        "\n",
        "# Task 4: plot for vizualization\n",
        "plt.bar(labels, problematic_variances)\n",
        "plt.xlabel(\"dimension\")\n",
        "plt.ylabel(\"variance\")"
      ],
      "metadata": {
        "id": "me3ErrBDlsRL"
      },
      "id": "me3ErrBDlsRL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Which dimension is best?** By this visualization, does any cardinal dimension have dramatically more variance explained than others? Is there a clear choice for the best cardinal dimension to project onto? Support your answer with quantitative evidence.\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "q2GAxJZjNwRn"
      },
      "id": "q2GAxJZjNwRn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Introduction to Principal Component Analysis\n",
        "\n",
        "From the last section, we saw that simple projection (e.g., looking only at x–y, x–z, or y–z) isn’t always reliable. A projection **throws away** whole directions, so if the important variation runs along some **diagonal combination** of axes, you may miss it.\n",
        "\n",
        "**Principal Component Analysis (PCA)** solves this by *rotating* the coordinate system to find directions that best capture the data’s variability—without restricting ourselves to the original cardinal axes.\n",
        "\n",
        "### Intuition\n",
        "\n",
        "- **Goal:** keep as much information (variance) as possible using as few dimensions as possible.  \n",
        "- **How:** find new, orthogonal directions (the **principal components**, PCs) along which the data vary the most.  \n",
        "- **Result:** PC1 is the single direction that maximizes spread; PC2 is the next best direction orthogonal to PC1, and so on.\n",
        "\n",
        "**Bottom line:** PCA doesn’t just *drop* dimensions; it **discovers better ones**—rotated axes that capture the most important variation—so you can reduce dimensionality **while preserving structure** that simple projection might hide.\n",
        "\n",
        "### The two \"Steps\" of PCA:\n",
        "\n",
        "1) **Rotation: find the best view.**  \n",
        "   Think of turning your camera around a point cloud to find the view where the data looks **most spread out** in one direction.\n",
        "   - That widest direction is **PC1** (Principal Component 1).\n",
        "   - The next widest direction, at a right angle to PC1, is **PC2**, and so on.\n",
        "   - In plain terms, we’re just **rotating the axes** to line up with how the data naturally varies.\n",
        "\n",
        "2) **Projection: keep the important parts.**  \n",
        "   After you’ve found these new directions (PC1, PC2, …), you **keep only the top few** (often 1–3) and **project out (collapse) the rest**.\n",
        "   - Keeping PC1 (and maybe PC2) gives a simpler picture that still captures **most of the spread**.\n",
        "   - Projecting out the small leftover directions removes detail/noise and reduces dimensionality.\n",
        "\n"
      ],
      "metadata": {
        "id": "RU67KQ141gqK"
      },
      "id": "RU67KQ141gqK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Interactive PCA: Step 1, Rotation\n",
        "\n",
        "Let's look at the first step, *Rotation*.\n",
        "\n",
        "Execute the interactive code cell below, where you’ll carry out PCA manually by **sweep the rotation angle θ.**\n",
        "\n",
        "**What to look for**\n",
        "- As you vary θ, the **variance along PC1** changes.  \n",
        "- The angle where the PC1 variance is **largest** is the PCA solution for **PC1**.  \n",
        "- **PC2** is always orthogonal to PC1; when PC1 captures more variance, PC2 captures less.  \n",
        "- The **total variance** of the dataset is unchanged by rotation.\n",
        "\n",
        "**How to use the interactive PCA demo:**\n",
        "1. Run the cell, then drag the **θ slider** to rotate the axes.  \n",
        "2. Observe when the PC1 variance is **highest**: this is your **principal direction**.  \n",
        "3. Relate that angle to the visible structure (e.g., cluster separation) in the scatter plot.\n"
      ],
      "metadata": {
        "id": "crx3_g-JO0QH"
      },
      "id": "crx3_g-JO0QH"
    },
    {
      "cell_type": "code",
      "source": [
        "PCA_dataset, _, _ = menglab.generate_PCA_data()\n",
        "menglab.interactive_PCA_rotation(PCA_dataset)"
      ],
      "metadata": {
        "id": "9F6J8TFOWavO"
      },
      "id": "9F6J8TFOWavO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 6 [8 Points] : Interactive PCA, Rotation\n",
        "\n",
        "**Tasks:** Use the interactive PCA code cell above to answer the following questions:\n",
        "\n",
        "1. Set $\\theta = 0$, $\\theta = 25$, $\\theta = 50$, and $\\theta = 75$.\n",
        "    - For each setting of $\\theta$, record the variance in PC1 and PC2 for all three settings of $\\theta$.  \n",
        "    - Calculate the total variance (the sum of PC1 and PC2 variances) for each setting of $\\theta$.\n",
        "    - Execute the code cell to plot the variances as a function of $\\theta$.\n",
        "\n",
        "2. From the analysis in Task 1, does the total variance change with $\\theta$ (within rounding error of approximately $\\pm$1 unit)?\n",
        "\n",
        "3. What choice of $\\theta$ ($\\theta = 0$, $\\theta = 25$, $\\theta = 50$, or $\\theta = 75$) results in PC1 containing the most variance in the data?\n",
        "\n",
        "4. For this optimal choice of $\\theta$, how much variance is explained by PC1 alone?"
      ],
      "metadata": {
        "id": "WMikzG1XWewB"
      },
      "id": "WMikzG1XWewB"
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# execute this code cell to define the provided variables\n",
        "thetas = [0, 25, 50, 75]\n",
        "PC1_var = np.array([0, 0, 0, 0])\n",
        "PC2_var = np.array([0, 0, 0, 0])"
      ],
      "metadata": {
        "id": "B1IsR6VRol-Y"
      },
      "id": "B1IsR6VRol-Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "PC1_var[0] = NotImplemented ### Your Solution here, theta = 0\n",
        "PC1_var[1] = NotImplemented ### Your Solution here, theta = 25\n",
        "PC1_var[2] = NotImplemented ### Your Solution here, theta = 50\n",
        "PC1_var[3] = NotImplemented ### Your Solution here, theta = 75\n",
        "\n",
        "PC2_var[0] = NotImplemented ### Your Solution here, theta = 0\n",
        "PC2_var[1] = NotImplemented ### Your Solution here, theta = 25\n",
        "PC2_var[2] = NotImplemented ### Your Solution here, theta = 50\n",
        "PC2_var[3] = NotImplemented ### Your Solution here, theta = 75\n",
        "\n",
        "total_variance = NotImplemented ### Your Solution here\n",
        "\n",
        "plt.plot(thetas, PC1_var, label = 'PC1 variance')\n",
        "plt.plot(thetas, PC2_var, label = 'PC2 variance')\n",
        "plt.plot(thetas, total_variance, label = 'total variance')\n",
        "plt.legend()\n",
        "plt.xlabel(\"theta\")\n",
        "plt.ylabel(\"variance\")"
      ],
      "metadata": {
        "id": "OT0J532woxc9"
      },
      "id": "OT0J532woxc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2. Does the total variance change with $\\theta$ (within rounding error of approximately $\\pm$1 unit)?\n",
        "\n",
        "YOUR SOLUTON HERE"
      ],
      "metadata": {
        "id": "7410gSabqjog"
      },
      "id": "7410gSabqjog"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3. What choice of $\\theta$ ($\\theta = 0$, $\\theta = 25$, $\\theta = 50$, and $\\theta = 75$) results in PC1 containing the most variance in the data?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "Xs0IChGImPYx"
      },
      "id": "Xs0IChGImPYx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4. For this optimal choice of $\\theta$, how much variance is explained by PC1 alone?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "fiVPu3FamNt6"
      },
      "id": "fiVPu3FamNt6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Interactive PCA: Projection\n",
        "\n",
        "Once we’ve found the **best rotation** (the principal components), we can **project** the data onto just a few of those directions and **drop the rest**. This keeps most of the important structure while making the dataset smaller and easier to work with.\n",
        "\n",
        "### What “projection” means (informally)\n",
        "- Think of shining a light so your 3D (or higher-D) cloud casts a shadow onto 1 or 2 chosen axes.\n",
        "- The axes we keep are the **top principal components** (PC1, PC2, …) because they capture the most **spread** (variance).\n",
        "- Everything **orthogonal** to those axes is discarded (set to zero in the reduced view).\n",
        "\n",
        "### Choosing how many components to keep (k)\n",
        "- **k = 1**: Collapse the data onto **PC1**. You get a single number per point (its **score** on PC1).  \n",
        "  Great for seeing the main trend or ordering along the dominant direction.\n",
        "- **k = 2**: Keep **PC1 and PC2**. You get a 2D view (scores on PC1–PC2), which often reveals clusters or elongated shapes.\n",
        "- **k > 2**: Keep the top **k** PCs for modeling while still reducing dimensionality (e.g., from 100 features down to 10–20).\n",
        "\n",
        "### What the interactive does here\n",
        "In this demo, we’ll **project onto just one dimension (PC1)**:\n",
        "- The **left plot** shows your data in the rotated frame (PC1 horizontal, PC2 vertical).\n",
        "- The **right plot** shows a **histogram of PC1 scores**—that’s the 1D projection.\n",
        "- As you adjust the rotation angle  $\\theta$, you’ll see the PC1 histogram widen or narrow.  \n",
        "  The **widest** histogram (largest variance) corresponds to the **true PC1**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hc3YWjovaTVP"
      },
      "id": "Hc3YWjovaTVP"
    },
    {
      "cell_type": "code",
      "source": [
        "PCA_dataset, _, _ = menglab.generate_PCA_data()\n",
        "menglab.interactive_PCA_projection(PCA_dataset)"
      ],
      "metadata": {
        "id": "d4txyVcCaYCi"
      },
      "id": "d4txyVcCaYCi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 7 [1 Points] : Interactive PCA, Projection\n",
        "\n",
        "**Tasks:** Use the interactive PCA code cell above to answer the following questions:\n",
        "\n",
        "Task 1. For this optimal choice of $\\theta$, are the two clusters in the dataset distinguishable along PC1?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "IM0-TCuUm-12"
      },
      "id": "IM0-TCuUm-12"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 PCA vs. Clustering\n",
        "\n",
        "So far we’ve used dimensionality reduction (simple projections and PCA) to *visualize* datasets that appear clustered.  \n",
        "It’s crucial to remember:\n",
        "\n",
        "> **PCA’s primary goal is not to find clusters.**  \n",
        "> PCA seeks directions of **maximum variance** (PC1, PC2, …) and—optionally—projects data onto a few of them to reduce dimension.\n",
        "\n",
        "### What this implies\n",
        "- PCA **optimizes variance**, not separation. A dataset can be clustered yet still look mixed in a PC1–PC2 plot.\n",
        "- We’ll use **k-means clustering** later to ask the *separate* question: “Is this data actually clustered?”\n",
        "\n",
        "### What you’ll do next\n",
        "Run the two cells below to:\n",
        "1. **Load a “paradoxical” PCA dataset**\n",
        "2. **Launch the interactive PCA demos** (Rotation and Projection -- these are the same analyses we ran above in Exercises 6/7) to see how:\n",
        "   - Rotating the axes changes the variance captured by “PC1,” and\n",
        "   - Projecting onto a small number of PCs can either **reveal** or **obscure** structure."
      ],
      "metadata": {
        "id": "i_IjRivarh0d"
      },
      "id": "i_IjRivarh0d"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data: paradoxical PCA dataset\n",
        "PCA_dataset_nocluster, y, u_true = menglab.generate_PCA_data(\n",
        "    n_per_cluster=300,\n",
        "    theta_data_deg=75.0,\n",
        "    sep=3.0,\n",
        "    sigma_parallel=0.5,\n",
        "    sigma_perp=3.0,\n",
        "    random_state=42\n",
        ")\n",
        "menglab.interactive_PCA_rotation(PCA_dataset_nocluster)"
      ],
      "metadata": {
        "id": "S_eEnsPEsAT9"
      },
      "id": "S_eEnsPEsAT9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.interactive_PCA_projection(PCA_dataset_nocluster)"
      ],
      "metadata": {
        "id": "Sl0-5v24r87R"
      },
      "id": "Sl0-5v24r87R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 8 [3 points]: Paradoxical PCA\n",
        "\n",
        "**Tasks:** Use the interactive demos above to explore a paradoxical PCA dataset.\n",
        "\n",
        "1. What value for $\\theta$ maximizes the variance in the PC1 direction? What is the variance in PC1 and variance explained ratio/percentage?\n",
        "\n",
        "2. Does the PCA optimal result distinguish the two clusters of data after projection into PC1?\n",
        "\n",
        "3. What about the structure of the data and the objectives of the PCA algorithm explains your observations?"
      ],
      "metadata": {
        "id": "_Yrb7_QIwoXq"
      },
      "id": "_Yrb7_QIwoXq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1. What value for $\\theta$ maximizes the variance in the PC1 direction? What is the variance in PC1 and variance explained ratio/percentage?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "KKZHzEnPxkcO"
      },
      "id": "KKZHzEnPxkcO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2. Does the PCA optimal result distinguish the two clusters of data after projection into PC1?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "kneA1KeHxrep"
      },
      "id": "kneA1KeHxrep"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3. What about the structure of the data and the objectives of the PCA algorithm explains your observations?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "_IhtkX4bxs4e"
      },
      "id": "_IhtkX4bxs4e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 PCA on Wines dataset\n",
        "\n",
        "Let’s apply PCA to a **real** dataset: the classic *Wine* dataset from **scikit-learn**.  \n",
        "We’ll loosely reproduce the analysis from the paper in Section 0.\n",
        "\n",
        "> **Note:** The paper we read did not release its original data. As a close stand-in, we’ll use scikit-learn’s Wine dataset, which includes physicochemical (molecular) measurements for Italian wines.\n",
        "\n",
        "Our goal is to use these features to uncover low-dimensional structure in the data—and, if present, cluster patterns (e.g., differences that might align with producers/cultivars or regions).  \n",
        "We’ll standardize the features and then run PCA to visualize and interpret the dominant sources of variation.\n",
        "\n",
        "### What’s in the dataset?\n",
        "- **wines** grown from the same region in Italy, belonging to **3 cultivars** (classes).\n",
        "- **chemical measurements** per wine (features), e.g. Alcohol, Malic acid, Ash, Alcalinity of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315, Proline.\n",
        "- Goal: **unsupervised dimensionality reduction** via PCA.\n",
        "\n",
        "### What is scikit-learn (sklearn)?\n",
        "\n",
        "**scikit-learn** is a popular, open-source **Python library for machine learning**. It gives you reliable, well-tested implementations of common algorithms—like **PCA**, **k-means**, **logistic regression**, **random forests**, and many more—**without** having to code the math and optimization details yourself.\n",
        "\n",
        "Why we use it here:\n",
        "- **Preprocessing:** tools like `StandardScaler` to standardize features before PCA.\n",
        "- **Dimensionality reduction:** `PCA` (and others) with a simple, uniform API.\n",
        "- **Model selection:** utilities like `train_test_split`, `Pipeline`, and cross-validation.\n",
        "- **Consistency:** almost everything follows the same pattern: `fit(...)`, then `transform(...)` or `predict(...)`.\n"
      ],
      "metadata": {
        "id": "Qu9zkkkY19RV"
      },
      "id": "Qu9zkkkY19RV"
    },
    {
      "cell_type": "markdown",
      "id": "a4ef2635",
      "metadata": {
        "id": "a4ef2635"
      },
      "source": [
        "\n",
        "## 3.1 Import Wine Chemistry Dataset (sklearn)\n",
        "\n",
        "Execute the code cell below to load the wines dataset and display the first 5 rows of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb0f6cc4",
      "metadata": {
        "id": "cb0f6cc4"
      },
      "outputs": [],
      "source": [
        "# Load wines dataset\n",
        "wine = load_wine()\n",
        "X_wine = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
        "y_wine = pd.Series(wine.target, name='class')  # 0,1,2 (wine cultivars)\n",
        "\n",
        "display(X_wine.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 9 [5 points]: Exploring the data\n",
        "\n",
        "**Tasks:** The wines dataset has been loaded into `X_wine` and `y_wine`\n",
        "  - `X_wine` is matrix of wines and their chemical features, each row corresponds to a wine, and each column corresponds to a chemical feature.\n",
        "  - `y_wine` is a list containing the cultivar type of each wine (represented as an integer).\n",
        "\n",
        "1. How many wines are there in this dataset? How many chemical features are provided for each wine?\n",
        "    - Use the `np.shape()` function to determine the dimensions of `X_wine` (returns a list of dimension sizes).\n",
        "    - Then, use list indexing to access the dimensions corresponding to the number of wines and number of features.\n",
        "\n",
        "2. How many unique cultivars are there?\n",
        "\n",
        "    - Use the `np.unique()` function to produce a list of unnique elements in a list.\n",
        "    - Use the `len()` function to determine the length of a list.\n",
        "    - Do we want to operate on `X_wine` or `y_wine` for this analysis?\n",
        "\n"
      ],
      "metadata": {
        "id": "PQJmOJ8V9tF7"
      },
      "id": "PQJmOJ8V9tF7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "dataset_shape = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "print(\"The shape of the dataset is: \", dataset_shape)\n",
        "\n",
        "n_wines = NotImplemented ### YOUR SOLUTION HERE\n",
        "n_features =  NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "print(\"The number of wines is: \", n_wines)\n",
        "print(\"The number of features are: \", n_features)"
      ],
      "metadata": {
        "id": "FWg76Qi3-6ek"
      },
      "id": "FWg76Qi3-6ek",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "unique_cultivars = NotImplemented ### YOUR SOLUTION HERE\n",
        "print(\"The unique cultivars are: \", unique_cultivars)\n",
        "\n",
        "n_unique_cultivars = NotImplemented ### YOUR SOLUTION HERE\n",
        "print(\"The number of unique cultivars is: \", n_unique_cultivars)\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "129rOxlxAI_4"
      },
      "id": "129rOxlxAI_4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 10 [5 Points]: Visualizing the data\n",
        "\n",
        "**Tasks:** Execute the two following code cells, for visualizing simple 2D projections of the wines dataset.\n",
        "\n",
        "  - In the first code cell, use the interactive dropdowns to visualize any simple 2D projection of the wines dataset using two of the feature dimensions.\n",
        "\n",
        "  - In the second code cell, all pairs of 2D feature projections are shown in a matrix.\n",
        "\n",
        "1. In visualizing the 2D projections, do any of them show distinct, highly separated clusters of data?\n",
        "\n",
        "2. Compare the values for different features:\n",
        "    - How does the magnitude and reange of `magnesium` values compare to the magnitude and range of `hue` values?\n",
        "    - If PCA is looking for directions of highest variance, how do you thing these different magnitudes and ranges will affect the PCA algorithm?\n",
        "\n",
        "3. What does the 'Standardize (z-score)' checkbox in the interactive code cell appear to do to the data? (In rough terms).\n"
      ],
      "metadata": {
        "id": "Ez1j8x67AdeC"
      },
      "id": "Ez1j8x67AdeC"
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.interactive_2d_projection(X_wine)"
      ],
      "metadata": {
        "id": "73j-u9hp77L_"
      },
      "id": "73j-u9hp77L_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plots of all possible 2D projections (may take a few seconds)\n",
        "from pandas.plotting import scatter_matrix\n",
        "scatter_matrix(X_wine, figsize=(14, 14), diagonal='hist', color='0.2', alpha=0.6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DkE4aAO_6iP9"
      },
      "id": "DkE4aAO_6iP9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1. In visualizing the 2D projections, do any of them show distinct, highly separated clusters of data?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "jmKWHmgtCg8o"
      },
      "id": "jmKWHmgtCg8o"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2. Compare the values for different features: How does the magnitude and range of `magnesium` values compare to the magnitude and range of `hue` values? If PCA is looking for directions of highest variance, how do you thing these different magnitudes and ranges will affect the PCA algorithm?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "WqewaYL6EYfj"
      },
      "id": "WqewaYL6EYfj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3. What does the 'Standardize (z-score)' checkbox in the interactive code cell appear to do to the data? (In rough terms).\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "o_-qy3TwCzfH"
      },
      "id": "o_-qy3TwCzfH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Standardizing the Data\n",
        "\n",
        "Standardizing is a critical step in the PCA algorithm.\n",
        "\n",
        "**Why standardize for PCA?**  \n",
        "1. PCA is doing **rotations** on the data. Therefore, it's important for the data to be centered on the origin (mean-zero) for these rotations to be interpretable.\n",
        "\n",
        "2. PCA looks for directions of **maximum variance**. If one feature has large units and another has small units, the large-scale feature can dominate the PCs. Standardizing puts features on a comparable footing so PCA reflects **structure**, not **units**.\n",
        "\n",
        "### What we do\n",
        "1. **Center (mean-zero) each feature**  \n",
        "   Subtract the column mean so the data cloud is centered at the origin.  \n",
        "2. **Scale to unit variance (z-score)**  \n",
        "   Divide by the column standard deviation so each feature has variance ≈ 1.\n",
        "   \n",
        "---\n",
        "**Z-score formula (per feature i):**\n",
        "$$\n",
        "z_{i} \\;=\\; \\frac{x_{i} - {\\mu}}{\\sigma}\n",
        "$$\n",
        "\n",
        "where $\\mu$ is the mean and $\\sigma$ is the standard deviation of feature $i$.\n",
        "\n"
      ],
      "metadata": {
        "id": "HAVRysQH7k4P"
      },
      "id": "HAVRysQH7k4P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 11 [7 Points]: Standardizing manually\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. Complete the `standardize` function, which takes in an input `x` corresponding to one feature (dimension) and returns a standardized version of that feature.\n",
        "    \n",
        "    - Use numpy functions like `np.mean` and `np.std` to calculate the mean and standard deviation of a list/np.array\n",
        "\n",
        "2. Use the `standardize` function to create a standardized version of the 'alcohol' feature.\n",
        "\n",
        "    - As a sanity check, calculate that mean and standard deviation are approximately 0 and 1, respectively.\n",
        "\n",
        "    - **Hint:** Individual features of the dataset can be accessed as follows: `X_wine['alcohol']` or `X_wine.alcohol`\n",
        "\n",
        "3. Complete the code cell to display a histogram of the alcohol feature before and after standardization."
      ],
      "metadata": {
        "id": "kKNEztj3F5Fg"
      },
      "id": "kKNEztj3F5Fg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "def standardize(x):\n",
        "  \"\"\"\n",
        "  Standardize a numeric array by z-scoring (mean 0, std 1).\n",
        "\n",
        "  This computes the global mean (μ) and standard deviation (σ) of `x` and returns\n",
        "  a standardized version of the input `x`\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  x : array_like\n",
        "      A 1D or n-D numeric array\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  z : numpy.ndarray\n",
        "      An array with the same shape as `x`, standardized to have mean ~0\n",
        "      and standard deviation ~1\n",
        "  \"\"\"\n",
        "  mu = NotImplemented ### YOUR SOLUTION HERE\n",
        "  sigma = NotImplemented ### YOUR SOLUTION HERE\n",
        "  z = NotImplemented ### YOUR SOLUTION HERE\n",
        "  return z"
      ],
      "metadata": {
        "id": "zLcM6V7StHBt"
      },
      "id": "zLcM6V7StHBt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "alcohol_feature = NotImplemented ### YOUR SOLUTION HERE\n",
        "alcohol_feature_standardized = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "print(\"the mean of the standardized feature is: \", np.mean(alcohol_feature_standardized))\n",
        "print(\"the standard deviation of the standardized feature is:\", np.std(alcohol_feature_standardized))"
      ],
      "metadata": {
        "id": "0m-tAhK2H2OZ"
      },
      "id": "0m-tAhK2H2OZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n",
        "\n",
        "axs[0].hist(NotImplemented) ### YOUR SOLUIION HERE\n",
        "axs[0].set_title('Alcohol')\n",
        "axs[0].set_xlabel('alcohol')\n",
        "axs[0].set_ylabel('count')\n",
        "axs[0].grid(True, alpha=0.3)\n",
        "\n",
        "axs[1].hist(NotImplemented) ### YOUR SOLUTION HERE\n",
        "axs[1].set_title('Alcohol (z-score)')\n",
        "axs[1].set_xlabel('alcohol (standardized)')\n",
        "axs[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "78TtXm_EJMef"
      },
      "id": "78TtXm_EJMef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 12 [2 Points]: Standardizing with `scikit-learn`\n",
        "\n",
        "In this exercise you’ll use **scikit-learn**’s `StandardScaler` to z-score features with a clean, consistent API.\n",
        "\n",
        "**Why sklearn?**  \n",
        "`scikit-learn` provides well-tested implementations of common ML steps (scaling, PCA, models) so you don’t have to hand-code the details. Most estimators follow the same pattern:\n",
        "\n",
        "- `fit(X)`: learn parameters from data (e.g., means/standard deviations).\n",
        "- `transform(X)`: apply the learned transformation to data.\n",
        "- `fit_transform(X)`: do both in one step.\n",
        "\n",
        "---\n",
        "\n",
        "#### Your tasks\n",
        "\n",
        "1) **construct** a `StandardScaler` object.  \n",
        "    - The sklearn library provides an object called `StandardScaler()`, which can be assigned to a variable (`scaler`).\n",
        "\n",
        "    **Hint:** Literally assign the variable `scaler` the value `StandardScaler()`\n",
        "\n",
        "2) **Fit** the scaler and **transform** the `X_wine` dataset.  \n",
        "    - Use the `scaler.fit_transform(data)` to return a standardized version of the data.\n",
        "    \n",
        "    - Execute the code cell and check your work by confirming that each standardized feature in `X_wine` has mean ~0 and std ~1.\n",
        "    \n",
        "    **N.B.** The `dataFrame.describe()` function displays the mean and standard deviation of every feature in a dataFrame"
      ],
      "metadata": {
        "id": "Xt5y0CIfIiPo"
      },
      "id": "Xt5y0CIfIiPo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "scaler = NotImplemented ### YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "aO8bkV1mKWkH"
      },
      "id": "aO8bkV1mKWkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "Xw_scaled = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "# recast to pandas dataFrame\n",
        "Xw_scaled = pd.DataFrame(Xw_scaled, columns=X_wine.columns)\n",
        "\n",
        "# visualize properties of each feature column\n",
        "Xw_scaled.describe()\n",
        "\n"
      ],
      "metadata": {
        "id": "etpNu7iFKq56"
      },
      "id": "etpNu7iFKq56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 PCA on wines dataset"
      ],
      "metadata": {
        "id": "X2QOt1qs7nos"
      },
      "id": "X2QOt1qs7nos"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 13 [2 Points]: PCA with scikit-learn\n",
        "\n",
        "Now that the Wine features are **standardized**, let’s run **Principal Component Analysis (PCA)**.\n",
        "\n",
        "**Protocol (same pattern as scaling):**\n",
        "1) **Create** a PCA object  \n",
        "2) **Fit & transform** the standardized data\n",
        "\n",
        "**Tasks**:\n",
        "\n",
        "1. Define a pca object using the `PCA()` function from sklearn.\n",
        "\n",
        "**HINT:** Like for the scaler task above, literally assign the variable `pca_object` the value `PCA()`.\n",
        "\n",
        "2. Call `.fit_transform(data)` on your pca object and set `data` to be the standardized data set you just constructed above. Look back to see what you called this variable.\n",
        "\n",
        "**That’s it! With just a few lines, scikit-learn handles all the heavy lifting of PCA!**"
      ],
      "metadata": {
        "id": "Sb44_UqBL3UX"
      },
      "id": "Sb44_UqBL3UX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "pca_object = NotImplemented ### YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "2ouVqMhENQKF"
      },
      "id": "2ouVqMhENQKF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "XW_pca = NotImplemented ### YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "XYbDFoaKNfmW"
      },
      "id": "XYbDFoaKNfmW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Scree plot and Cumulative Variance Explained"
      ],
      "metadata": {
        "id": "yaHXcieWNu9S"
      },
      "id": "yaHXcieWNu9S"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 14 [6 Points]: Plot PCA Variance Explained\n",
        "**Tasks:** Use sklearn functionality to plot the results of PCA\n",
        "\n",
        "1. Modify the code below to make a scree plot.\n",
        "    - The scree plot displays the explained variance ratios for each of the principal components of the data, in decreasing order.\n",
        "\n",
        "**HINT:** sklearn provides the explained variance ratios within the pca_object: `pca_object.explained_variance_ratio_`\n",
        "\n",
        "2. Modify the code below to calculate the cumulative explained variance, and plot the results.\n",
        "\n",
        "**HINT:** use `np.cumsum` to calculate the cumulative sum of a list/numpy array.\n",
        "\n",
        "3. How much cumulative variance is explained by the first 2 principal components? How much cumulative variance is explained by the first 3 principal components?\n",
        "\n",
        "**HINT:** Can you get Python to print out a variable that will help you answer this question more accurately?"
      ],
      "metadata": {
        "id": "xZvR53vRRF2O"
      },
      "id": "xZvR53vRRF2O"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "variance_explained_ratios = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "pc_labels = np.arange(1, len(var)+1)\n",
        "plt.figure()\n",
        "plt.plot(pc_labels, variance_explained_ratios, marker='o')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('scree plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3mQFTa0jPGo-"
      },
      "id": "3mQFTa0jPGo-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "cumulative_variance = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "x_labels = np.arange(1, len(cumulative_variance)+1)\n",
        "plt.figure()\n",
        "plt.plot(x_labels, cumulative_variance, marker='o')\n",
        "plt.xlabel('Number of PCs')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Wine: Cumulative Explained Variance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tfgr33fiPQdl"
      },
      "id": "tfgr33fiPQdl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3. How much cumulative variance is explained by the first 2 principal components? How much cumulative variance is explained by the first 3 principal components?\n",
        "\n",
        "**HINT:** Can you get Python to print out a variable that will help you answer this question more accurately?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "F1elsaIqQKQD"
      },
      "id": "F1elsaIqQKQD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Visualize data projected into PCA dimensions\n",
        "\n",
        "The output of the `.fit_transform(...)` function is the data projected into the Principal component directions. we can plot this data in 2D (using the first two principal components), or 3D (using the first 3 principal components)."
      ],
      "metadata": {
        "id": "_c1bbgIG7sI1"
      },
      "id": "_c1bbgIG7sI1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12a190e3",
      "metadata": {
        "id": "12a190e3"
      },
      "outputs": [],
      "source": [
        "# Visualize first two PCs, with true classes to build intuition\n",
        "menglab.pca_scatter(Xw_pca, y=y_wine, pcx=1, pcy=2, title='Wine: PC1 vs PC2 (colored by true class)')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.PCA_projection_3D(Xw_pca, y=y_wine, pcs=(1,2,3), evr=pca_object.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "u7YkQ6TvTdiZ"
      },
      "id": "u7YkQ6TvTdiZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Introduction to K-means Clustering\n",
        "\n",
        "In the Wine dataset, the **PC1–PC2** plot *looks* clustered — but that’s because we’ve been peeking at the cultivar **labels**.  \n",
        "What if we **couldn’t** see the labels? Could a computer still discover groups on its own — and even suggest **how many** there are?\n",
        "\n",
        "This is the purpose of **k-means** clustering.\n",
        "- **Unsupervised:** K-means ignores labels and looks only at the feature values.\n",
        "- **Goal:** Partition the data into **k** clusters where points are close to their cluster’s **centroid** (mean)\n",
        "\n",
        "## The core algorithm (in plain English)\n",
        "\n",
        "0. **Pick k** (how many groups you want).\n",
        "1. **Start with k centers** (often chosen automatically).\n",
        "2. **Assign** each point to its **nearest** center.\n",
        "3. **Update** each center to be the **average** of the points assigned to it.\n",
        "4. Repeat **assign → update** until things stop changing much.\n",
        "\n",
        "That’s it! The algorithm tries to put points into compact, well-separated clumps.\n",
        "\n",
        "---\n",
        "\n",
        "**Important:** **Step 0 is to Pick `k`.**  \n",
        "You must start with a **guess** for how many clusters (`k`) you think the data contains.\n",
        "\n",
        "Don’t worry—we’ll soon learn practical ways to **choose `k` automatically** (e.g., elbow plots and silhouette scores). For now, pick a reasonable starting value and iterate.\n",
        "\n",
        "---\n",
        "\n",
        "**Now, let's implement k-means clustering from scratch on a simple 1-D dataset.**\n",
        "\n",
        "Execute the code below to see a 1-D visualization of the toy dataset, which is constructed to have three distinct groupings of datapoints spread along the x-direction.\n"
      ],
      "metadata": {
        "id": "BLL3oWWMSER6"
      },
      "id": "BLL3oWWMSER6"
    },
    {
      "cell_type": "code",
      "source": [
        "data = menglab.generate_kmeans_data()\n",
        "\n",
        "plt.scatter(data, np.zeros_like(data), s=100, alpha=0.8, edgecolor='k', linewidths=0.2)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "#plt.scatter(centers, np.zeros_like(centers), c='r', s=500, alpha=0.5, edgecolor='k', linewidths=0.2)\n",
        "plt.title(\"Toy k-means dataset\")"
      ],
      "metadata": {
        "id": "iLHxATvWz0hH"
      },
      "id": "iLHxATvWz0hH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Step 1: Initialize cluster centers to 'k' random data points"
      ],
      "metadata": {
        "id": "3-kHIDMwzMjb"
      },
      "id": "3-kHIDMwzMjb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 15 [2 points]: Initialize clusters\n",
        "\n",
        "**Tasks:** The first step in **k-means** clustering is to initialize the cluster centers. The simplest way to do this is to select $k$ data points at random, and simply assign the cluster centers to lie on top of those data points.\n",
        "\n",
        "1. Complete the function `initialize_clusters(data, k)` that takes as input:\n",
        "    - `data`, a 1-D list of data points scattered in the x-direction\n",
        "    - `k` (int), the number of clusters\n",
        "    \n",
        "    And Returns:\n",
        "\n",
        "    - `centers`, a 1-D list of initialized cluster centers.\n",
        "\n",
        "    To accomplish this, assign the clusters to $k$ random points from the dataset.\n",
        "\n",
        "    - **Hint:** Random data points can be selected with the numpy function `np.random.choice(a, size)` where `a` is the source of data points and `size` is the number of randomly drawn points from the data source.\n",
        "\n",
        "2. Initialize the cluster locations using the above function with $k=3$. Execute the code cell to visualize the dataset and corresponding cluster centers."
      ],
      "metadata": {
        "id": "Vi067wPq02kq"
      },
      "id": "Vi067wPq02kq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1.\n",
        "def initialize_centers(data, k):\n",
        "  \"\"\"\n",
        "  Initialize k-means cluster centers by sampling from the data.\n",
        "\n",
        "  This function selects `k` values from `data` using `np.random.choice`\n",
        "  and returns them as the initial cluster centers.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : array-like\n",
        "      Input data. This implementation assumes **1D** data (e.g., shape (n,)).\n",
        "  k : int\n",
        "      Number of initial centers to select.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  centers : numpy.ndarray\n",
        "      Array of length `k` containing the chosen initial centers\n",
        "  \"\"\"\n",
        "  np.random.seed(0) # Set a seed for reproducibility\n",
        "  centers = NotImplemented ### YOUR SOLUTION HERE\n",
        "  return centers\n"
      ],
      "metadata": {
        "id": "L6h0HCnR4eAE"
      },
      "id": "L6h0HCnR4eAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "# Visualization - execute to visualize the initialization of cluster centers\n",
        "centers = NotImplemented ### YOUR SOLUTION HERE\n",
        "plt.scatter(data, np.zeros_like(data), s=100, alpha=0.8, edgecolor='k', linewidths=0.2, label='data')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.scatter(centers, np.zeros_like(centers), c='r', s=500, alpha=0.5, edgecolor='k', linewidths=0.2, label='Cluster centers')\n",
        "plt.title(\"Toy k-means dataset\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "BnkickZN7zG_"
      },
      "id": "BnkickZN7zG_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Step 2: Assign points to their nearest cluster center."
      ],
      "metadata": {
        "id": "hGda1uLDzUrm"
      },
      "id": "hGda1uLDzUrm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 16 [2 points]: Assign points to clusters.\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Complete the function `assign_labels(data, centers, labels)` that takes as arguments:\n",
        "    - `data`, a 1-D list of data points scattered in the x-direction\n",
        "    - `centers`, a list of 'k' cluster centers\n",
        "    - `labels`, a 1-D list of cluster label assignments, one for each point in `data`\n",
        "\n",
        "    ... And returns\n",
        "\n",
        "    - `labels`, an updated list of cluster label assignments\n",
        "\n",
        "    To assign points to clusters, for each point calculate:\n",
        "    - `distances`, a 1-D list of distances from the point to the cluster centers.\n",
        "    - `closest_cluster` the index corresponding to the smallest distance (the closest cluster)\n",
        "\n",
        "    **Hint:** use `np.argmin` to get the **index** corresponding to the smallest value in a list. Do *not* use `np.min` as this would give the value of the smallest distance itself, not the index of the cluster with the smallest distance!\n",
        "\n",
        "2. Assign points to clusters using the function you just developed in Task 1, with cluster centers defined from the previous exercise (Exercise 15). Execute the code cell to visualize the data points colored by cluster assignment."
      ],
      "metadata": {
        "id": "D_FG45ZN4p94"
      },
      "id": "D_FG45ZN4p94"
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1\n",
        "def assign_labels(data, centers, labels):\n",
        "  \"\"\"\n",
        "  Assign each data point to its nearest cluster center (1D k-means step).\n",
        "\n",
        "  For every point in `data`, compute the absolute distance to each value in\n",
        "  `centers` and write the index of the closest center into `labels`.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : array-like of shape (n_samples,)\n",
        "      1D data points to be clustered.\n",
        "  centers : array-like of shape (k,)\n",
        "      Current cluster center locations (in the same 1D space as `data`).\n",
        "  labels : array-like of shape (n_samples,)\n",
        "      Preallocated integer array; will be filled with cluster indices in 0..k-1.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  labels : numpy.ndarray\n",
        "      The same array passed in, filled with the nearest-center index per sample.\n",
        "  \"\"\"\n",
        "  for i, point in enumerate(data):\n",
        "    distances = np.abs(point - centers)\n",
        "    closest_cluster = NotImplemented ### YOUR SOLUTION HERE\n",
        "    labels[i] = closest_cluster\n",
        "  return labels"
      ],
      "metadata": {
        "id": "YPF-Md9s5neZ"
      },
      "id": "YPF-Md9s5neZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "# Visualization\n",
        "labels = np.zeros_like(data) # initialized to zeros\n",
        "labels = NotImplemented ### YOUR SOLUTION HERE\n",
        "plt.scatter(data, np.zeros_like(data), c = labels, s=100, alpha=0.8, edgecolor='k', linewidths=0.2, label='data')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.scatter(centers, np.zeros_like(centers), c='r', s=500, alpha=0.5, edgecolor='k', linewidths=0.2, label='Cluster centers')\n",
        "plt.title(\"Toy k-means dataset\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "H2EsFCco8NcK"
      },
      "id": "H2EsFCco8NcK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Step 3: Update Cluster Centers"
      ],
      "metadata": {
        "id": "u-ek1cjOzeha"
      },
      "id": "u-ek1cjOzeha"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 17 [2 points]: Update Cluster Centers\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Complete the function `update_cluster_centers(data, labels)` that takes in the following as arguments:\n",
        "    - `data`, a 1-D list of data points scattered in the x-direction\n",
        "    - `labels`, a 1-D list of cluster label assignments, one for each point in `data`\n",
        "    \n",
        "    ... And returns:\n",
        "    - `centers`, a list of 'k' updated cluster centers.\n",
        "\n",
        "    To update cluster centers, for each cluster: calculate the mean value of data points (\"members\") in the cluster\n",
        "\n",
        "**HINT:** Use the numpy function `np.mean` and take the mean value of all of the `cluster_members`\n",
        "  \n",
        "2. Update cluster centers using the function you just developed in Task 1, with labels defined according to the previous exercise (Exercise 16). Execute the code cell to visualize the new cluster centers. Note that the data points are not yet re-assigned to the closest cluster for these new cluster positions!\n",
        "\n",
        "Remember that the value of $k$ we are using is $k=3$.\n",
        "    "
      ],
      "metadata": {
        "id": "a1qAByD-8cBE"
      },
      "id": "a1qAByD-8cBE"
    },
    {
      "cell_type": "code",
      "source": [
        "# TASK 1\n",
        "def update_cluster_centers(data, labels, k):\n",
        "  \"\"\"\n",
        "  Update k-means cluster centers as the mean of their assigned points (1D).\n",
        "\n",
        "  This function recomputes each cluster center by taking the arithmetic mean of\n",
        "  all data points currently assigned to that cluster,\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : array-like of shape (n_samples,)\n",
        "      1D data values.\n",
        "  labels : array-like of shape (n_samples,)\n",
        "      Integer cluster assignments in the range 0..k-1 for each sample.\n",
        "  k : int\n",
        "      Number of clusters.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  centers : numpy.ndarray of shape (k,)\n",
        "      The updated center positions.\n",
        "  \"\"\"\n",
        "  for cluster in range(k):\n",
        "    cluster_members = data[labels == cluster] # select data points belonging to a cluster\n",
        "    centers[cluster] = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "  return centers"
      ],
      "metadata": {
        "id": "eQ8otwZc9q8o"
      },
      "id": "eQ8otwZc9q8o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "# Visualization\n",
        "# for this cell only: don't overwrite centers, store results in new_centers\n",
        "new_centers = NotImplemented ### YOUR SOLUTION HERE\n",
        "plt.scatter(data, np.zeros_like(data), c = labels, s=100, alpha=0.8, edgecolor='k', linewidths=0.2, label='data')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.scatter(centers, np.zeros_like(new_centers), c='r', s=500, alpha=0.5, edgecolor='k', linewidths=0.2, label='Cluster centers')\n",
        "plt.title(\"New cluster centers (data points not re-assigned yet)\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "pHJt8xl9-I-l"
      },
      "id": "pHJt8xl9-I-l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Step 4: Repeat! (Putting it all together)"
      ],
      "metadata": {
        "id": "Ky2Zzifezlmi"
      },
      "id": "Ky2Zzifezlmi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 18 [4 Points]: Implementing full K-means from scratch\n",
        "\n",
        "**Tasks:** Complete the `kmeans(data, k, iterations)` function to accomplish the kmeans algorithm. Use the functions you previously defined to complete the kmeans code.\n",
        "\n",
        "The code cell below outlines the scaffolding of the full kmeans algorithm. For this implementation, we will simply specify the number of iterations to take rather than detect when convergence has occured.\n",
        "\n",
        "1. Initially assign cluster centers using `initialize_clusters`\n",
        "\n",
        "For a specified number of iterations, on each iteration:\n",
        "\n",
        "2. Assign points to their closest cluster using `assign_labels`\n",
        "\n",
        "3. calculate new cluster centers using `update_cluster_centers`\n",
        "\n",
        "4. Use the completed `kmeans` function to run **k-means** clustering with k=4 for 5 iterations, and plot the results for each step. (set `plot=True`)"
      ],
      "metadata": {
        "id": "QwT27LZgzHGb"
      },
      "id": "QwT27LZgzHGb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tasks 1, 2, 3\n",
        "def kmeans(data, k, iterations=5, plot=False):\n",
        "  \"\"\"\n",
        "  Run a simple 1D k-means loop for a fixed number of iterations and plot progress.\n",
        "\n",
        "  This implementation clusters 1D points on the real line using Lloyd's algorithm:\n",
        "    (1) initialize k centers by sampling data points,\n",
        "    (2) assign each point to its nearest center (absolute distance),\n",
        "    (3) update each center to the mean of its assigned points,\n",
        "    (4) repeat for `iterations` steps.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : array-like of shape (n_samples,)\n",
        "      1D data values to cluster.\n",
        "  k : int\n",
        "      Number of clusters (centroids).\n",
        "  iterations : int, default=5\n",
        "      Number of Lloyd iterations to perform (fixed-iteration stopping).\n",
        "  plot : bool, default=False\n",
        "      optional argument, if True, plot results every iteration.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  centers : numpy.ndarray of shape (k,)\n",
        "      Final center positions after the last update.\n",
        "  labels : numpy.ndarray of shape (n_samples,)\n",
        "      Cluster index (0..k-1) assigned to each data point.\n",
        "\n",
        "  Dependencies\n",
        "  ------------\n",
        "  Uses three helper functions assumed to be defined previously:\n",
        "    - `initialize_centers(data, k)` -> centers\n",
        "    - `assign_labels(data, centers, labels)` -> labels\n",
        "    - `update_cluster_centers(data, labels, k)` -> centers\n",
        "  \"\"\"\n",
        "  labels = np.zeros_like(data)\n",
        "\n",
        "  # initially assign cluster centers to k random data points\n",
        "  centers = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "  for i in range(iterations):\n",
        "    # for each point, calculate distance to clusters, assign to closest one\n",
        "    labels = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "    # for each cluster, calculate the new center of mass\n",
        "    centers = NotImplemented ### YOUR SOLUTION HERE\n",
        "\n",
        "    # optionally, plot after each iteration\n",
        "    if plot:\n",
        "      plt.scatter(data, np.zeros_like(data), c = labels, s=100, alpha=0.8, edgecolor='k', linewidths=0.2, label='data')\n",
        "      plt.scatter(centers, np.zeros_like(centers), c='r', s=500, alpha=0.5, edgecolor='k', linewidths=0.2, label='Cluster centers')\n",
        "      plt.xlabel(\"x\")\n",
        "      plt.ylabel(\"y\")\n",
        "      plt.title(f\"K-means after iteration {i}\")\n",
        "      plt.legend()\n",
        "\n",
        "  return centers, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "8JMgrhGi-s3v"
      },
      "id": "8JMgrhGi-s3v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4\n",
        "centers, labels = NotImplemented ### YOUR SOLUTION HERE\n"
      ],
      "metadata": {
        "id": "J1nZsy4hDb3K"
      },
      "id": "J1nZsy4hDb3K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Determining the best k with Silhouette Scores\n",
        "\n",
        "**Motivation: choosing $k$ in k-means.**  \n",
        "K-means needs you to pick the number of clusters $k$ *before* fitting. How do we know a good $k$?  \n",
        "The **silhouette score** gives a **scale-free** measure of cluster quality you can compare **across different $k$**. Higher is better.\n",
        "\n",
        "---\n",
        "\n",
        "### Definition (per point)\n",
        "For a data point $i$:\n",
        "\n",
        "- $a(i)$: the **average distance** from $i$ to all **other points in its own cluster**.\n",
        "- $b(i)$: for every **other** cluster, compute the average distance from $i$ to that cluster; take the **smallest** of these averages (the “nearest other cluster”).\n",
        "\n",
        "The silhouette value for $i$ is\n",
        "$$\n",
        "s(i) \\;=\\; \\frac{b(i) - a(i)}{\\max\\{a(i),\\, b(i)\\}} \\;\\in\\; [-1,\\,1].\n",
        "$$\n",
        "\n",
        "**Interpretation:**\n",
        "- $s(i) \\approx 1$: well matched to its cluster and far from others.  \n",
        "- $s(i) \\approx 0$: on a boundary.  \n",
        "- $s(i) < 0$: may be misassigned.\n",
        "\n",
        "The **overall silhouette score** is the **mean** of $s(i)$ over all points.\n",
        "\n",
        "---\n",
        "\n",
        "### Using silhouette to pick $k$\n",
        "\n",
        "1. For each $k \\in \\{2,3,\\dots,K\\}$:\n",
        "   - Fit k-means (on **standardized** features).\n",
        "   - Compute the **mean silhouette** $\\bar{s}_k$.\n",
        "2. Plot $\\bar{s}_k$ vs. $k$ and choose the $k$ with a **higher** score (balance with simplicity and domain sense).\n",
        "\n"
      ],
      "metadata": {
        "id": "yqJatYJPDuv7"
      },
      "id": "yqJatYJPDuv7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 19 [4 Points]: Implementing silhouette score\n",
        "\n",
        "**Tasks:** Use `scikit-learn` to execute k-means clustering and get a silhouette score.\n",
        "\n",
        "1. Complete the `get_silhouette_score(data, k)` function that takes in as arguments:\n",
        "     - `data`, a 1-D list of data points scattered in the x-direction\n",
        "     - `k`, the number of clusters\n",
        "\n",
        "     ... And returns\n",
        "     - `score`, the silhouette score for the k-means clustering.\n",
        "\n",
        "     **Tips:**\n",
        "\n",
        "     - First, we use sklearn to define a k-means estimator: `KMeans(n_clusters)`\n",
        "\n",
        "     - Then, we use the k-means estimator to fit and predict labels using:\n",
        "     `km.fit_predict(data)`. (Note that we're using *fit_predict* and not *fit_transform* here because the labels are separate predictions, not transformations of the underlying dataset.)\n",
        "     \n",
        "     - Finally we need to use `silhouette_score(data, labels)` from sklearn to calculate the score from the data and labels.\n",
        "\n",
        "2. Complete the `silhouette_sweep(data, k_min, k_max, ...)`function that executes k-means clustering on the data and calculates a silhouette score for values of k ranging from `k_min` to `k_max`. Use the `get_silhouette_score` function defined in Task 1.\n",
        "\n",
        "3. Execute the `silhouette_sweep` on the 1-D toy dataset, for values of k from 2 to 10.\n",
        "\n",
        "4. Examine the plot produced by silhouette sweep. Which choice for k (how many clusters) best separates the data? How does this line up with your intuition, given your obersvation of the underlying dataset?\n"
      ],
      "metadata": {
        "id": "nH2KRFahHSJT"
      },
      "id": "nH2KRFahHSJT"
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "def get_silhouette_score(data, k):\n",
        "  \"\"\"\n",
        "  Compute the mean silhouette score for a K-Means clustering.\n",
        "\n",
        "  This convenience function fits `KMeans(n_clusters=k)` on `data`,\n",
        "  obtains cluster labels, and returns the mean silhouette score\n",
        "  (range: [-1, 1], higher is better).\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : array-like of shape (n_samples, n_features)\n",
        "      Feature matrix. For 1D data, pass `x.reshape(-1, 1)`.\n",
        "  k : int\n",
        "      Number of clusters (must be >= 2).\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  score : float\n",
        "      Mean silhouette score for the fitted clustering.\n",
        "  \"\"\"\n",
        "  km = KMeans(n_clusters=k)\n",
        "  labels = km.fit_predict(data)\n",
        "  score = NotImplemented ### YOUR SOLUTION HERE\n",
        "  return score"
      ],
      "metadata": {
        "id": "p6TZiV_GIhcd"
      },
      "id": "p6TZiV_GIhcd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2\n",
        "def silhouette_sweep(data, k_min, k_max, plot=True, random_state=0):\n",
        "  \"\"\"\n",
        "  Compute (and optionally plot) the mean silhouette score across a range of k.\n",
        "\n",
        "  For each k in [k_min, k_max], this function fits a K-Means model on `data`,\n",
        "  obtains cluster labels, and computes the mean silhouette score\n",
        "  (range: [-1, 1], higher is better). If `plot=True`, it produces a line plot\n",
        "  of silhouette score vs. k.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  data : array-like of shape (n_samples,) or (n_samples, n_features)\n",
        "      Input data. If 1D, pass a 1D array; implementations typically reshape\n",
        "      internally to (n_samples, 1) for compatibility with scikit-learn.\n",
        "  k_min : int\n",
        "      Smallest number of clusters to evaluate (must be >= 2).\n",
        "  k_max : int\n",
        "      Largest number of clusters to evaluate. Implementations may clip this\n",
        "      to at most n_samples - 1.\n",
        "  plot : bool, default=True\n",
        "      If True, plot silhouette score vs. k.\n",
        "  random_state : int, default=0\n",
        "      Seed used for K-Means initialization to make results reproducible.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  ks : list of int\n",
        "      The evaluated k values.\n",
        "  scores : list of float\n",
        "      The mean silhouette score for each k (NaN where undefined).\n",
        "  \"\"\"\n",
        "  if len(data.shape) == 1:\n",
        "    data = np.asarray(data).reshape(-1, 1) # reshape for compatibility with sklearn\n",
        "  ks = range(k_min, k_max + 1)\n",
        "  scores = []\n",
        "\n",
        "  # calculate silhouette score for each value of k\n",
        "  for k in ks:\n",
        "      score = NotImplemented ### YOUR SOLUTION HERE\n",
        "      scores.append(score)\n",
        "\n",
        "  if plot:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(ks, scores, \"-o\")\n",
        "    plt.xlabel(\"k\")\n",
        "    plt.ylabel(\"Silhouette score\")\n",
        "    plt.title(\"Silhouette scores\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "  return ks, scores"
      ],
      "metadata": {
        "id": "OsFz8jklLQuk"
      },
      "id": "OsFz8jklLQuk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3\n",
        "ks, scores = NotImplemented ### YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "2zwj-xcELu7D"
      },
      "id": "2zwj-xcELu7D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4. Examine the plot produced by silhouette sweep. Which choice for k (how many clusters) best separates the data? How does this line up with your intuition, given your obersvation of the underlying dataset?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "L5TrmXAQMcP6"
      },
      "id": "L5TrmXAQMcP6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 K-means clustering on PCA of wines dataset.\n",
        "\n",
        "Now let's return to the wines dataset. Execute the code below to visualize the wines dataset projected into the first two Principal components"
      ],
      "metadata": {
        "id": "lBCAjQAKMq93"
      },
      "id": "lBCAjQAKMq93"
    },
    {
      "cell_type": "code",
      "source": [
        "menglab.pca_scatter(Xw_pca, y=y_wine, pcx=1, pcy=2, title='Wine: PC1 vs PC2 (colored by true class)')"
      ],
      "metadata": {
        "id": "QJ62J_2EcnmH"
      },
      "id": "QJ62J_2EcnmH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1  Interactive K-Means Clustering Demo\n",
        "\n",
        "This interactive demo helps you **visualize how K-Means clustering works step by step**.  \n",
        "You can adjust the **number of clusters (`k`)** and the **random seed** to see how the algorithm behaves differently with each setup.\n",
        "\n",
        "At each iteration, the algorithm:\n",
        "1. Assigns each point to its **nearest cluster centroid**.\n",
        "2. Recalculates the **centroid positions** based on those assignments.\n",
        "3. Updates the visualization to show how the clusters evolve.\n"
      ],
      "metadata": {
        "id": "TWZzs3_xcsA-"
      },
      "id": "TWZzs3_xcsA-"
    },
    {
      "cell_type": "code",
      "source": [
        "k = 2\n",
        "wine_2PCs = Xw_pca[:, :2]\n",
        "menglab.interactive_k_means(wine_2PCs, y_wine)"
      ],
      "metadata": {
        "id": "L2DeTKEdUgy1"
      },
      "id": "L2DeTKEdUgy1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 WCSS and Elbow Plots\n",
        "The interactive demo above calculates the **Within Cluster Sum of Squares** (**WCSS**).\n",
        "\n",
        "WCSS measures **how tightly grouped** the points are inside each cluster.  \n",
        "- If points are **very close** to their cluster center, the WCSS value will be **small**.  \n",
        "- If points are **spread out**, the WCSS value will be **large**.\n",
        "\n",
        "So, a **lower WCSS** means the clusters are better — they’re more compact and consistent.\n",
        "\n",
        "### Why Do We Care About WCSS?\n",
        "\n",
        "As we increase the number of clusters (for example, from 2 to 3 to 4, and so on), the WCSS usually **goes down**, because each cluster gets smaller and fits the data better.  \n",
        "But if we keep adding clusters, we eventually stop getting much improvement.  \n",
        "\n",
        "To find the **best number of clusters**, we often use the **elbow method**:\n",
        "- Plot the WCSS value for different numbers of clusters.\n",
        "- Look for the point where the line starts to **bend** like an elbow.\n",
        "- That “elbow” is often a good choice for the number of clusters.\n",
        "- This method can be used alongside the **silhouette method** for determining k.\n"
      ],
      "metadata": {
        "id": "FuV-9bbxVyTV"
      },
      "id": "FuV-9bbxVyTV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 20 [2 Points]: Elbow Plot\n",
        "\n",
        "**Tasks** Use the interactive k-means demo above to answer the following questions.\n",
        "\n",
        "1. What is the within-cluster sum of squares (**WCSS**) for $k=2, 3, 4,$ and $5$ after k-means clustering has converged?\n",
        "\n",
        "**N.B.** Leave the random seed set to 0!\n",
        "\n",
        "2. Execute the code cell below to populate an \"Elbow plot\" (i.e. WCCS vs k). Is there a visible elbow in the plot where the WCCS stops dropping dramatically? What value of 'k' occurs at the elbow? How does this correspond to the number of clusters we might expect from the dataset?"
      ],
      "metadata": {
        "id": "21BxWRKyYfgI"
      },
      "id": "21BxWRKyYfgI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (provided)\n",
        "# Execute once to define these variables\n",
        "ks = [2, 3, 4, 5]\n",
        "WCSS = [0, 0, 0, 0]\n"
      ],
      "metadata": {
        "id": "6XJcMbvuXlRT"
      },
      "id": "6XJcMbvuXlRT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "WCSS[0] = NotImplemented ### YOUR SOLUTION HERE, k=2\n",
        "WCSS[1] = NotImplemented ### YOUR SOLUTION HERE, k=3\n",
        "WCSS[2] = NotImplemented ### YOUR SOLUTION HERE, k=4\n",
        "WCSS[3] = NotImplemented ### YOUR SOLUTION HERE, k=5"
      ],
      "metadata": {
        "id": "DLCCdiS0U37K"
      },
      "id": "DLCCdiS0U37K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Visualization\n",
        "plt.plot(ks, WCSS, '-o')\n",
        "plt.xlabel(\"k (number of clusters)\")\n",
        "plt.ylabel(\"Within Cluster Sum of Squares (WCCS)\")\n",
        "plt.title(\"Elbow Plot\")"
      ],
      "metadata": {
        "id": "XsTewODYX5ND"
      },
      "id": "XsTewODYX5ND",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Execute the code cell below to populate an \"Elbow plot\" (i.e. WCCS vs k). Is there a visible elbow in the plot where the WCCS stops dropping dramatically? What value of 'k' occurs at the elbow? How does this correspond to the number of clusters we might expect from the dataset?\n",
        "\n",
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "wpvzmUx6YWIf"
      },
      "id": "wpvzmUx6YWIf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Silhouette Score Plot\n",
        "\n",
        "Let's see if the Silhouette plot agrees with the Elbow plot analysis for determining the optimal number of clusters"
      ],
      "metadata": {
        "id": "sZMJhmkN7ziY"
      },
      "id": "sZMJhmkN7ziY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Exercise 21 [2 points]: Silhouette Score\n",
        "\n",
        "**Tasks**\n",
        "\n",
        "1. Use the `silhouette_sweep` function you implemented in Exercise 19 to plot silhouette scores for k values between 2 and 8 for the `wine_2PCS` dataset defined below.\n",
        "\n",
        "2. What is the optimal value of k (number of clusters) according to the Silhouette Score analysis? Does it agree with the Elbow plot?"
      ],
      "metadata": {
        "id": "lvw6jqjvatp5"
      },
      "id": "lvw6jqjvatp5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables (proviede)\n",
        "# Execute this code cell to define provided variables\n",
        "wine_2PCS = Xw_pca[:, :2] # Wines dataset projected into the first 2 PCs"
      ],
      "metadata": {
        "id": "x5Jgu247bqHu"
      },
      "id": "x5Jgu247bqHu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1\n",
        "ks, scores = NotImplemented ### YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "EQdsTZaVb55E"
      },
      "id": "EQdsTZaVb55E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the optimal value of k (number of clusters) according to the Silhouette Score analysis? Does it agree with the Elbow plot?\n",
        "\n",
        "YOUR SOLUTION HERE"
      ],
      "metadata": {
        "id": "cUrTFHiQcBWV"
      },
      "id": "cUrTFHiQcBWV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations!** You have completed Lab 3!  🥳"
      ],
      "metadata": {
        "id": "113MRv7vlyyU"
      },
      "id": "113MRv7vlyyU"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}